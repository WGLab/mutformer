{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mutformer_pretraining_data_generation_(with_dynamic_masking_op).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c4H3ng9asnK"
      },
      "source": [
        "Note: Run multiple copies of this notebook in multiple VMs to generate data in parallel for multiple models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US3YtJAW-eHV"
      },
      "source": [
        "**Note: TO ACCESS ANY BUCKET YOU HAVE PERMISSION TO VIEW: go to this address: https://console.cloud.google.com/storage/browser/(BUCKET_NAME)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "source": [
        "#@markdown ###General Config\n",
        "#@markdown #####Whether you are using a Google Cloud VM and Google Cloud Platform TPU; NOTE: make sure this value is the same in the training script when you run it:\n",
        "USE_GCP_TPU = True #@param {type:\"boolean\"}\n",
        "MAX_SEQ_LENGTH =  1024#@param {type:\"integer\"}\n",
        "#@markdown #####also make sure this value is the same in the training script:\n",
        "DATA_COPIES = 20 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "DO_LOWER_CASE = False #@param {type:\"boolean\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "MODEL_ID = \"modified_large\" #@param {type:\"string\"}\n",
        "PRETRAINING_DIR = \"pretraining_data_1024\" #@param {type:\"string\"}\n",
        "TESTING_DIR = \"testing_data_1024\" #@param {type:\"string\"}\n",
        "EVAL_DIR = \"eval_data_1024\" #@param {type:\"string\"}\n",
        "#@markdown #####for miscellaneous temporary storage (make sure this value is the same in the training script)\n",
        "TEMP_DIR = \"modified_large_temp\" #@param {type:\"string\"}\n",
        "#@markdown whether or not this script is being run in a GCP runtime (if more memory is required for large databases)\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "\n",
        "##Vocabulary for the model (B and J are markers for the beginning and ending of a protein sequence)\n",
        "vocab = \\\n",
        "'''[PAD]\n",
        "[UNK]\n",
        "[CLS]\n",
        "[SEP]\n",
        "[MASK]\n",
        "L\n",
        "S\n",
        "B\n",
        "J\n",
        "E\n",
        "A\n",
        "P\n",
        "T\n",
        "G\n",
        "V\n",
        "K\n",
        "R\n",
        "D\n",
        "Q\n",
        "I\n",
        "N\n",
        "F\n",
        "H\n",
        "Y\n",
        "C\n",
        "M\n",
        "W'''\n",
        "\n",
        "\n",
        "with open(\"vocab.txt\", \"w\") as fo:\n",
        "  for token in vocab.split(\"\\n\"):\n",
        "    fo.write(token+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXZmTC1GZWZf"
      },
      "source": [
        "#If using a GCP runtime to generate data (if database is large and more memory is needed), use these commands prior to running this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBnucZk7jS_c"
      },
      "source": [
        "To ssh into the VM:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Make sure the port above matches the port below (in this case it's 8888)\n",
        "\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "\n",
        "(one command):sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "And then copy and paste the outputted link with \"locahost: ...\" into the colab connect to local runtime option\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw5pJGZcWZNl"
      },
      "source": [
        "#Clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhvlnsEAWYvz"
      },
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git\n",
        "#@markdown ######where to clone the repo into (only value that it can't be is \"mutformer\"):\n",
        "REPO_DESTINATION_PATH = \"code/mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Imports/Authenticate for GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW"
      },
      "source": [
        "if not GCP_RUNTIME:\n",
        "  print(\"Authorize for GCS:\")\n",
        "  auth.authenticate_user()\n",
        "  print(\"Authorize done\")\n",
        "\n",
        "  %tensorflow_version 1.x\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import auth\n",
        "\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization\n",
        "from mutformer.modeling import BertModel,BertModelModified\n",
        "from mutformer.run_pretraining import input_fn_builder, model_fn_builder\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# create formatter and add it to the handlers\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBTg4r3tZT3W"
      },
      "source": [
        "#Specify Data location/Mount Drive if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYsYBUCJMTdz"
      },
      "source": [
        "if not GCP_RUNTIME:\n",
        "  from google.colab import drive,auth\n",
        "import os\n",
        "import shutil\n",
        "#@markdown input data folder (can be GCS for large databases)\n",
        "data_folder = \"gs://theodore_jiang/gcs_pretraining_data\" #@param {type: \"string\"}\n",
        "if \"/content/drive\" in data_folder:\n",
        "  !fusermount -u /content/drive\n",
        "  drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH = \"/content/drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdQEOzhYmSh"
      },
      "source": [
        "## Data preparation (make shards)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIrtA2cT1VZM"
      },
      "source": [
        "#@markdown if using a data from google drive, size of a single chunk/shard of data (in terms of lines/datatpoints)\n",
        "chunk_size_gd = 256000 #@param {type:\"number\"}\n",
        "#@markdown if using a data from GCS, size of a single chunk/shard of data (in terms of bytes)\n",
        "chunk_size_gcs =  50e6 #@param {type:\"number\"}\n",
        "chunk_size_gcs = int(chunk_size_gcs)\n",
        "\n",
        "if \"gs://\" in data_folder:\n",
        "  def make_shards(dataset):\n",
        "    print(\"Generating shards for \"+dataset+\":\\n\")\n",
        "    if os .path.exists(\"./shards_\"+dataset+\"\"): ##data will be written as shards to prevent one single files from getting too large\n",
        "      shutil.rmtree(\"./shards_\"+dataset+\"\")\n",
        "    cmd = \"mkdir ./shards_\"+dataset\n",
        "    print(\"removing existing data if it exists...\")\n",
        "    cmd = \"gsutil -m rm -r \"+data_folder+\"/shards_\"+dataset+\"\"\n",
        "    !{cmd}\n",
        "    start = 0\n",
        "    end = chunk_size_gcs\n",
        "    previous_truncated = \"\"\n",
        "    i=0\n",
        "    while True:\n",
        "      print(\"Processing shard \"+str(i))\n",
        "      ##download the selected portion of the input file\n",
        "      cmd = \"gsutil cat -r \"+str(start)+\"-\"+str(end)+\" \"+data_folder+\"/\"+dataset+\".txt\"+\" | gsutil -q cp - ./shards_\"+dataset+\"/shard_tmp\"\n",
        "      !{cmd}\n",
        "      ##get the line count\n",
        "      cmd = \"wc -l <./shards_\"+dataset+\"/shard_tmp\"\n",
        "      line_count = !{cmd}\n",
        "      line_count = int(line_count[0])\n",
        "      \n",
        "      ##get the actual byte count\n",
        "      cmd = \"wc -c <./shards_\"+dataset+\"/shard_tmp\"\n",
        "      byte_count = !{cmd}\n",
        "      byte_count = int(byte_count[0])\n",
        "      if line_count == 0:\n",
        "        print(\"finished after processing \"+str(i)+\" shards... appending the last truncated line to the end and continuing\\n\\n\")\n",
        "        i-=1\n",
        "        break\n",
        "      print(\"processing\",line_count,\"lines...\")\n",
        "      ##get the last few lines of the downloaded file\n",
        "      cmd = \"dd  if=./shards_\"+dataset+\"/shard_tmp ibs=1 skip=\"+str(byte_count-MAX_SEQ_LENGTH*2)+\" count=\"+str(MAX_SEQ_LENGTH*2)+\" status=none > previous_tcd.txt\"\n",
        "      !{cmd}\n",
        "      ##truncate off the last line\n",
        "      cmd = \"sed -ni \\'\"+str(1)+\",\"+str(line_count)+\"p;\"+str(line_count)+\"q\\' ./shards_\"+dataset+\"/shard_tmp\"\n",
        "      !{cmd}\n",
        "\n",
        "      ##add the previously truncated line to the front of the file\n",
        "      cmd = \"sed -i \\'1s/^/\"+previous_truncated+\" /\\' ./shards_\"+dataset+\"/shard_tmp >garbage.txt\"\n",
        "      !{cmd}\n",
        "      ##get the last line, which just got truncated, but will be added to the front of the next shard\n",
        "      previous_truncated = open(\"previous_tcd.txt\").read().split(\"\\n\")[-1]\n",
        "      ##copy data to GCS\n",
        "      print(\"Uploading to GCS...\\n\")\n",
        "      cmd = \"gsutil -q cp ./shards_\"+dataset+\"/shard_tmp \"+data_folder+\"/shards_\"+dataset+\"/shard_\"+str(i)\n",
        "      !{cmd}\n",
        "\n",
        "      start+=chunk_size_gcs\n",
        "      end+=chunk_size_gcs\n",
        "      i+=1\n",
        "    ##appending the last truncated line to the end of the last file\n",
        "    cmd = \"gsutil -q cp \"+data_folder+\"/shards_\"+dataset+\"/shard_\"+str(i)+\" ./shards_\"+dataset+\"/shard_tmp\"\n",
        "    !{cmd}\n",
        "    with open(\"./shards_\"+dataset+\"/shard_tmp\",\"a\") as writer:\n",
        "      writer.write(previous_truncated)\n",
        "    cmd = \"gsutil cp ./shards_\"+dataset+\"/shard_tmp \"+data_folder+\"/shards_\"+dataset+\"/shard_\"+str(i)\n",
        "    !{cmd}\n",
        "    return data_folder+\"/shards_\"+dataset\n",
        "  input_train_dir = make_shards(\"train\")\n",
        "  input_eval_dir = make_shards(\"eval\")\n",
        "  input_test_dir = make_shards(\"test\")\n",
        "\n",
        "else:\n",
        "  DATA_FPATH_train = data_folder+\"/train.txt\" \n",
        "  DATA_FPATH_eval = data_folder+\"/eval.txt\"\n",
        "  DATA_FPATH_test = data_folder+\"/test.txt\"\n",
        "\n",
        "  !split -a 4 -l $chunk_size_gd -d $DATA_FPATH_train ./shards_train/shard_\n",
        "  !ls ./shards_train/\"\n",
        "  input_train_dir = \"./shards_train\"\n",
        "\n",
        "  if os .path.exists(\"./shards_eval\"):\n",
        "    shutil.rmtree(\"./shards_eval\")\n",
        "  !mkdir ./shards_eval\n",
        "  !split -a 4 -l $chunk_size_gd -d $DATA_FPATH_eval ./shards_eval/shard_\n",
        "  !ls ./shards_eval/\n",
        "  input_eval_dir = \"./shards_eval\"\n",
        "\n",
        "  if os .path.exists(\"./shards_test\"):\n",
        "    shutil.rmtree(\"./shards_test\")\n",
        "  !mkdir ./shards_test\n",
        "  !split -a 4 -l $chunk_size_gd -d $DATA_FPATH_test ./shards_test/shard_\n",
        "  !ls ./shards_test/\n",
        "  input_test_dir = \"./shards_test\"\n",
        "\n",
        "def generate_data(input_dir,dir):\n",
        "  seed = random.randrange(sys.maxsize)\n",
        "  input_files = \",\".join([input_dir+\"/\"+file for file in tf.io.gfile.listdir(input_dir)])\n",
        "  out_files = \",\".join([dir+\"/\"+file+\".tfrecord\" for file in tf.io.gfile.listdir(input_dir)])\n",
        "  print(\"input_files:\",input_files,\"output_files:\",out_files)\n",
        "  XARGS_CMD = (\"python3 mutformer/create_pretraining_data.py \"\n",
        "              \"--input_file={} \"\n",
        "              \"--output_file={} \"\n",
        "              \"--vocab_file=vocab.txt \"\n",
        "              \"--do_lower_case={} \"\n",
        "              \"--max_predictions_per_seq={} \"\n",
        "              \"--max_seq_length={} \"\n",
        "              \"--masked_lm_prob={} \"\n",
        "              \"--random_seed={} \"\n",
        "              \"--dupe_factor=1\")\n",
        "\n",
        "  XARGS_CMD = XARGS_CMD.format(input_files, out_files, \n",
        "                              DO_LOWER_CASE, \n",
        "                              MAX_PREDICTIONS, MAX_SEQ_LENGTH, MASKED_LM_PROB,seed)\n",
        "  if os.path.exists(dir):\n",
        "    shutil.rmtree(dir)\n",
        "  os.mkdir(dir)\n",
        "  \n",
        "  !$XARGS_CMD\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF_2adfbPSjt"
      },
      "source": [
        "#Eval Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgcdEiRjRVes"
      },
      "source": [
        "generate_data(input_eval_dir,EVAL_DIR)\n",
        "cmd=\"gsutil -m cp -r \"+EVAL_DIR +\" gs://\"+BUCKET_NAME\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbYGckHG9DYt"
      },
      "source": [
        "#Testing Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irEl7uV7sqOD"
      },
      "source": [
        "generate_data(input_eval_dir,TESTING_DIR)\n",
        "cmd=\"gsutil -m cp -r \"+TESTING_DIR +\" gs://\"+BUCKET_NAME\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z0MdJkS8_3t"
      },
      "source": [
        "#Constant Parallel Training Data Generation (for dynamic masking)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNvRQBgz9n1O"
      },
      "source": [
        "import time\n",
        "\n",
        "operating_files = [\"available_indexes\"]\n",
        "\n",
        "def download_tmp_files(operating_files): ##for downloading tmp files from drive or GCS\n",
        "  if not os.path.exists(TEMP_DIR):\n",
        "    os.mkdir(TEMP_DIR)\n",
        "  else:\n",
        "    shutil.rmtree(TEMP_DIR)\n",
        "    os.mkdir(TEMP_DIR)\n",
        "\n",
        "  for op_file in operating_files:\n",
        "    if USE_GCP_TPU: ##If using GCP TPU, drive isn't available, so we need to store temporary files in GCS\n",
        "      cmd = \"gsutil -m cp -r gs://\"+BUCKET_NAME+\"/\"+TEMP_DIR+\"/\"+op_file+\".txt \"+TEMP_DIR+\"/\"+op_file+\".txt\"\n",
        "      !{cmd}\n",
        "    else:\n",
        "      shutil.copy(DRIVE_PATH+\"/\"+TEMP_DIR+\"/\"+op_file+\".txt\",TEMP_DIR+\"/\"+op_file+\".txt\")\n",
        "\n",
        "def upload_tmp_files(operating_files): ##for uploading tmp files to drive or GCS\n",
        "  for op_file in operating_files:\n",
        "    if USE_GCP_TPU: ##doing the same thing as above^^\n",
        "      cmd = \"gsutil -m cp -r \"+TEMP_DIR+\"/\"+op_file+\".txt gs://\"+BUCKET_NAME+\"/\"+TEMP_DIR+\"/\"+op_file+\".txt\"\n",
        "      !{cmd}\n",
        "    else:\n",
        "      shutil.copy(TEMP_DIR+\"/\"+op_file+\".txt\",DRIVE_PATH+\"/\"+TEMP_DIR+\"/\"+op_file+\".txt\")\n",
        "\n",
        "while True:\n",
        "  for i in range(0,DATA_COPIES):\n",
        "    run=MODEL_ID\n",
        "    print(\"RUN:\",run)\n",
        "    if not os.path.exists(PRETRAINING_DIR+ \"_\"+ run):\n",
        "      os.mkdir(PRETRAINING_DIR+ \"_\"+run)\n",
        "    else:\n",
        "      shutil.rmtree(PRETRAINING_DIR+ \"_\"+run)\n",
        "      os.mkdir(PRETRAINING_DIR+ \"_\"+run)\n",
        "      \n",
        "    download_tmp_files(operating_files)\n",
        "\n",
        "    print(\"processing data for epoch:\",i)\n",
        "    if not os.path.exists(TEMP_DIR+\"/available_indexes.txt\"): ##checking the tmp files to see which data bins have already been trained on and need to be replaced\n",
        "      available_indexes = open(TEMP_DIR+\"/available_indexes.txt\",\"w+\").read().split(\"\\n\")[:-1]\n",
        "    else:\n",
        "      available_indexes = open(TEMP_DIR+\"/available_indexes.txt\").read().split(\"\\n\")[:-1]\n",
        "    print(\"available_indexes:\",available_indexes)\n",
        "    if str(i) not in available_indexes:\n",
        "      directory = PRETRAINING_DIR+\"_\"+run+\"/\"+str(i)\n",
        "      print(\"writing into dir:\",directory)\n",
        "      generate_data(input_train_dir,directory)\n",
        "      cmd=\"gsutil -m cp -r \"+PRETRAINING_DIR+\"_\"+run+ \" gs://\"+BUCKET_NAME\n",
        "      !{cmd}\n",
        "      open(TEMP_DIR+\"/available_indexes.txt\",\"a\").write(str(i)+\"\\n\")\n",
        "      upload_tmp_files(operating_files)\n",
        "\n",
        "  time.sleep(1200)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}