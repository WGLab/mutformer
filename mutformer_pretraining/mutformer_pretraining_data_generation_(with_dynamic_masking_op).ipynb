{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Pretraining Data Generation Script"
      ],
      "metadata": {
        "id": "5EdmfR8oIjJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook processes tsv data and uploads the processed data to GCS to be used for pretraining Mutformer.\n",
        "\n",
        "Note: Run multiple copies of this notebook in multiple VMs to generate data in parallel for multiple models\n",
        "\n",
        "* Note: TO ACCESS ANY BUCKET WITH PERMISSION TO VIEW: go to this address: https://console.cloud.google.com/storage/browser/(BUCKET_NAME)\n",
        "\n"
      ],
      "metadata": {
        "id": "1S6JjH5PIqFF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downgrade TensorFlow (most likely requires runtime restart if using Colab runtime)"
      ],
      "metadata": {
        "id": "-E7g5dW24N-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "metadata": {
        "id": "CGVYeIyb4Nrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "outputs": [],
      "source": [
        "#@markdown ###General Config\n",
        "#@markdown Whether or not this script is being run in a GCP runtime (if more memory is required for large databases)\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "#@markdown Name of the GCS bucket to use:\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown Because of dynamic masking, which requires generating a new dataset for each train epoch, the train, eval, and test set are written into separate folders.\n",
        "#@markdown * Folder in GCS to store pretraining data in (dynamic masking generation write multiple datasets as subfolders inside of this folder):\n",
        "DATA_DIR = \"pretraining_data_1024_embedded_mutformer\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ###Data Config\n",
        "#@markdown Maximum number of datasets to keep at a time (make sure this value is the same in the training script):\n",
        "DATA_COPIES = 20 #@param {type:\"integer\"}\n",
        "#@markdown Maximum output data sequence length:\n",
        "MAX_SEQ_LENGTH =  1024#@param {type:\"integer\"}\n",
        "#@markdown For the masked LM task, a certain number of amino acids per sequence are masked. This number will be determined below by either a probability or a fixed number of masks, whichever number is lower.\n",
        "#@markdown * What probability to use for masking amino acids:\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "#@markdown * What fixed max number of masked amino acids to use:\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "\n",
        "DATA_INFO = {      ##dictionary that will be uploaded alongside each dataset to indicate its parameters\n",
        "      \"sequence_length\":MAX_SEQ_LENGTH,\n",
        "      \"max_num_predictions\":MAX_PREDICTIONS,\n",
        "      \"max_masked_prob\":MASKED_LM_PROB\n",
        "}\n",
        "#### Vocabulary for the model (MutFormer uses the vocabulary below) ([PAD]\n",
        "#### [UNK],[CLS],[SEP], and [MASK] are necessary default tokens; B and J\n",
        "#### are markers for the beginning and ending of a protein sequence,\n",
        "#### respectively; the rest are all amino acids possible, ranked \n",
        "#### approximately by frequency of occurence in human population)\n",
        "#### vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
        "vocab = \\\n",
        "'''[PAD]\n",
        "[UNK]\n",
        "[CLS]\n",
        "[SEP]\n",
        "[MASK]\n",
        "L\n",
        "S\n",
        "B\n",
        "J\n",
        "E\n",
        "A\n",
        "P\n",
        "T\n",
        "G\n",
        "V\n",
        "K\n",
        "R\n",
        "D\n",
        "Q\n",
        "I\n",
        "N\n",
        "F\n",
        "H\n",
        "Y\n",
        "C\n",
        "M\n",
        "W'''\n",
        "with open(\"vocab.txt\", \"w\") as fo:\n",
        "  for token in vocab.split(\"\\n\"):\n",
        "    fo.write(token+\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXZmTC1GZWZf"
      },
      "source": [
        "#If using a GCP runtime to generate data (if database is large and more memory is needed), follow these instructions to set it up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBnucZk7jS_c"
      },
      "source": [
        "###1) Create a VM from the GCP website\n",
        "###2) Open a command prompt on your computer and perform the following steps\"\n",
        "To ssh into the VM, run:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Note: Make sure the port above matches the port below (in this case it's 8888)\n",
        "\\\n",
        "\\\n",
        "In the new command prompt that popped out, either run each of the commands below individually, or copy and paste the one liner below:\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "One command:\n",
        "```\n",
        "sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "###3) In this notebook, click the \"connect to local runtime\" option under the connect button, and copy and paste the link outputted by command prompt with \"locahost: ...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw5pJGZcWZNl"
      },
      "source": [
        "#Clone the MutFormer repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhvlnsEAWYvz"
      },
      "outputs": [],
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git-all\n",
        "#@markdown Where to clone the repo into:\n",
        "REPO_DESTINATION_PATH = \"mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Imports/Authenticate for GCP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S4CiOh3RzFW"
      },
      "outputs": [],
      "source": [
        "if not GCP_RUNTIME:\n",
        "  def authenticate_user(): ##authentication function that uses link authentication instead of popup\n",
        "    if os.path.exists(\"/content/.config/application_default_credentials.json\"): \n",
        "      return\n",
        "    print(\"Authorize for runtime GCS:\")\n",
        "    !gcloud auth login --no-launch-browser\n",
        "    print(\"Authorize for TPU GCS:\")\n",
        "    !gcloud auth application-default login  --no-launch-browser\n",
        "  authenticate_user()\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow.compat.v1 as tf\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import importlib\n",
        "\n",
        "if REPO_DESTINATION_PATH == \"mutformer\":\n",
        "  if os.path.exists(\"mutformer_code\"):\n",
        "    shutil.rmtree(\"mutformer_code\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH,\"mutformer_code\")\n",
        "  REPO_DESTINATION_PATH = \"mutformer_code\"\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import tokenization\n",
        "\n",
        "##reload modules so that you don't need to restart the runtime to reload modules in case that's needed\n",
        "modules2reload = [tokenization]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBTg4r3tZT3W"
      },
      "source": [
        "#Specify Input Data location/Mount Drive if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYsYBUCJMTdz"
      },
      "outputs": [],
      "source": [
        "if not GCP_RUNTIME:\n",
        "  from google.colab import drive\n",
        "#@markdown Input finetuning data folder: data will be read from here to be processed and uploaded to GCS (can be a drive path or a GCS path; must be a GCS path if using GCP_RUNTIME):\n",
        "INPUT_DATA_FOLDER = \"gs://theodore_jiang/gcs_pretraining_data\" #@param {type: \"string\"}\n",
        "if \"/content/drive\" in INPUT_DATA_FOLDER:\n",
        "  !fusermount -u /content/drive\n",
        "  drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH = \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPxC6JGfWDBS"
      },
      "source": [
        "#Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As part of the data generation process, this script:\n",
        "1. Create shards for eval, test, and train sets: in order to conserve memory during data generation, data is split into multiple shards to be processed and uploaded.\n",
        "2. Generates eval, test sets:\n",
        "  * Eval and test sets do not need to be dynamically masked, so they are generated once into seperate folders inside GCS \n",
        "3. Generates train set repeatedly for dynamic masking:\n",
        "  * The script generates DATA_COPIES datasets using the same input train data, but with each epoch containing data with different amino acid masking/alterations, which prevents overfitting during pretraining"
      ],
      "metadata": {
        "id": "YexMts8ovgD1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gdQEOzhYmSh"
      },
      "source": [
        "## Data preparation (make shards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIrtA2cT1VZM"
      },
      "outputs": [],
      "source": [
        "#@markdown Whether or not to store shards into GCS (useful for large databases)\n",
        "GCS_shards = False #@param {type:\"boolean\"}\n",
        "#@markdown If using a data from google drive, size of a single chunk/shard of data (in terms of lines/datatpoints)\n",
        "chunk_size_gd = 256000 #@param {type:\"number\"}\n",
        "#@markdown If using a data from GCS, size of a single chunk/shard of data (in terms of bytes)(the code segment will take care of abnormal line cutoffs)\n",
        "chunk_size_gcs =  500e6 #@param {type:\"number\"}\n",
        "chunk_size_gcs = int(chunk_size_gcs)\n",
        "\n",
        "def make_shards(dataset):    \n",
        "  print(\"Generating shards for \"+dataset+\":\\n\")\n",
        "  if os .path.exists(\"./shards_tmp_\"+dataset): ##data will be written as shards to prevent one single files from getting too large\n",
        "    shutil.rmtree(\"./shards_tmp_\"+dataset)\n",
        "  os.makedirs(\"./shards_tmp_\"+dataset)\n",
        "\n",
        "  if os .path.exists(\"./shards_\"+dataset): \n",
        "    shutil.rmtree(\"./shards_\"+dataset)\n",
        "  os.makedirs(\"./shards_\"+dataset)\n",
        "  \n",
        "  if GCS_shards:\n",
        "    print(\"removing existing data if it exists...\")\n",
        "    cmd = \"gsutil -m rm -r \"+INPUT_DATA_FOLDER+\"/shards_\"+dataset+\"\"\n",
        "    !{cmd}\n",
        "  start = 0\n",
        "  end = chunk_size_gcs\n",
        "  previous_truncated = \"\"\n",
        "  i=0\n",
        "  while True:\n",
        "    print(\"Processing shard \"+str(i))\n",
        "    ##download the selected portion of the input file\n",
        "    if \"gs://\" in INPUT_DATA_FOLDER:\n",
        "      cmd = \"gsutil cat -r \"+str(start)+\"-\"+str(end)+\" \"+INPUT_DATA_FOLDER+\"/\"+dataset+\".txt\"+\" | gsutil -q cp - ./shards_tmp_\"+dataset+\"/shard_tmp\"\n",
        "      !{cmd}\n",
        "    else:\n",
        "      cmd = \"cat -r \"+str(start)+\"-\"+str(end)+\" \"+INPUT_DATA_FOLDER+\"/\"+dataset+\".txt\"+\" | gsutil -q cp - ./shards_tmp_\"+dataset+\"/shard_tmp\"\n",
        "      !{cmd}\n",
        "    ##get the line count\n",
        "    cmd = \"wc -l <./shards_tmp_\"+dataset+\"/shard_tmp\"\n",
        "    line_count = !{cmd}\n",
        "    line_count = int(line_count[0])\n",
        "    \n",
        "    ##get the actual byte count\n",
        "    cmd = \"wc -c <./shards_tmp_\"+dataset+\"/shard_tmp\"\n",
        "    byte_count = !{cmd}\n",
        "    byte_count = int(byte_count[0])\n",
        "    if line_count == 0:\n",
        "      print(\"(Ignore the previous ServiceException.) finished after processing \"+str(i+1)+\" shards... appending the last truncated line to the end and continuing\\n\\n\")\n",
        "      i-=1\n",
        "      break\n",
        "    print(\"processing\",line_count,\"lines...\")\n",
        "    ##get the last few lines of the downloaded file\n",
        "    cmd = \"dd  if=./shards_tmp_\"+dataset+\"/shard_tmp ibs=1 skip=\"+str(byte_count-MAX_SEQ_LENGTH*2)+\" count=\"+str(MAX_SEQ_LENGTH*2)+\" status=none > previous_tcd.txt\"\n",
        "    !{cmd}\n",
        "    ##truncate off the last line\n",
        "    cmd = \"sed -ni \\'\"+str(1)+\",\"+str(line_count)+\"p;\"+str(line_count)+\"q\\' ./shards_tmp_\"+dataset+\"/shard_tmp\"\n",
        "    !{cmd}\n",
        "\n",
        "    ##add the previously truncated line to the front of the file\n",
        "    cmd = \"sed -i \\'1s/^/\"+previous_truncated+\" /\\' ./shards_tmp_\"+dataset+\"/shard_tmp >garbage.txt\"\n",
        "    !{cmd}\n",
        "    ##get the last line, which just got truncated, but will be added to the front of the next shard\n",
        "    previous_truncated = open(\"previous_tcd.txt\").read().split(\"\\n\")[-1]\n",
        "    ##copy data to GCS\n",
        "    \n",
        "    if GCS_shards:\n",
        "      print(\"Uploading to GCS...\\n\")\n",
        "      cmd = \"gsutil -q cp ./shards_tmp_\"+dataset+\"/shard_tmp \"+INPUT_DATA_FOLDER+\"/shards_\"+dataset+\"/shard_\"+str(i)\n",
        "      !{cmd}\n",
        "    else:\n",
        "      cmd = \"cp ./shards_tmp_\"+dataset+\"/shard_tmp ./shards_\"+dataset+\"/shard_\"+str(i)\n",
        "      !{cmd}\n",
        "\n",
        "    start+=chunk_size_gcs\n",
        "    end+=chunk_size_gcs\n",
        "    i+=1\n",
        "  ##appending the last truncated line to the end of the last file\n",
        "\n",
        "  if GCS_shards:\n",
        "    cmd = \"gsutil -q cp \"+INPUT_DATA_FOLDER+\"/shards_\"+dataset+\"/shard_\"+str(i)+\" ./shards_tmp_\"+dataset+\"/shard_tmp\"\n",
        "    !{cmd}\n",
        "    with open(\"./shards_tmp_\"+dataset+\"/shard_tmp\",\"a\") as writer:\n",
        "      writer.write(previous_truncated)\n",
        "    cmd = \"gsutil cp ./shards_tmp_\"+dataset+\"/shard_tmp \"+INPUT_DATA_FOLDER+\"/shards_\"+dataset+\"/shard_\"+str(i)\n",
        "    !{cmd}\n",
        "  else:\n",
        "    cmd = \"cp ./shards_\"+dataset+\"/shard_\"+str(i)+\" ./shards_tmp_\"+dataset+\"/shard_tmp\"\n",
        "    !{cmd}\n",
        "    with open(\"./shards_tmp_\"+dataset+\"/shard_tmp\",\"a\") as writer:\n",
        "      writer.write(previous_truncated)\n",
        "    cmd = \"cp ./shards_tmp_\"+dataset+\"/shard_tmp ./shards_\"+dataset+\"/shard_\"+str(i)\n",
        "    !{cmd}\n",
        "  if GCS_shards:\n",
        "    data_dir = INPUT_DATA_FOLDER+\"/shards_\"+dataset\n",
        "  else:\n",
        "    data_dir = \"./shards_\"+dataset\n",
        "  return data_dir\n",
        "input_train_dir = make_shards(\"train\")\n",
        "input_eval_dir = make_shards(\"eval\")\n",
        "input_test_dir = make_shards(\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GunaRNUnV8xq"
      },
      "source": [
        "##Define Data Generation Op"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1_TljiOV6gB"
      },
      "outputs": [],
      "source": [
        "def generate_data(input_dir,dir):\n",
        "  seed = random.randrange(sys.maxsize)\n",
        "  input_files = \",\".join([input_dir+\"/\"+file for file in tf.io.gfile.listdir(input_dir)])\n",
        "  out_files = \",\".join([dir+\"/\"+file+\".tfrecord\" for file in tf.io.gfile.listdir(input_dir)])\n",
        "  print(\"input_files:\",input_files,\"output_files:\",out_files)\n",
        "\n",
        "  XARGS_CMD = (f\"python3 mutformer/create_pretraining_data.py \"\n",
        "              f\"--input_file={input_files} \"\n",
        "              f\"--output_file={out_files} \"\n",
        "              f\"--vocab_file=vocab.txt \"\n",
        "              f\"--do_lower_case=False \"\n",
        "              f\"--max_predictions_per_seq={MAX_PREDICTIONS} \"\n",
        "              f\"--max_seq_length={MAX_SEQ_LENGTH} \"\n",
        "              f\"--masked_lm_prob={MASKED_LM_PROB} \"\n",
        "              f\"--random_seed={seed} \"\n",
        "              f\"--dupe_factor=1\")\n",
        "  \n",
        "  if os.path.exists(dir):\n",
        "    shutil.rmtree(dir)\n",
        "  os.makedirs(dir)\n",
        "  \n",
        "  !$XARGS_CMD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF_2adfbPSjt"
      },
      "source": [
        "##Eval and Test Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgcdEiRjRVes"
      },
      "outputs": [],
      "source": [
        "EVAL_DIR = f\"{DATA_DIR}/eval\"\n",
        "TESTING_DIR = f\"{DATA_DIR}/test\"\n",
        "\n",
        "##Eval set\n",
        "if os.path.exists(EVAL_DIR):\n",
        "  shutil.rmtree(EVAL_DIR)\n",
        "os.makedirs(EVAL_DIR)\n",
        "generate_data(input_eval_dir,EVAL_DIR)\n",
        "with open(EVAL_DIR+\"/info.json\",\"w+\") as out: ##writes out the dictionary containing\n",
        "      json.dump(DATA_INFO,out,indent=2)                   ##the dataset's parameters\n",
        "\n",
        "##Test set\n",
        "if os.path.exists(TESTING_DIR):\n",
        "  shutil.rmtree(TESTING_DIR)\n",
        "os.makedirs(TESTING_DIR)\n",
        "generate_data(input_test_dir,TESTING_DIR)\n",
        "with open(TESTING_DIR+\"/info.json\",\"w+\") as out: \n",
        "      json.dump(DATA_INFO,out,indent=2)                  \n",
        "\n",
        "cmd=\"gsutil -m cp -r \"+DATA_DIR +\" gs://\"+BUCKET_NAME\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z0MdJkS8_3t"
      },
      "source": [
        "##Repeated Parallel Training Data Generation (for dynamic masking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNvRQBgz9n1O"
      },
      "outputs": [],
      "source": [
        "#@markdown When finished generating all DATA_COPIES datasets, how long to wait before checking again if the data has been used (to minimize interaction with GCS, should be around the same time it takes for the mode train script to train 1 model):\n",
        "CHECK_DATA_EVERY_N_SECS = 1200 #@param {type:\"integer\"}\n",
        "\n",
        "TRAIN_DIR = f\"{DATA_DIR}/train\"\n",
        "\n",
        "while True:\n",
        "  try:\n",
        "    available_indexes = tf.io.gfile.listdir(BUCKET_PATH+\"/\"+TRAIN_DIR) ##get all currently \n",
        "  except Exception:                                                    ##generated datasets\n",
        "    available_indexes = []\n",
        "\n",
        "  print(\"Already generated datasets:\",available_indexes)\n",
        "   \n",
        "  for i in range(0,DATA_COPIES):                                     \n",
        "    if os.path.exists(TRAIN_DIR):\n",
        "      shutil.rmtree(TRAIN_DIR)\n",
        "    \n",
        "    os.makedirs(TRAIN_DIR)\n",
        "    available_indexes = [''.join([i for i in available_index if i.isdigit()]) for available_index in available_indexes]\n",
        "    if str(i) not in available_indexes:\n",
        "      print(\"Processing data for dataset number:\",i)\n",
        "      out_dir = TRAIN_DIR+\"/\"+str(i)\n",
        "      print(\"Writing into dir:\",out_dir)\n",
        "      if not os.path.exists(out_dir):\n",
        "        os.makedirs(out_dir)\n",
        "      generate_data(input_train_dir,out_dir)\n",
        "      print(\"\\nUpdating and uploading data info json...\\n\")\n",
        "      \n",
        "      with open(out_dir+\"/info.json\",\"w+\") as out: ##writes out a dictionary containing\n",
        "        json.dump(DATA_INFO,out,indent=2)                   ##the dataset's parameters\n",
        "      print(\"Data info json uploaded successfully\")\n",
        "      cmd=\"gsutil -m cp -r \"+out_dir+\" \"+BUCKET_PATH+\"/\"+out_dir\n",
        "      !{cmd}\n",
        "      available_indexes = tf.io.gfile.listdir(BUCKET_PATH+\"/\"+TRAIN_DIR) ##we only have to refresh \n",
        "                                                                        ##available indexes after \n",
        "                                                                        ##each data generation \n",
        "    time.sleep(1) ##1 second sleep to prevent abnormal cloud interactions due to timing\n",
        "  time.sleep(CHECK_DATA_EVERY_N_SECS)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nXZmTC1GZWZf",
        "tF_2adfbPSjt"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}