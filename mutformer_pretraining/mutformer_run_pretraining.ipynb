{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUgSgfCHJEvv"
      },
      "source": [
        "#Pretraining Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDW2Nsy2JHTZ"
      },
      "source": [
        "This script pretrains a transformer model on protein sequences.\n",
        "\n",
        "Note: If using a TPU from Google Cloud (not the Colab TPU), make sure to run this notebook on a VM with access to all GCP APIs, and make sure TPUs are enabled for the GCP project\n",
        "\n",
        "Note: Run multiple copies of this notebook in multiple VMs to train multiple models in parallel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downgrade Python and Tensorflow \n",
        "\n",
        "(the default python version in Colab does not support Tensorflow 1.15)\n",
        "\n",
        "* **Note** that because the Python used in this notebook is not the default path, syntax highlighting most likely will not function."
      ],
      "metadata": {
        "id": "csTvz4cJ69od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. First, download and install Python version 3.7:"
      ],
      "metadata": {
        "id": "j2TIlZ-xP5ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py37_22.11.1-1-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py37\" --user"
      ],
      "metadata": {
        "id": "VVgNJsj68iTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Then, reload the webpage (not restart runtime) to allow Colab to recognize the newly installed python\n",
        "####3. Finally, run the following commands to install tensorflow 1.15:"
      ],
      "metadata": {
        "id": "jx0kPbqA-fLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow==1.15"
      ],
      "metadata": {
        "id": "EvdskbIR9zHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "outputs": [],
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown If preferred, a GCP TPU/runtime can be used to run this notebook (instructions below):\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "#@markdown How many TPU scores the TPU has: if using colab, NUM_TPU_CORES is 8:\n",
        "NUM_TPU_CORES = 8 #@param {type:\"number\"}\n",
        "#@markdown Name of the GCS bucket to use (Make sure to set this to the name of your own GCS  bucket):\n",
        "BUCKET_NAME = \"\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown ## IO Config\n",
        "OUTPUT_MODEL_DIR = \"bert_model_embedded_mutformer_12L\" #@param {type:\"string\"}\n",
        "#@markdown Folder in GCS where data was stored:\n",
        "DATA_DIR = \"pretraining_data_1024_embedded_mutformer\" #@param {type:\"string\"}\n",
        "LOGGING_DIR = \"mutformer2_0_pretraining_logs\" #@param {type:\"string\"}\n",
        "RUN_NAME = \"bert_model_embedded_mutformer_12L\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "#### Vocabulary for the model (MutFormer uses the vocabulary below) ([PAD]\n",
        "#### [UNK],[CLS],[SEP], and [MASK] are necessary default tokens; B and J\n",
        "#### are markers for the beginning and ending of a protein sequence,\n",
        "#### respectively; the rest are all amino acids possible, ranked \n",
        "#### approximately by frequency of occurence in human population)\n",
        "#### vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
        "vocab = \"\\n\".join(\"[PAD] [UNK] [CLS] [SEP] [MASK] L S B J E A P T G V K R D Q I N F H Y C M W\".split(\" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMeXHe0mPDwY"
      },
      "source": [
        "#If running on a GCP TPU, use these commands prior to running this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqoH1BZfkOJE"
      },
      "source": [
        "To ssh into the VM:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Make sure the port above matches the port below (in this case it's 8888)\n",
        "\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "\n",
        "(one command):sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "And then copy and paste the outputted link with \"locahost: ...\" into the colab connect to local runtime option\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AKPxJa2kSo6"
      },
      "source": [
        "####Also run this code segment, which creates a TPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bieoa9XIPMXx"
      },
      "outputs": [],
      "source": [
        "GCE_PROJECT_NAME = \"\" #@param {type:\"string\"}\n",
        "TPU_ZONE = \"us-central1-f\" #@param {type:\"string\"}\n",
        "TPU_NAME = \"mutformer-tpu\" #@param {type:\"string\"}\n",
        "\n",
        "!gcloud alpha compute tpus create $TPU_NAME --accelerator-type=tpu-v2 --version=1.15.5 --zone=$TPU_ZONE ##create new TPU\n",
        "\n",
        "!gsutil iam ch serviceAccount:`gcloud alpha compute tpus describe $TPU_NAME | grep serviceAccount | cut -d' ' -f2`:admin $BUCKET_PATH && echo 'Successfully set permissions!' ##give TPU access to GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O8Q2nFs5RHb"
      },
      "source": [
        "#Clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5whu5PjE5Q56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0a35d82-0879-441b-ea3e-3d45ebe10b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'code/mutformer'...\n",
            "remote: Enumerating objects: 1506, done.\u001b[K\n",
            "remote: Counting objects: 100% (386/386), done.\u001b[K\n",
            "remote: Compressing objects: 100% (197/197), done.\u001b[K\n",
            "remote: Total 1506 (delta 265), reused 257 (delta 184), pack-reused 1120\u001b[K\n",
            "Receiving objects: 100% (1506/1506), 6.02 MiB | 17.60 MiB/s, done.\n",
            "Resolving deltas: 100% (1054/1054), done.\n"
          ]
        }
      ],
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git\n",
        "#@markdown ######Where to clone the repo into (only value that it can't be is \"mutformer\"):\n",
        "REPO_DESTINATION_PATH = \"code/mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Imports/Authenticate for GCP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S4CiOh3RzFW"
      },
      "outputs": [],
      "source": [
        "if not GCP_RUNTIME:\n",
        "  def authenticate_user(): ##authentication function that uses link authentication instead of popup\n",
        "    if os.path.exists(\"/content/.config/application_default_credentials.json\"): \n",
        "      return\n",
        "    print(\"Authorize for runtime GCS:\")\n",
        "    !gcloud auth login --no-launch-browser\n",
        "    print(\"Authorize for TPU GCS:\")\n",
        "    !gcloud auth application-default login  --no-launch-browser\n",
        "  authenticate_user()\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import importlib\n",
        "\n",
        "if REPO_DESTINATION_PATH == \"mutformer\":\n",
        "  shutil.copytree(REPO_DESTINATION_PATH,\"mutformer_code\")\n",
        "  REPO_DESTINATION_PATH = \"mutformer_code\"\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization, run_pretraining\n",
        "\n",
        "##reload modules so that you don't need to restart the runtime to reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_pretraining]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "from modeling import *\n",
        "\n",
        "##configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "#@markdown Whether or not to write logs to a file\n",
        "DO_FILE_LOGGING = True #@param {type:\"boolean\"}\n",
        "if DO_FILE_LOGGING:\n",
        "  #@markdown If using file logging, what path to write logs to\n",
        "  FILE_LOGGING_PATH = 'file_logging/spam.log' #@param {type:\"string\"}\n",
        "  if not os.path.exists(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1])):\n",
        "    os.makedirs(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1]))\n",
        "  fh = logging.FileHandler(FILE_LOGGING_PATH)\n",
        "  fh.setLevel(logging.INFO)\n",
        "  fh.setFormatter(formatter)\n",
        "  log.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "if GCP_RUNTIME:\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_NAME, zone=TPU_ZONE, project=GCE_PROJECT_NAME)\n",
        "  TPU_ADDRESS = tpu_cluster_resolver.get_master()\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "      log.info('TPU address is ' + TPU_ADDRESS)\n",
        "      # Upload credentials to TPU.\n",
        "      tf.contrib.cloud.configure_gcs(session)\n",
        "else:\n",
        "  if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    log.info(\"Using TPU runtime\")\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "    with tf.Session(TPU_ADDRESS) as session:\n",
        "      log.info('TPU address is ' + TPU_ADDRESS)\n",
        "      # Upload credentials to TPU.\n",
        "      with tf.gfile.Open(\"/content/.config/application_default_credentials.json\", 'r') as f:\n",
        "        auth_info = json.load(f)\n",
        "      tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "      \n",
        "  else:\n",
        "    raise Exception('Not connected to TPU runtime, TPU required to run mutformer')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0TXw9GtCKz"
      },
      "source": [
        "#Auto Detect amount of sequences per epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYsYBUCJMTdz"
      },
      "outputs": [],
      "source": [
        "#@markdown If not GCP_RUNTIME and data was stored in drive, folder where the original data was stored (for detecting the # of steps per epoch) (this variable should match up with the \"INPUT_DATA_FOLDER\" variable in the data generation script) (this is used to limit interaction with GCS; it can also be left blank and steps will be automatically detected from tfrecords stored in GCS).\n",
        "#@markdown \n",
        "#@markdown Note: if data was originally stored in GCS or GCP_RUNTIME is true, leave this item blank and steps per epoch will be autodetected from tfrecords:\n",
        "ORIG_DATA_FOLDER = \"\" #@param {type: \"string\"}\n",
        "\n",
        "if not GCP_RUNTIME and \"/content/drive\" in ORIG_DATA_FOLDER:\n",
        "  from google.colab import drive\n",
        "  !fusermount -u /content/drive\n",
        "  drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH = \"/content/drive/My Drive\"\n",
        "\n",
        "  data_path_train = ORIG_DATA_FOLDER+\"/train.txt\" \n",
        "\n",
        "  lines = tf.gfile.Open(data_path_train).read().split(\"\\n\")\n",
        "  SEQUENCES_PER_EPOCH = len(lines)\n",
        "\n",
        "  print(\"sequences per epoch:\",SEQUENCES_PER_EPOCH)\n",
        "else:\n",
        "  from tqdm import tqdm\n",
        "  def steps_getter(input_files):\n",
        "    tot_sequences = 0\n",
        "    for input_file in input_files:\n",
        "      print(\"reading:\",input_file)\n",
        "\n",
        "      d = tf.data.TFRecordDataset(input_file)\n",
        "\n",
        "      with tf.Session() as sess:\n",
        "        tot_sequences+=sess.run(d.reduce(0, lambda x,_: x+1))\n",
        "\n",
        "    return tot_sequences\n",
        "\n",
        "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "  got_data = False\n",
        "  while not got_data: ##will keep trying to access the data until available\n",
        "    try:\n",
        "      for f in tf.io.gfile.listdir(BUCKET_PATH+\"/\"+DATA_DIR+\"/train\"): ##try to access any of the data bins\n",
        "          print(\"trying to access training data from saved copy number \"+str(f))\n",
        "          DATA_GCS_DIR = BUCKET_PATH+\"/\"+DATA_DIR+\"/train/\"+str(f)\n",
        "          train_input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
        "          print(\"Using:\",train_input_files)\n",
        "          if len(train_input_files)>0:\n",
        "            got_data = True\n",
        "            try:\n",
        "              SEQUENCES_PER_EPOCH = steps_getter(train_input_files)\n",
        "              print(\"sequences per epoch:\",SEQUENCES_PER_EPOCH)\n",
        "              if not SEQUENCES_PER_EPOCH:\n",
        "                for file in train_input_files:\n",
        "                  tf.io.gfile.remove(file)\n",
        "                raise\n",
        "              break\n",
        "            except:\n",
        "              got_data=False\n",
        "    except:\n",
        "      pass\n",
        "    if got_data:\n",
        "      break\n",
        "    raise Exception(\"Could not find data, wait for data generation to create another epoch of data and try again.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "# Run Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the pretraining loop (should run this in parallel with the dynamic masking data generation loop)."
      ],
      "metadata": {
        "id": "NtWrmBgYg1SP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "outputs": [],
      "source": [
        "#@markdown ## Model Config:\n",
        "#@markdown Model architecture to use (BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture without integrated convs, MutFormer_embedded_convs indicates MutFormer with integrated convolutions):\n",
        "MODEL_ARCHITECTURE = MutFormer_embedded_convs #@param\n",
        "#@markdown Maximum sequence length the model should be able to handle (the internal attention mechanisms and embeddings will be created to only account for sequences up to this length) (larger maximum sequence length will take more memory and time to train):\n",
        "model_max_seq_length = 1024 #@param\n",
        "#@markdown Other miscellaneous config entries:\n",
        "hidden_size =   768 #@param {type:\"integer\"}\n",
        "num_hidden_layers =   12#@param {type:\"integer\"}\n",
        "tf_variables_intializer_value_stdev = 0.02 #@param {type:\"number\"}\n",
        "hidden_layers_dropout_probability = 0.1 #@param {type:\"number\"}\n",
        "intermediate_size = 3072 #@param {type:\"integer\"}\n",
        "self_attention_dropout_probability = 0.1 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "bert_config = {                            \n",
        "  \"hidden_size\": hidden_size,\n",
        "  \"hidden_act\": \"gelu\", \n",
        "  \"initializer_range\": tf_variables_intializer_value_stdev, \n",
        "  \"hidden_dropout_prob\": hidden_layers_dropout_probability, \n",
        "  \"num_attention_heads\": num_hidden_layers, \n",
        "  \"type_vocab_size\": 2, \n",
        "  \"max_position_embeddings\": model_max_seq_length, \n",
        "  \"num_hidden_layers\": num_hidden_layers, \n",
        "  \"intermediate_size\": intermediate_size, \n",
        "  \"attention_probs_dropout_prob\": self_attention_dropout_probability\n",
        "}\n",
        "\n",
        "##upload config\n",
        "bert_config[\"vocab_size\"] = len(vocab.split(\"\\n\"))\n",
        "\n",
        "if not os.path.exists(OUTPUT_MODEL_DIR):\n",
        "  os.makedirs(OUTPUT_MODEL_DIR)\n",
        "with tf.gfile.Open(OUTPUT_MODEL_DIR+\"/config.json\", \"w\") as fo:\n",
        "  json.dump(bert_config, fo, indent=2)\n",
        "\n",
        "!gsutil -m cp -r $OUTPUT_MODEL_DIR gs://$BUCKET_NAME\n",
        "\n",
        "\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ## Training procedure config\n",
        "#@markdown When checking for dynamically generated data, how long to wait between each check (to minimize interaction with GCS, should be around the same time it takes for the data generation script to generate 1 epoch worth of data):\n",
        "CHECK_DATA_EVERY_N_SECS = 1200 #@param {type:\"integer\"}\n",
        "INIT_LEARNING_RATE =  2e-5 #@param {type:\"number\"}\n",
        "END_LEARNING_RATE = 1e-9 #@param {type:\"number\"}\n",
        "#@markdown How many checkpoints to keep at a time (older checkpoints will be deleted):\n",
        "KEEP_N_CHECKPOINTS_AT_A_TIME = 20 #@param {type:\"integer\"}\n",
        "#@markdown Stopping condition for training can be set by either a certain number of sequences or a certain number of steps. from below, PLANNED_TOTAL_STEPS will override PLANNED_TOTAL_SEQUENCES_SEEN; therefore, if using PLANNED_TOTAL_SEQUENCES_SEEN, set PLANNED_TOTAL_STEPS to -1.\n",
        "#@markdown \n",
        "#@markdown * Option 1: How many sequences the model should train on before stopping:\n",
        "PLANNED_TOTAL_SEQUENCES_SEEN =  1e9 #@param {type:\"number\"}\n",
        "#@markdown * Option 2: How many steps the model should train for before stopping (number of total sequences trained on will depend on the batch size used):\n",
        "PLANNED_TOTAL_STEPS =  -1#@param {type:\"number\"}\n",
        "TRAIN_BATCH_SIZE =   64#@param {type:\"integer\"}\n",
        "#@markdown If using gradient accumulation (to save memory), what multiplier to use (memory usage and training speed will both be divided by this value) (Note: batch size must be divisible by this number):\n",
        "GRADIENT_ACCUMULATION_MULTIPLIER = 2 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "#@markdown How many steps to wait for each save (not that if SAVE_CHECKPOINT_STEPS is larger than the steps per epoch, the model will be saved every \"steps per epoch\" number of steps):\n",
        "SAVE_CHECKPOINTS_STEPS = 1000 #@param {type:\"integer\"}\n",
        "#@markdown When writing out training logs, how often to write them out:\n",
        "SAVE_LOGS_EVERY_N_STEPS = 500 #@param (type:\"integer\")\n",
        "\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_SEQUENCES_SEEN/TRAIN_BATCH_SIZE if PLANNED_TOTAL_STEPS==-1 else PLANNED_TOTAL_STEPS\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/PLANNED_TOTAL_STEPS\n",
        "\n",
        "\n",
        "BERT_GCS_DIR = BUCKET_PATH+\"/\"+OUTPUT_MODEL_DIR\n",
        "GCS_LOGGING_DIR = BUCKET_PATH+\"/\"+LOGGING_DIR+\"/\"+RUN_NAME\n",
        "\n",
        "CONFIG_FILE = BERT_GCS_DIR+\"/config.json\"\n",
        "\n",
        "while True: ##training loop\n",
        "  INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "  try:\n",
        "    INIT_CHECKPOINT_STEP = int(INIT_CHECKPOINT.split(\"-\")[-1])\n",
        "    current_epoch = int(INIT_CHECKPOINT_STEP/STEPS_PER_EPOCH)\n",
        "    print(\"CURRENT STEP:\",INIT_CHECKPOINT_STEP)\n",
        "    if int(INIT_CHECKPOINT_STEP)>=2000000:#PLANNED_TOTAL_STEPS: ##if reached planed total steps, stop\n",
        "      break\n",
        "  except:\n",
        "    current_epoch = 0\n",
        "  try: ###wrap entire training loop into try and except loop so glitches don't kill training\n",
        "    print(\"\\n\\n\\n\\n\\nEPOCH:\"+str(current_epoch)+\"\\n\")\n",
        "    STEPS_PER_EPOCH = int(SEQUENCES_PER_EPOCH/TRAIN_BATCH_SIZE)\n",
        "    print(\"Steps per epoch:\",STEPS_PER_EPOCH)\n",
        "    print(\"\\n\\n\\n\\n\\n\")\n",
        "\n",
        "    got_data = False\n",
        "    while not got_data:\n",
        "      try:\n",
        "        for f in tf.io.gfile.listdir(BUCKET_PATH+\"/\"+DATA_DIR+\"/train\"): ##try to access any of the data bins\n",
        "          print(\"trying to access training data from saved copy number \"+str(f))\n",
        "          DATA_GCS_DIR = BUCKET_PATH+\"/\"+DATA_DIR+\"/train/\"+str(f)\n",
        "          train_input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR,'*tfrecord'))\n",
        "          print(\"train_input_files:\",train_input_files)\n",
        "          if len(train_input_files)>0:\n",
        "            got_data = True\n",
        "            break\n",
        "      except:\n",
        "          pass\n",
        "      if not got_data:\n",
        "        print(\"Could not find data, waiting for data generation...trying again in another \"+str(CHECK_DATA_EVERY_N_SECS)+\" seconds.\")\n",
        "        time.sleep(CHECK_DATA_EVERY_N_SECS)\n",
        "\n",
        "    config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "\n",
        "    log.info(f\"Using checkpoint: {INIT_CHECKPOINT}\")\n",
        "    log.info(f\"Using {len(train_input_files)} data shards for training\")\n",
        "    model_fn = run_pretraining.model_fn_builder(\n",
        "        bert_config=config,\n",
        "        logging_dir=GCS_LOGGING_DIR,\n",
        "        save_logs_every_n_steps=SAVE_LOGS_EVERY_N_STEPS,\n",
        "        init_checkpoint=INIT_CHECKPOINT,\n",
        "        init_learning_rate=INIT_LEARNING_RATE,\n",
        "        decay_per_step=DECAY_PER_STEP,\n",
        "        num_warmup_steps=10,\n",
        "        use_tpu=True,\n",
        "        use_one_hot_embeddings=True,\n",
        "        bert=MODEL_ARCHITECTURE,\n",
        "        grad_accum_mul=GRADIENT_ACCUMULATION_MULTIPLIER)\n",
        "\n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "    run_config = tf.contrib.tpu.RunConfig(\n",
        "        cluster=tpu_cluster_resolver,\n",
        "        model_dir=BERT_GCS_DIR,\n",
        "        save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "        keep_checkpoint_max=KEEP_N_CHECKPOINTS_AT_A_TIME,\n",
        "        tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "            iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "            num_shards=NUM_TPU_CORES,\n",
        "            per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "    estimator = tf.contrib.tpu.TPUEstimator(\n",
        "        use_tpu=True,\n",
        "        model_fn=model_fn,\n",
        "        config=run_config,\n",
        "        train_batch_size=TRAIN_BATCH_SIZE//GRADIENT_ACCUMULATION_MULTIPLIER,\n",
        "        eval_batch_size=1)\n",
        "      \n",
        "    \n",
        "    DATA_INFO = json.load(tf.gfile.Open(DATA_GCS_DIR+\"info.json\"))\n",
        "    MAX_SEQ_LENGTH = DATA_INFO[\"sequence_length\"]\n",
        "    MAX_PREDICTIONS = DATA_INFO[\"max_num_predictions\"]\n",
        "    \n",
        "    train_input_fn = run_pretraining.input_fn_builder(\n",
        "            input_files=train_input_files,\n",
        "            max_seq_length=MAX_SEQ_LENGTH,\n",
        "            max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "            is_training=True)\n",
        "  except Exception as e:\n",
        "    log.info(f\"Training load failed. error: {e}\")\n",
        "    continue\n",
        "  try:\n",
        "    estimator.train(input_fn=train_input_fn, steps=STEPS_PER_EPOCH)\n",
        "    # For dynamic masking, a parallel data generation is used. This portion deletes the current dataset.\n",
        "    cmd = \"gsutil -m rm -r \"+DATA_GCS_DIR\n",
        "    !{cmd}\n",
        "  except Exception as e:\n",
        "    log.info(f\"Training loop failed. error: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "VMeXHe0mPDwY"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7",
      "name": "py37"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}