{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mutformer_run_pretraining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRCkU78mPzra"
      },
      "source": [
        "Note: If using a TPU from Google Cloud (not the Colab TPU), make sure to run this notebook on a VM with access to all GCP APIs, and make sure TPUs are enabled for the GCP project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3MXrOYYTnZv"
      },
      "source": [
        "Note: Run multiple copies of this notebook in multiple VMs to train multiple models in parallel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "source": [
        "#@markdown ## General Config\n",
        "USE_GCP_TPU = False #@param {type:\"boolean\"}\n",
        "MAX_SEQ_LENGTH =  1024#@param {type:\"integer\"}\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "DO_LOWER_CASE = False #@param {type:\"boolean\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "MODEL_DIR = \"bert_model_modified_large\" #@param {type:\"string\"}\n",
        "PRETRAINING_DIR = \"pretraining_data_1024_modified_large\" #@param {type:\"string\"}\n",
        "LOGGING_DIR = \"bert_model_pretraining_loss_spam\" #@param {type:\"string\"}\n",
        "#@markdown ######for miscellaneous temporary storage\n",
        "TEMP_DIR = \"modified_large_temp\" #@param {type:\"string\"}\n",
        "RUN_NAME = \"bert_model_modified_large\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ## Input data pipeline config\n",
        "DATA_COPIES = 20 #@param {type:\"integer\"}\n",
        "TRAIN_BATCH_SIZE =  32 #@param {type:\"integer\"}\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "MASKED_LM_PROB = 0.15 #@param\n",
        "#@markdown ######When checking data, how long to wait between each check (to minimize interaction with GCS, should be around the same time it takes for the data generation script to generate 1 epoch worth of data)\n",
        "CHECK_DATA_EVERY_N_SECS = 1200 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Training procedure config\n",
        "EVAL_BATCH_SIZE = 64\n",
        "INIT_LEARNING_RATE =  2e-5#@param {type:\"number\"}\n",
        "END_LEARNING_RATE = 1e-9\n",
        "SAVE_CHECKPOINTS_STEPS =  1000#@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8\n",
        "PLANNED_TOTAL_SEQUENCES_SEEN =  1e9 #@param {type:\"number\"}\n",
        "#@markdown ###### (PLANNED_TOTAL_STEPS will override PLANNED_TOTAL_SEQUENCES_SEEN; if you wish to use PLANNED_TOTAL_SEQUENCES_SEEN, set PLANNED_TOTAL_STEPS to -1)\n",
        "PLANNED_TOTAL_STEPS =  2e6#@param {type:\"number\"}\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_SEQUENCES_SEEN/TRAIN_BATCH_SIZE if PLANNED_TOTAL_STEPS==-1 else PLANNED_TOTAL_STEPS\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/PLANNED_TOTAL_STEPS\n",
        "#@markdown ## Model Config:\n",
        "#@markdown ######Possible values for MODEL_TO_USE: orig, withConv:\n",
        "MODEL_TO_USE = \"withConv\" #@param {type:\"string\"}\n",
        "HIDDEN_SIZE =   768#@param {type:\"integer\"}\n",
        "HIDDEN_LAYERS =  12#@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "CUSTOM_MODEL = None ##change this to a model_fn style function if you wish to use a custom model\n",
        "\n",
        "bert_config = {\n",
        "  \"hidden_size\": HIDDEN_SIZE, \n",
        "  \"hidden_act\": \"gelu\", \n",
        "  \"initializer_range\": 0.02, \n",
        "  \"hidden_dropout_prob\": 0.1, \n",
        "  \"num_attention_heads\": HIDDEN_LAYERS, \n",
        "  \"type_vocab_size\": 2, \n",
        "  \"max_position_embeddings\": MAX_SEQ_LENGTH, \n",
        "  \"num_hidden_layers\": HIDDEN_LAYERS, \n",
        "  \"intermediate_size\": 3072, \n",
        "  \"attention_probs_dropout_prob\": 0.1\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMeXHe0mPDwY"
      },
      "source": [
        "#If running on a GCP TPU, use these commands prior to running this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqoH1BZfkOJE"
      },
      "source": [
        "To ssh into the VM:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Make sure the port above matches the port below (in this case it's 8888)\n",
        "\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "\n",
        "(one command):sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "And then copy and paste the outputted link with \"locahost: ...\" into the colab connect to local runtime option\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AKPxJa2kSo6"
      },
      "source": [
        "###Also run this code segment, which creates a TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bieoa9XIPMXx"
      },
      "source": [
        "GCE_PROJECT_NAME = \"genome-project-319100\" #@param {type:\"string\"}\n",
        "TPU_ZONE = \"us-central1-f\" #@param {type:\"string\"}\n",
        "TPU_NAME = \"mutformer-tpu\" #@param {type:\"string\"}\n",
        "\n",
        "!gcloud alpha compute tpus create $TPU_NAME --accelerator-type=tpu-v2 --version=1.15.5 --zone=$TPU_ZONE ##create new TPU\n",
        "\n",
        "!gsutil iam ch serviceAccount:`gcloud alpha compute tpus describe $TPU_NAME | grep serviceAccount | cut -d' ' -f2`:admin gs://theodore_jiang && echo 'Successfully set permissions!' ##give TPU access to GCS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O8Q2nFs5RHb"
      },
      "source": [
        "#Clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5whu5PjE5Q56"
      },
      "source": [
        "if USE_GCP_TPU:\n",
        "  !sudo apt-get -y install git\n",
        "#@markdown ######where to clone the repo into (only value that it can't be is \"mutformer\"):\n",
        "REPO_DESTINATION_PATH = \"code/mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Imports/Authenticate for GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW"
      },
      "source": [
        "if not USE_GCP_TPU:\n",
        "  %tensorflow_version 1.x\n",
        "  from google.colab import auth\n",
        "  print(\"Authorize for GCS:\")\n",
        "  auth.authenticate_user()\n",
        "  print(\"Authorize done\")\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization\n",
        "from mutformer.modeling import BertModel,BertModelModified\n",
        "from mutformer.run_pretraining import input_fn_builder, model_fn_builder\n",
        "\n",
        "if MODEL_TO_USE==\"orig\":\n",
        "  MODEL = BertModel\n",
        "  print(\"Using model: orig\")\n",
        "elif MODEL_TO_USE == \"withConv\":\n",
        "  MODEL = BertModelModified\n",
        "  print(\"Using model: withConv\")\n",
        "else:\n",
        "  raise Exception(\"The model specified was not one of the available models: [\\\"orig\\\", \\\"withConv\\\"].\")\n",
        "\n",
        "  \n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "#@markdown ###### Whether or not to write logs to a file\n",
        "DO_FILE_LOGGING = True #@param {type:\"boolean\"}\n",
        "if DO_FILE_LOGGING:\n",
        "  #@markdown ###### If using file logging, what path to write logs to\n",
        "  FILE_LOGGING_PATH = 'file_logging/spam.log' #@param {type:\"string\"}\n",
        "  if not os.path.exists(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1])):\n",
        "    os.makedirs(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1]))\n",
        "  fh = logging.FileHandler(FILE_LOGGING_PATH)\n",
        "  fh.setLevel(logging.INFO)\n",
        "  fh.setFormatter(formatter)\n",
        "  log.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "if USE_GCP_TPU:\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_NAME, zone=TPU_ZONE, project=GCE_PROJECT_NAME)\n",
        "  TPU_ADDRESS = tpu_cluster_resolver.get_master()\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "      log.info('TPU address is ' + TPU_ADDRESS)\n",
        "      # Upload credentials to TPU.\n",
        "      tf.contrib.cloud.configure_gcs(session)\n",
        "else:\n",
        "  if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    log.info(\"Using TPU runtime\")\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "    with tf.Session(TPU_ADDRESS) as session:\n",
        "      log.info('TPU address is ' + TPU_ADDRESS)\n",
        "      # Upload credentials to TPU.\n",
        "      with tf.gfile.Open('/content/adc.json', 'r') as f:\n",
        "        auth_info = json.load(f)\n",
        "      tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "      \n",
        "  else:\n",
        "    raise Exception('Not connected to TPU runtime, TPU required to run mutformer')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0TXw9GtCKz"
      },
      "source": [
        "#Auto Detect amount of train steps per epoch in the source data/Mount Drive if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYsYBUCJMTdz"
      },
      "source": [
        "#@markdown ###### if not USE_GCP_TPU and data was stored in drive, folder where the original data was stored (if data was stored in GCS or USE_GCP_TPU is true, leave this item blank)\n",
        "data_folder = \"/content/drive/My Drive/BERT pretraining/mutformer_pretraining_data\" #@param {type: \"string\"}\n",
        "\n",
        "if not USE_GCP_TPU and \"/content/drive\" in data_folder:\n",
        "  from google.colab import drive\n",
        "  !fusermount -u /content/drive\n",
        "  drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH = \"/content/drive/My Drive\"\n",
        "\n",
        "  data_path_train = drive_data_folder+\"/train.txt\" \n",
        "\n",
        "  lines = tf.gfile.Open(data_path_train).read().split(\"\\n\")\n",
        "  SEQUENCES_PER_EPOCH = len(lines)\n",
        "  STEPS_PER_EPOCH = int(SEQUENCES_PER_EPOCH/TRAIN_BATCH_SIZE)\n",
        "\n",
        "  print(\"sequences per epoch:\",SEQUENCES_PER_EPOCH, \"steps per epoch:\",STEPS_PER_EPOCH)\n",
        "else:\n",
        "  from tqdm import tqdm\n",
        "  def steps_getter(input_files):\n",
        "    tot_sequences = 0\n",
        "    for input_file in input_files:\n",
        "      print(\"reading:\",input_file)\n",
        "\n",
        "      d = tf.data.TFRecordDataset(input_file)\n",
        "\n",
        "      with tf.Session() as sess:\n",
        "        tot_sequences+=sess.run(d.reduce(0, lambda x,_: x+1))\n",
        "\n",
        "    return tot_sequences\n",
        "\n",
        "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "  got_data = False\n",
        "  while not got_data: ##will keep trying to access the data until available\n",
        "    for f in range(0,DATA_COPIES):\n",
        "        DATA_GCS_DIR_train = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR+\"/\"+str(f))\n",
        "        train_input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR_train,'*tfrecord'))\n",
        "        print(\"Using:\",train_input_files)\n",
        "        if len(train_input_files)>0:\n",
        "          got_data = True\n",
        "          try:\n",
        "            SEQUENCES_PER_EPOCH = steps_getter(train_input_files)\n",
        "            STEPS_PER_EPOCH = int(SEQUENCES_PER_EPOCH/TRAIN_BATCH_SIZE)\n",
        "            print(\"sequences per epoch:\",SEQUENCES_PER_EPOCH, \"steps per epoch:\",STEPS_PER_EPOCH)\n",
        "            break\n",
        "          except:\n",
        "            got_data=False\n",
        "    if got_data:\n",
        "      break\n",
        "    print(\"Could not find data, waiting for data generation...trying again in another \"+str(CHECK_DATA_EVERY_N_SECS)+\" seconds.\")\n",
        "    time.sleep(CHECK_DATA_EVERY_N_SECS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0ZQ6OJXjXMp"
      },
      "source": [
        "#Upload config to GCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8re3mRujW3c"
      },
      "source": [
        "bert_config[\"vocab_size\"] = len(vocab.split(\"\\n\"))\n",
        "\n",
        "with tf.gfile.Open(\"{}/config.json\".format(MODEL_DIR), \"w\") as fo:\n",
        "  json.dump(bert_config, fo, indent=2)\n",
        "\n",
        "!gsutil -m cp -r $MODEL_DIR gs://$BUCKET_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "# Run Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "source": [
        "import time\n",
        "\n",
        "operating_files = [\"available_indexes\",\"epoch\"]\n",
        "\n",
        "def download_tmp_files(operating_files): ##for downloading tmp files from drive or GCS\n",
        "  for op_file in operating_files:\n",
        "    if USE_GCP_TPU: ##If using GCP TPU, drive isn't available, so we need to store temporary files in GCS\n",
        "      cmd = \"gsutil -m cp -r gs://\"+BUCKET_NAME+\"/\"+TEMP_DIR+\"/\"+op_file+\".txt \"+TEMP_DIR+\"/\"+op_file+\".txt\"\n",
        "      !{cmd}\n",
        "    else:\n",
        "      shutil.copy(DRIVE_PATH+\"/\"+TEMP_DIR+\"/\"+op_file+\".txt\",TEMP_DIR+\"/\"+op_file+\".txt\")\n",
        "\n",
        "def upload_tmp_files(operating_files): ##for uploading tmp files to drive or GCS\n",
        "  for op_file in operating_files:\n",
        "    if USE_GCP_TPU: ##doing the same thing as above^^\n",
        "      cmd = \"gsutil -m cp -r \"+TEMP_DIR+\"/\"+op_file+\".txt gs://\"+BUCKET_NAME+\"/\"+TEMP_DIR+\"/\"+op_file+\".txt\"\n",
        "      !{cmd}\n",
        "    else:\n",
        "      shutil.copy(TEMP_DIR+\"/\"+op_file+\".txt\",DRIVE_PATH+\"/\"+TEMP_DIR+\"/\"+op_file+\".txt\")\n",
        "\n",
        "download_tmp_files(operating_files)\n",
        "\n",
        "if os.path.exists(TEMP_DIR+\"/epoch.txt\"): ##detect the current epoch\n",
        "  current_epoch = int(tf.gfile.Open(TEMP_DIR+\"/epoch.txt\").read())\n",
        "else:\n",
        "  current_epoch=0\n",
        "\n",
        "BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
        "GCS_LOGGING_DIR = \"{}/{}\".format(BUCKET_PATH, LOGGING_DIR+\"/\"+RUN_NAME)\n",
        "\n",
        "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"config.json\")\n",
        "\n",
        "while True: ##training loop\n",
        "  print(\"\\n\\n\\n\\n\\nEPOCH:\"+str(current_epoch)+\"\\n\\n\\n\\n\\n\\n\")\n",
        "  \n",
        "  got_data = False\n",
        "  while not got_data:\n",
        "    for f in range(0,DATA_COPIES): ##try to access any of the data bins\n",
        "      print(\"trying to access training data from saved sector number \"+str(f))\n",
        "      DATA_GCS_DIR_train = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR+\"/\"+str(f))\n",
        "      train_input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR_train,'*tfrecord'))\n",
        "      print(\"train_input_files:\",train_input_files)\n",
        "      if len(train_input_files)>0:\n",
        "        got_data = True\n",
        "        break\n",
        "      else:\n",
        "        current_available_indexes = tf.gfile.Open(TEMP_DIR+\"/available_indexes.txt\").read().split(\"\\n\")[:-1]\n",
        "        print(\"current:\",current_available_indexes)\n",
        "\n",
        "        new_inds = \"\"\n",
        "        for ind in current_available_indexes:\n",
        "          if int(ind) != f:\n",
        "            new_inds += ind +\"\\n\"\n",
        "        print(\"new_inds\",new_inds)\n",
        "        tf.gfile.Open(TEMP_DIR+\"/available_indexes.txt\",\"w+\").write(new_inds)\n",
        "    upload_tmp_files([\"available_indexes\"])\n",
        "    if not got_data:\n",
        "      time.sleep(CHECK_DATA_EVERY_N_SECS)\n",
        "        \n",
        "\n",
        "  INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "  try:\n",
        "    INIT_CHECKPOINT_STEP = INIT_CHECKPOINT.split(\"-\")[-1]\n",
        "    print(\"CURRENT STEP:\",INIT_CHECKPOINT_STEP)\n",
        "    if int(INIT_CHECKPOINT_STEP)>=PLANNED_TOTAL_STEPS: ##if reached planed total steps, stop\n",
        "      break\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "\n",
        "  log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "  log.info(\"Using {} data shards for training\".format(len(train_input_files)))\n",
        "  model_fn = model_fn_builder(\n",
        "      bert_config=config,\n",
        "      logging_dir=GCS_LOGGING_DIR,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      init_learning_rate=INIT_LEARNING_RATE,\n",
        "      decay_per_step=DECAY_PER_STEP,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=True,\n",
        "      use_one_hot_embeddings=True,\n",
        "      bert=MODEL)\n",
        "\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=BERT_GCS_DIR,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=True,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=TRAIN_BATCH_SIZE,\n",
        "      eval_batch_size=EVAL_BATCH_SIZE)\n",
        "    \n",
        "  train_input_fn = input_fn_builder(\n",
        "          input_files=train_input_files,\n",
        "          max_seq_length=MAX_SEQ_LENGTH,\n",
        "          max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "          is_training=True)\n",
        "  try:\n",
        "    estimator.train(input_fn=train_input_fn, steps=STEPS_PER_EPOCH)\n",
        "    current_epoch+=1\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  # For dynamic masking, a parallel data generation is used. This portion deletes the current data and \n",
        "  # updates the list of available data via a txt (to minimize interaction with GCS) so that the data \n",
        "  # generation algortihm can generate the data with different masking positions \n",
        "  cmd = \"gsutil -m rm -r \"+DATA_GCS_DIR_train\n",
        "  !{cmd}\n",
        "  current_available_indexes = tf.gfile.Open(TEMP_DIR+\"/available_indexes.txt\").read().split(\"\\n\")[:-1]\n",
        "  print(\"current:\",current_available_indexes)\n",
        "\n",
        "  new_inds = \"\"\n",
        "  for ind in current_available_indexes:\n",
        "    if int(ind) != f:\n",
        "      new_inds += ind +\"\\n\"\n",
        "  print(\"new_inds\",new_inds)\n",
        "  tf.gfile.Open(TEMP_DIR+\"/available_indexes.txt\",\"w+\").write(new_inds)\n",
        "  tf.gfile.Open(TEMP_DIR+\"/epoch.txt\",\"w+\").write(str(current_epoch))\n",
        "  upload_tmp_files(operating_files)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}