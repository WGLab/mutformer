{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mutformer run_pretraining-eval",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "source": [
        "#@markdown ### General Config\n",
        "MAX_SEQ_LENGTH =  1024 #@param {type:\"integer\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8 #@param {type:\"integer\"}\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "#@markdown ###### For if multiple models need to be evaluated at the same time: xxx is the placeholder for the individual model identifier (if only one is being evaluated xxx will only placehold for that single model)\n",
        "MODEL_NAME_FORMAT = \"bert_model_xxx\" #@param {type:\"string\"}\n",
        "LOGGING_DIR_NAME_FORMAT = \"bert_model_xxx_loss_spam\" #@param {type:\"string\"}\n",
        "PRETRAINING_DIR = \"pretraining_data_1024\" #@param {type:\"string\"}\n",
        "EVAL_DIR = \"eval_data_1024\" #@param {type:\"string\"}\n",
        "TESTING_DIR = \"testing_data_1024\" #@param {type:\"string\"}\n",
        "RUN_NAME = \"human_pretraining\" #@param {type:\"string\"}\n",
        "MAX_PREDICTIONS = 20 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Evaluation procedure config\n",
        "EVAL_TEST_BATCH_SIZE = 64 #@param {type:\"integer\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIVqP04jiFF1"
      },
      "source": [
        "#Clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SanOExwkiEC_"
      },
      "source": [
        "#@markdown ######where to clone the repo into (only value that it can't be is \"mutformer\"):\n",
        "REPO_DESTINATION_PATH = \"code/mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Imports/Authenticate for GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import auth\n",
        "\n",
        "print(\"Authorize for GCS:\")\n",
        "auth.authenticate_user()\n",
        "print(\"Authorize done\")\n",
        "\n",
        "print(\"current date/time:\",time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())))\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization\n",
        "from mutformer.modeling import BertModel,BertModelModified\n",
        "from mutformer.run_pretraining import input_fn_builder, model_fn_builder\n",
        "\n",
        "  \n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "log.handlers = []\n",
        "#@markdown ###### Whether or not to write logs to a file\n",
        "DO_FILE_LOGGING = False #@param {type:\"boolean\"}\n",
        "if DO_FILE_LOGGING:\n",
        "  #@markdown ###### If using file logging, what path to write logs to\n",
        "  FILE_LOGGING_PATH = '/content/drive/My Drive/spam.log' #@param {type:\"string\"}\n",
        "  fh = logging.FileHandler(FILE_LOGGING_PATH)\n",
        "  fh.setLevel(logging.INFO)\n",
        "  fh.setFormatter(formatter)\n",
        "  log.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  raise Exception('Not connected to TPU runtime, TPU required to run mutformer (evaluation will also run really slow without TPU)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzRqYyB-Mesv"
      },
      "source": [
        "#Auto Detect amount of train steps per epoch in the source data/Mount Drive if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYsYBUCJMTdz"
      },
      "source": [
        "#@markdown ###### if data was stored in drive, folder where the original data was stored (if data was stored in GCS, leave this item blank)\n",
        "data_folder = \"/content/drive/My Drive/BERT pretraining/mutformer_pretraining_data\" #@param {type: \"string\"}\n",
        "if \"/content/drive\" in data_folder:\n",
        "  from google.colab import drive\n",
        "  !fusermount -u /content/drive\n",
        "  drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH = \"/content/drive/My Drive\"\n",
        "\n",
        "  data_path_train = drive_data_folder+\"/train.txt\" \n",
        "\n",
        "  lines = open(data_path_train).read().split(\"\\n\")\n",
        "  SEQUENCES_PER_EPOCH = len(lines)\n",
        "  STEPS_PER_EPOCH = int(SEQUENCES_PER_EPOCH/TRAIN_BATCH_SIZE)\n",
        "\n",
        "  print(\"sequences per epoch:\",SEQUENCES_PER_EPOCH, \"steps per epoch:\",STEPS_PER_EPOCH)\n",
        "else:\n",
        "  from tqdm import tqdm\n",
        "  def steps_getter(input_files):\n",
        "    tot_sequences = 0\n",
        "    for input_file in input_files:\n",
        "      print(\"reading:\",input_file)\n",
        "\n",
        "      d = tf.data.TFRecordDataset(input_file)\n",
        "\n",
        "      with tf.Session() as sess:\n",
        "        tot_sequences+=sess.run(d.reduce(0, lambda x,_: x+1))\n",
        "\n",
        "    return tot_sequences\n",
        "\n",
        "  BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "  got_data = False\n",
        "  while not got_data: ##will keep trying to access the data until available\n",
        "    for f in range(0,DATA_COPIES):\n",
        "        DATA_GCS_DIR_train = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR+\"/\"+str(f))\n",
        "        train_input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR_train,'*tfrecord'))\n",
        "        print(\"Using:\",train_input_files)\n",
        "        if len(train_input_files)>0:\n",
        "          got_data = True\n",
        "          try:\n",
        "            SEQUENCES_PER_EPOCH = steps_getter(train_input_files)\n",
        "            STEPS_PER_EPOCH = int(SEQUENCES_PER_EPOCH/TRAIN_BATCH_SIZE)\n",
        "            print(\"sequences per epoch:\",SEQUENCES_PER_EPOCH, \"steps per epoch:\",STEPS_PER_EPOCH)\n",
        "            break\n",
        "          except:\n",
        "            got_data=False\n",
        "    if got_data:\n",
        "      break\n",
        "    print(\"Could not find data, waiting for data generation...trying again in another \"+str(1200)+\" seconds.\")\n",
        "    time.sleep(1200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhZV6JNh3Qxg"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3V5T3cT9-Bl"
      },
      "source": [
        "###Setting up Evaluation operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stkmJtg2tnyR"
      },
      "source": [
        "def reload_ckpt(model_dir,logging_dir,current_ckpt,model,data_dir):\n",
        "  BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, model_dir)\n",
        "\n",
        "  CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"config.json\")\n",
        "\n",
        "  INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "  print(\"init chkpt:\",INIT_CHECKPOINT)\n",
        "  print(\"current chkpt:\",current_ckpt)\n",
        "  if INIT_CHECKPOINT != current_ckpt:\n",
        "    config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "    test_input_files = tf.gfile.Glob(os.path.join(data_dir,'*tfrecord'))\n",
        "    log.info(\"Using {} data shards for testing\".format(len(test_input_files)))\n",
        "    model_fn = model_fn_builder(\n",
        "          bert_config=config,\n",
        "          logging_dir=logging_dir,\n",
        "          init_checkpoint=INIT_CHECKPOINT,\n",
        "          init_learning_rate=1,\n",
        "          decay_per_step=1,\n",
        "          num_warmup_steps=10,\n",
        "          use_tpu=True,\n",
        "          use_one_hot_embeddings=True,\n",
        "          bert=model)\n",
        "\n",
        "    \n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "    run_config = tf.contrib.tpu.RunConfig(\n",
        "        cluster=tpu_cluster_resolver,\n",
        "        model_dir=BERT_GCS_DIR,\n",
        "        save_checkpoints_steps=1,\n",
        "        tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "            iterations_per_loop=1,\n",
        "            num_shards=NUM_TPU_CORES,\n",
        "            per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "    estimator = tf.contrib.tpu.TPUEstimator(\n",
        "        use_tpu=True,\n",
        "        model_fn=model_fn,\n",
        "        config=run_config,\n",
        "        train_batch_size=1,\n",
        "        eval_batch_size=EVAL_TEST_BATCH_SIZE)\n",
        "    \n",
        "    test_input_fn = input_fn_builder(\n",
        "        input_files=test_input_files,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "        is_training=False)\n",
        "    return INIT_CHECKPOINT,estimator,test_input_fn,True\n",
        "  else:\n",
        "    return None,None,None,False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pAVF8hSXHVv"
      },
      "source": [
        "###Run Eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "source": [
        "import time\n",
        "BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "#@markdown ###### whether to evaluate on the test set or the dev set (\"test\" or \"dev\") (test set will only run once, dev set will run continuously)\n",
        "dataset = \"test\" #@param{type:\"string\"}\n",
        "\n",
        "if dataset==\"test\":\n",
        "  DATA_DIR = TESTING_DIR\n",
        "elif dataset==\"dev\":\n",
        "  DATA_DIR = EVAL_DIR\n",
        "else:\n",
        "  raise Exception(\"only datasets supported are dev and test\")\n",
        "\n",
        "models_to_evaluate = [\"modified\",\"orig\",\"large\",\"modified_medium\",\"modified_large\"] #@param #list of models to evaluate\n",
        "\n",
        "name2model = {      ##dictionary mapping model architecture to each model name\n",
        "    \"modified\":BertModelModified,\n",
        "    \"modified_medium\":BertModelModified,\n",
        "    \"modified_large\":BertModelModified,\n",
        "    \"orig\":BertModel,\n",
        "    \"large\":BertModel\n",
        "}\n",
        "\n",
        "def write_metrics(metrics,dir): ##evaluation metrics will be written into google drive to minimize interations with GCS\n",
        "  gs = metrics[\"global_step\"]\n",
        "  print(\"global step\",gs)\n",
        "\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "  tf.reset_default_graph()  \n",
        "  for key,value in metrics.items():\n",
        "    print(key,value)\n",
        "    x_scalar = tf.constant(value)\n",
        "    first_summary = tf.summary.scalar(name=key, tensor=x_scalar)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        writer = tf.summary.FileWriter(dir)\n",
        "        sess.run(init)\n",
        "        summary = sess.run(first_summary)\n",
        "        writer.add_summary(summary, gs)\n",
        "        writer.flush()\n",
        "        print('Done with writing the scalar summary')\n",
        "    time.sleep(1)\n",
        "  if not os.path.exists(DRIVE_PATH+\"/\"+dir):\n",
        "    os.makedirs(DRIVE_PATH+\"/\"+dir)\n",
        "  cmd = \"cp -r \\\"\"+dir+\"/.\\\" \\\"\"+DRIVE_PATH+\"/\"+dir+\"\\\"\"\n",
        "  !{cmd}\n",
        "current_ckpts = [\"N/A\" for i in range(len(models_to_evaluate))]\n",
        "\n",
        "total_metrics = {}\n",
        "\n",
        "while True:\n",
        "  for n,model in enumerate(models_to_evaluate):\n",
        "    MODEL_DIR = MODEL_NAME_FORMAT.replace(\"xxx\",model)\n",
        "    LOCAL_LOGGING_DIR = \"{}/{}\".format(LOGGING_DIR_NAME_FORMAT.replace(\"xxx\",model),RUN_NAME)\n",
        "    current_ckpt = current_ckpts[n]\n",
        "    current_ckpt,estimator,test_input_fn,new = reload_ckpt(MODEL_DIR,GCS_LOGGING_DIR,current_ckpt,name2model[model],\"{}/{}\".format(BUCKET_PATH, DATA_DIR))\n",
        "    current_ckpts[n] = current_ckpt\n",
        "    if new:\n",
        "      print(\"\\n\\nEVALUATING \"+model+\" MODEL\\n\\n\")\n",
        "      log.info(\"Using checkpoint: {}\".format(current_ckpt))\n",
        "      metrics = estimator.evaluate(input_fn=test_input_fn, steps=(TEST_STEPS if dataset==\"test\" else EVAL_STEPS))\n",
        "      if dataset == \"dev\":\n",
        "        write_metrics(metrics,LOCAL_LOGGING_DIR)\n",
        "      else:\n",
        "        total_metrics[LOCAL_LOGGING_DIR] = metrics\n",
        "\n",
        "  print(\"finished 1 eval loop\")\n",
        "  if dataset==\"test\":\n",
        "    break\n",
        "  time.sleep(600)\n",
        "if dataset == \"test\":\n",
        "  for logging_dir,metrics in total_metrics.items():\n",
        "    print(\"Printing metrics for:\",logging_dir,\"\\n\")\n",
        "    for key,metric in metrics.items():\n",
        "      print(key+\":\",metric)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}