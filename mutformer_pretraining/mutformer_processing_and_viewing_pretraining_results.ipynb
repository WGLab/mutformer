{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mutformer_processing_and_viewing_pretraining_results.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-tpQzwoZxiz"
      },
      "source": [
        "#Viewing Pretraining Curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gulatPJkTi-T"
      },
      "source": [
        "###Mount drive and Authenticate for GCP & Copy tfevents from GCS into drive (If there is new training data to download from GCS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC47pTHib75n"
      },
      "source": [
        "from google.colab import auth,drive\n",
        "print(\"Mount drive:\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "DRIVE_PATH=\"/content/drive/My Drive\"\n",
        "import os\n",
        "print(\"Authenticate for GCS:\")\n",
        "auth.authenticate_user()\n",
        "\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param{type:\"string\"}\n",
        "TFEVENTs_DESTINATION_PATH = \"/content/drive/My Drive\" #@param{type:\"string\"}\n",
        "\n",
        "runs = [\"bert_model_modified_loss_spam\",\n",
        "        \"bert_model_orig_loss_spam\",\n",
        "        \"bert_model_large_loss_spam\",\n",
        "        \"bert_model_modified_large_loss_spam\",\n",
        "        \"bert_model_modified_medium_loss_spam\"]\n",
        "\n",
        "for run in runs: ##This will copy all of the tfevent files from GCS into drive, and will also delete all of the files from GCS\n",
        "  cmd = \"gsutil -m cp -r \\\"gs://\"+BUCKET_NAME+\"/\"+run+\"\\\" \\\"\"+TFEVENTs_DESTINATION_PATH+\"\\\"\"\n",
        "  !{cmd}\n",
        "  cmd = \"gsutil -m rm -r \"+\"\\\"gs://\"+BUCKET_NAME+\"/\"+run+\"\\\"\"\n",
        "  !{cmd}\n",
        "\n",
        "\n",
        "  !gsutil -m cp -r $run gs://$BUCKET_NAME\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSs8ATHOhJd_"
      },
      "source": [
        "###Just mount drive (use this if you wish to only use existing tfevent files from drive)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnZidFzKyqvU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "DRIVE_PATH = \"/content/drive/My Drive\"\n",
        "TFEVENTs_DESTINATION_PATH = \"/content/drive/My Drive\" #@param{type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYQENqLQTuqM"
      },
      "source": [
        "###Obtain events from tfevents for pretraining and update graph data json\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ztjm3huM9Bx"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import defaultdict\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "\n",
        "runs = [\"bert_model_modified_loss_spam\",\n",
        "             \"bert_model_orig_loss_spam\",\n",
        "             \"bert_model_large_loss_spam\",\n",
        "             \"bert_model_modified_large_loss_spam\",\n",
        "             \"bert_model_modified_medium_loss_spam\"]\n",
        "\n",
        "#@markdown ######only the (skip)-th tfevent file will be considered (useful when amount of tfevent files is more than what is necessary for viewing curves) (if no tfevents should be skipped, set to 1)\n",
        "skip = 500 #@param {type:\"integer\"}\n",
        "\n",
        "def tabulate_events(dpath,out_dict = {}):\n",
        "    for dname in tqdm(os.listdir(dpath),\"converting to dict\"):\n",
        "        if \"tfevents\" not in dname:\n",
        "          continue\n",
        "        ea = EventAccumulator(os.path.join(dpath, dname)).Reload()\n",
        "        tags = ea.Tags()['scalars']\n",
        "\n",
        "        for tag in tags:\n",
        "            tag_values=[]\n",
        "            wall_time=[]\n",
        "            steps=[]\n",
        "\n",
        "            for event in ea.Scalars(tag):\n",
        "                 if event.step%skip==0 or \"eval\" in tag:\n",
        "                    try:\n",
        "                        out_dict[tag].append((event.step,event.value))\n",
        "                    except:\n",
        "                        out_dict[tag] = [(event.step,event.value)]\n",
        "    return out_dict\n",
        "\n",
        "graph_data = {}\n",
        "\n",
        "for run in runs: ##compiles the tfevent files into a json \n",
        "                 ##NOTE:DO NOT INTERRUPT THIS PORTION; IT MAY RESULT IN TRAINING GRAPH DATA BEING DELETED \n",
        "                 ##(if using google drive and data is deleted, you can always click manage versions in google drive and revert to a previous version on the corresonding .json file) \n",
        "  print(\"Processing tfevents for run:\",run)\n",
        "  data = {}\n",
        "  if os.path.exists(TFEVENTs_DESTINATION_PATH+\"/\"+run+\"_data.json\"):\n",
        "    data.update(json.load(open(TFEVENTs_DESTINATION_PATH+\"/\"+run+\"_data.json\")))\n",
        "  if os.path.exists(TFEVENTs_DESTINATION_PATH+\"/\"+run+\"/human_pretraining\"):\n",
        "    data = tabulate_events(TFEVENTs_DESTINATION_PATH+\"/\"+run+\"/human_pretraining\",data)\n",
        "    shutil.rmtree(TFEVENTs_DESTINATION_PATH+\"/\"+run)\n",
        "    os.makedirs(TFEVENTs_DESTINATION_PATH+\"/\"+run+\"/human_pretraining\")\n",
        "  for metric,datapoints in tqdm(data.items(),\"removing data overlaps\"): ##remove multiple values with the same step\n",
        "    new_dps = []\n",
        "    for i,point in enumerate(datapoints):\n",
        "      overlap = False\n",
        "      for j,point2 in enumerate(datapoints[i+1:]):\n",
        "        if point[0] == point2[0]:\n",
        "          overlap=True\n",
        "      if not overlap:\n",
        "        new_dps.append(point)\n",
        "    data[metric] = new_dps\n",
        "  with open(TFEVENTs_DESTINATION_PATH+\"/\"+run+\"_data.json\",\"w+\") as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "  for metric,datapoints in tqdm(data.items(),\"processing data\"):\n",
        "      try:\n",
        "          graph_data[metric][run].append(datapoints)\n",
        "      except:\n",
        "          try:\n",
        "              graph_data[metric][run] = [datapoints]\n",
        "          except:\n",
        "              graph_data[metric] = {}\n",
        "              graph_data[metric][run] = [datapoints]\n",
        "  \n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fmmecLvTp7S"
      },
      "source": [
        "###Plotting smoothed average curves using matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU2X7Cg_Jl9q",
        "cellView": "code"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "#@markdown range of the local average for viewing training graphs (to disable local averaging, set it to 0)\n",
        "avg_range = 100 #@param {type:\"integer\"}\n",
        "#@markdown local average range of calculating end slope\n",
        "slope_avg_range = 30 #@param {type:\"integer\"}\n",
        "#@markdown destination path for saving graphs (for no saving set to None)\n",
        "outfolder = DRIVE_PATH+\"/training graphs\" #@param\n",
        "if outfolder and not os.path.exists(outfolder):\n",
        "  os.makedirs(outfolder)\n",
        "for metric,runs in graph_data.items():\n",
        "    if \"rando\" in metric:\n",
        "      continue\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(metric+\" graph\")\n",
        "    plt.xlabel(\"steps\")\n",
        "    plt.ylabel(metric)\n",
        "    for run,run_data in runs.items():\n",
        "        run_data = run_data[0]\n",
        "        steps = []\n",
        "        values = []\n",
        "        nan = 0\n",
        "        for datapt in run_data:\n",
        "            if not math.isnan(datapt[1]):\n",
        "                values.append(datapt[1])\n",
        "                steps.append(datapt[0])\n",
        "            else:\n",
        "                nan+=1\n",
        "        print(\"nan values found in \"+run+\":\",nan)\n",
        "        values = [x for _, x in sorted(zip(steps, values), key=lambda pair: pair[0])][:-10]\n",
        "\n",
        "        if \"eval\" in metric or avg_range == 0:\n",
        "          avged_values = values\n",
        "        else:\n",
        "          avged_values = [sum(values[max(n-avg_range,0):min(n+avg_range,len(values))])/len(values[max(n-avg_range,0):min(n+avg_range,len(values))]) for n,value in enumerate(values)]\n",
        "        steps = sorted(steps)[:-10]\n",
        "        ##calculating slopes between two points\n",
        "        slopes = [((avged_values[n+slope_avg_range]-v)/(steps[n+slope_avg_range]-steps[n])) for n,v in enumerate(avged_values) if n<len(avged_values)-slope_avg_range]\n",
        "\n",
        "        plt.plot(steps,avged_values,label=run+\", ending slope=\"+str(slopes[-1]))\n",
        "                \n",
        "    plt.legend()\n",
        "    if outfolder:\n",
        "      plt.savefig(outfolder+\"/\"+metric.replace(\"/\",\"_\")+\".png\")\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbXhWZ7tTm76"
      },
      "source": [
        "###Tensorboard viewing (If you wish to use tensorboard instead)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvVNtrI1cH-H"
      },
      "source": [
        "LOGS_DIR = \"/content/drive/My Drive\" #@param (type:\"string\")\n",
        "LOGS_DIR = \"\\\"\"+LOGS_DIR+\"\\\"\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir $LOGS_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIThI2-ifqLC"
      },
      "source": [
        "#Copy a model checkpoint from GCS to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awQiti663ecW"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "!gsutil -m cp \\\n",
        "\"gs://theodore_jiang/bert_model_modified/model.ckpt-1332386.data-00000-of-00001\" \\ ##replace these with whatever files you are trying to copy from GCS\n",
        "\"gs://theodore_jiang/bert_model_modified/model.ckpt-1332386.index\" \\               ##alternatively you can copy the entire command from GCS as well\n",
        "\"gs://theodore_jiang/bert_model_modified/model.ckpt-1332386.meta\" \\\n",
        "\"/content/drive/My Drive/folder-to-copy-to\" ##destination folder"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}