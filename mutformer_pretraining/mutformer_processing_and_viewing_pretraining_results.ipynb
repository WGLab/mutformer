{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-tpQzwoZxiz"
      },
      "source": [
        "#Viewing Pretraining Curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gulatPJkTi-T"
      },
      "source": [
        "###Mount drive and Authenticate for GCP (always run this first)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NC47pTHib75n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "#@markdown Name of the GCS bucket to use (Make sure to set this to the name of your own GCS  bucket):\n",
        "BUCKET_NAME = \"\" #@param{type:\"string\"}\n",
        "#@markdown Folder in GCS where pretraining logs were written:\n",
        "TFEVENTs_SOURCE_FOLDER = \"mutformer_pretraining_data_30\" #@param {type:\"string\"}\n",
        "#@markdown Drive path where tfevent source folder is copied into (Note: do not store other files in the folder downloaded to this location, as it will mess with the tfevent processing script):\n",
        "TFEVENTs_DESTINATION_PATH = \"/content/drive/MyDrive\" #@param{type:\"string\"}\n",
        "\n",
        "def mount_drive(): ##mount drive function which uses link mounting instead of popup mounting so that a different drive other than that associated with the current google account can be used\n",
        "  if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "    os.makedirs(\"/content/drive/MyDrive\")\n",
        "    !sudo add-apt-repository -y ppa:alessandro-strada/ppa &> /dev/null ##install google-drive-ocamlfuse\n",
        "    !sudo apt-get update -qq &> /dev/null\n",
        "    !sudo apt -y install -qq google-drive-ocamlfuse &> /dev/null\n",
        "  if len(os.listdir(\"/content/drive/MyDrive\")) >0:\n",
        "    print(\"Drive already mounted.\")\n",
        "    return\n",
        "\n",
        "  if not os.path.exists(\"/content/driveauthlink.txt\") or not open(\"/content/driveauthlink.txt\").read(): ##if the auth link has not been generated, generate it\n",
        "    !google-drive-ocamlfuse &> /content/driveauthlink.txt\n",
        "  import re\n",
        "  link = re.findall(\"https://.+\",[x for x in open(\"/content/driveauthlink.txt\").read().split(\"\\n\") if x][-1])[0].split(\"\\\"\")[0]\n",
        "  print(f\"Click this link to authenticate for mounting drive: {link}\") ##print auth link\n",
        "  print(\"Waiting for valid athentication...\")\n",
        "  !sudo apt-get install -qq w3m &> /dev/null\n",
        "  !xdg-settings set default-web-browser w3m.desktop &> /dev/null\n",
        "\n",
        "  error = None\n",
        "  while True: ##while the google-drive-ocamlfuse mounting doesn't work (user hasn't athenticated yet), keep trying    \n",
        "    if os.path.exists(\"/content/drivemounterror.txt\"):\n",
        "      os.remove(\"/content/drivemounterror.txt\")\n",
        "    !google-drive-ocamlfuse /content/drive/MyDrive 2> \"/content/drivemounterror.txt\" 1> /dev/null\n",
        "    if error and open(\"/content/drivemounterror.txt\").read()!=error:\n",
        "      raise Exception(f\"Drive mount failed. Error: \\n\\n {open('/content/drivemounterror.txt').read()}\")\n",
        "    error = open(\"/content/drivemounterror.txt\").read()\n",
        "    no_error = not len(error) >0\n",
        "    if no_error:\n",
        "      if len(os.listdir(\"/content/drive/MyDrive\")) >0:\n",
        "        print(\"Drive mounted successfully!\")\n",
        "      else:\n",
        "        raise Exception(f\"Drive mount failed. Error: Unknown (likely Keyboard Interrupt)\")\n",
        "      break\n",
        "mount_drive()\n",
        "\n",
        "def authenticate_user(): ##authentication function that uses link authentication instead of popup\n",
        "  if os.path.exists(\"/content/.config/application_default_credentials.json\"): \n",
        "    return\n",
        "  print(\"Authorize for runtime GCS:\")\n",
        "  !gcloud auth login --no-launch-browser\n",
        "  print(\"Authorize for TPU GCS:\")\n",
        "  !gcloud auth application-default login  --no-launch-browser\n",
        "authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSs8ATHOhJd_"
      },
      "source": [
        "###Copy tfevents from GCS into drive (If there is new training data to download from GCS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnZidFzKyqvU"
      },
      "outputs": [],
      "source": [
        "##This will copy all of the tfevent files from GCS into drive, and will also delete all of the files from GCS\n",
        "cmd = \"gsutil -m cp -r \\\"gs://\"+BUCKET_NAME+\"/\"+TFEVENTs_SOURCE_FOLDER+\"\\\" \\\"\"+TFEVENTs_DESTINATION_PATH+\"\\\"\"\n",
        "!{cmd}\n",
        "cmd = \"gsutil -m rm -r \"+\"\\\"gs://\"+BUCKET_NAME+\"/\"+TFEVENTs_SOURCE_FOLDER+\"\\\"\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYQENqLQTuqM"
      },
      "source": [
        "###Obtain events from tfevents for pretraining and update graph data json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ztjm3huM9Bx",
        "outputId": "1278b6c4-f267-4312-a868-74c8901e38ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing tfevents for run: bert_model_embedded_mutformer_8L\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "removing data overlaps: 100%|██████████| 19/19 [00:02<00:00,  8.18it/s]\n",
            "processing data: 100%|██████████| 19/19 [00:00<00:00, 8333.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing tfevents for run: bert_model_modified_large_loss_spam\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "removing data overlaps: 100%|██████████| 11/11 [00:03<00:00,  3.00it/s]\n",
            "processing data: 100%|██████████| 11/11 [00:00<00:00, 49878.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing tfevents for run: bert_model_modified_loss_spam\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "removing data overlaps: 100%|██████████| 13/13 [00:04<00:00,  2.96it/s]\n",
            "processing data: 100%|██████████| 13/13 [00:00<00:00, 57577.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing tfevents for run: bert_model_modified_medium_loss_spam\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "removing data overlaps: 100%|██████████| 11/11 [00:05<00:00,  2.02it/s]\n",
            "processing data: 100%|██████████| 11/11 [00:00<00:00, 72657.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing tfevents for run: bert_model_large_loss_spam\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "removing data overlaps: 100%|██████████| 13/13 [00:04<00:00,  3.10it/s]\n",
            "processing data: 100%|██████████| 13/13 [00:00<00:00, 52784.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing tfevents for run: bert_model_orig_loss_spam\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "removing data overlaps: 100%|██████████| 13/13 [00:04<00:00,  2.62it/s]\n",
            "processing data: 100%|██████████| 13/13 [00:00<00:00, 74590.91it/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "from collections import defaultdict\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "\n",
        "\n",
        "#@markdown Only the (skip)-th tfevent file will be considered (useful when amount of tfevent files is more than what is necessary for viewing curves, so that data size doesn't get unnecessarily large) (if no tfevents should be skipped, set to 1)\n",
        "skip = 1 #@param {type:\"integer\"}\n",
        "\n",
        "tag_translator = {\n",
        "    \"eval_masked_lm_accuracy\":\"eval_accuracy\"\n",
        "}\n",
        "\n",
        "def tabulate_events(dpath,out_dict = {}):\n",
        "    for dname in tqdm(os.listdir(dpath),\"converting to dict\"):\n",
        "        if \"tfevents\" not in dname:\n",
        "          continue\n",
        "        ea = EventAccumulator(os.path.join(dpath, dname)).Reload()\n",
        "        tags = ea.Tags()['scalars']\n",
        "\n",
        "        for tag in tags:\n",
        "            tag_values=[]\n",
        "            wall_time=[]\n",
        "            steps=[]\n",
        "\n",
        "            for event in ea.Scalars(tag):\n",
        "              if event.step % skip==0 or \"eval\" in tag:\n",
        "                if tag in tag_translator.keys():\n",
        "                  tag = tag_translator[tag]\n",
        "                try:\n",
        "                    out_dict[tag].append((event.step,event.value))\n",
        "                except:\n",
        "                    out_dict[tag] = [(event.step,event.value)]\n",
        "    return out_dict\n",
        "\n",
        "graph_data = {}\n",
        "\n",
        "DESTINATION_PATH = f\"{TFEVENTs_DESTINATION_PATH}/{TFEVENTs_SOURCE_FOLDER}\"\n",
        "\n",
        "finished_runs = []\n",
        "\n",
        "for run in os.listdir(DESTINATION_PATH): ##compiles the tfevent files into a json and deletes the original tfevent files, since they clutter folders too much\n",
        "                                          ##NOTE:DO NOT INTERRUPT THIS PORTION; IT MAY RESULT IN TRAINING GRAPH DATA BEING DELETED \n",
        "  \n",
        "  data = {}\n",
        "  \n",
        "  run = run.replace(\"_data.json\",\"\")\n",
        "  if run in finished_runs:\n",
        "    continue\n",
        "  print(f\"\\nProcessing tfevents for run: {run}\\n\")\n",
        "  \n",
        "  \n",
        "  if os.path.exists(f\"{DESTINATION_PATH}/{run}_data.json\"):\n",
        "    data.update(json.load(open(f\"{DESTINATION_PATH}/{run}_data.json\")))\n",
        "  if os.path.isdir(DESTINATION_PATH+\"/\"+run):\n",
        "    data = tabulate_events(DESTINATION_PATH+\"/\"+run,data)\n",
        "    shutil.rmtree(DESTINATION_PATH+\"/\"+run)\n",
        "    os.makedirs(DESTINATION_PATH+\"/\"+run)\n",
        "  for metric,datapoints in tqdm(data.items(),\"removing data overlaps\"): ##remove multiple values that have the same step recorded in tfevents\n",
        "    new_dps = []\n",
        "    for i,point in enumerate(datapoints):\n",
        "      overlap = False\n",
        "      for j,point2 in enumerate(datapoints[i+1:]):\n",
        "        if point[0] == point2[0]:\n",
        "          overlap=True\n",
        "      if not overlap:\n",
        "        new_dps.append(point)\n",
        "    data[metric] = new_dps\n",
        "  with open(f\"{DESTINATION_PATH}/{run}_data.json\",\"w+\") as f:\n",
        "    json.dump(data, f)\n",
        "\n",
        "  for metric,datapoints in tqdm(data.items(),\"processing data\"):\n",
        "    \n",
        "    if metric in tag_translator.keys():\n",
        "      metric = tag_translator[metric]\n",
        "    metric = \"eval_\".join([thing.split(\"/\")[-1] for thing in metric.split(\"eval_\")])\n",
        "\n",
        "    try:\n",
        "        graph_data[metric][run].extend(datapoints)\n",
        "    except:\n",
        "        try:\n",
        "            graph_data[metric][run] = datapoints\n",
        "        except:\n",
        "            graph_data[metric] = {}\n",
        "            graph_data[metric][run] = datapoints\n",
        "  finished_runs.append(run)\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fmmecLvTp7S"
      },
      "source": [
        "###Plotting smoothed average curves using matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU2X7Cg_Jl9q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive\"\n",
        "\n",
        "#@markdown range of the local average for viewing train log graphs (not eval logs) (local averaging increases smoothness; to disable local averaging, set it to 0)\n",
        "avg_range = 100 #@param {type:\"integer\"}\n",
        "#@markdown local average range of calculating end slope\n",
        "slope_avg_range = 10 #@param {type:\"integer\"}\n",
        "#@markdown destination path for saving graphs (for no saving set to None)\n",
        "outfolder = DRIVE_PATH+\"/training graphs\" #@param\n",
        "if outfolder and not os.path.exists(outfolder):\n",
        "  os.makedirs(outfolder)\n",
        "\n",
        "\n",
        "for metric,runs in graph_data.items():\n",
        "    if \"rando\" in metric:\n",
        "      continue\n",
        "    fig=plt.figure(figsize=(20,10),dpi=120) ##figsize specifies aspect ratio, dpi specifies resolution\n",
        "    ax = fig.add_subplot()\n",
        "    colormap = plt.cm.nipy_spectral\n",
        "    colors = [colormap(i) for i in np.linspace(0, 1,len(graph_data))]\n",
        "    ax.set_prop_cycle('color', colors)\n",
        "\n",
        "    plt.title(metric+\" graph\")\n",
        "    plt.xlabel(\"steps\")\n",
        "    plt.ylabel(metric)\n",
        "    for run,run_data in runs.items():\n",
        "        if not run_data: continue\n",
        "        #if not \"distance\" in run:\n",
        "        #  continue\n",
        "\n",
        "        steps = []\n",
        "        values = []\n",
        "        for datapt in run_data:\n",
        "            \n",
        "            if not math.isnan(datapt[1]) and not datapt[0] in steps: ##check for nan and repeats\n",
        "                if datapt[0]==0:\n",
        "                  continue\n",
        "                values.append(datapt[1])\n",
        "                steps.append(datapt[0])\n",
        "        values = [x for _, x in sorted(zip(steps, values), key=lambda pair: pair[0])]\n",
        "\n",
        "        if \"eval\" in metric or \"learning_rate\" in metric or avg_range == 0: ## evaluation result logs\n",
        "          avged_values = values\n",
        "        else:\n",
        "          avged_values = [sum(values[max(n-avg_range,0):min(n+avg_range,len(values))])/len(values[max(n-avg_range,0):min(n+avg_range,len(values))]) for n,value in enumerate(values)]\n",
        "        steps = sorted(steps)\n",
        "        ##calculating slopes between two points\n",
        "        slopes = [((avged_values[n+slope_avg_range]-v)/(steps[n+slope_avg_range]-steps[n])) for n,v in enumerate(avged_values) if n<len(avged_values)-slope_avg_range]\n",
        "        if not slopes:\n",
        "          slopes = [0]\n",
        "\n",
        "        ax.plot(steps,avged_values,label=f\"{run}, ending slope={str(slopes[-1])}, ending value={avged_values[-1]}\")\n",
        "                \n",
        "    ax.legend()\n",
        "    if outfolder:\n",
        "      fig.savefig(outfolder+\"/\"+metric.replace(\"/\",\"_\")+\".png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbXhWZ7tTm76"
      },
      "source": [
        "###Tensorboard viewing (If you wish to use tensorboard instead)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvVNtrI1cH-H"
      },
      "outputs": [],
      "source": [
        "LOGS_DIR = \"/content/drive/My Drive\" #@param (type:\"string\")\n",
        "LOGS_DIR = \"\\\"\"+LOGS_DIR+\"\\\"\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir $LOGS_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dealing with trained models"
      ],
      "metadata": {
        "id": "v-bjrUlGjBEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Downgrade Python and Tensorflow \n",
        "\n",
        "(the default python version in Colab does not support Tensorflow 1.15)\n",
        "\n",
        "* **Note** that because the Python used in this notebook is not the default path, syntax highlighting most likely will not function."
      ],
      "metadata": {
        "id": "XtuK2efUOnCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. First, download and install Python version 3.7:"
      ],
      "metadata": {
        "id": "RRi21qSgOnYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py37_22.11.1-1-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py37\" --user"
      ],
      "metadata": {
        "id": "shmQAlu0On1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Then, reload the webpage (not restart runtime) to allow Colab to recognize the newly installed python\n",
        "####3. Finally, run the following commands to install tensorflow 1.15:"
      ],
      "metadata": {
        "id": "SytQYoySOoMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow==1.15"
      ],
      "metadata": {
        "id": "k9tN7wEwOoiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIThI2-ifqLC"
      },
      "source": [
        "##Copy a model checkpoint from GCS to drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awQiti663ecW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#@markdown checkpoint folder path:\n",
        "ckpt_folder = \"bert_model_modified\" #@param {type:\"string\"}\n",
        "#@markdown path to copy checkpoint folder into\n",
        "destination_folderp = \"/content/drive/My Drive/folder-to-copy-to\" #@param {type:\"string\"}\n",
        "\n",
        "ckpt_folderp = f\"gs://{BUCKET_NAME}/{ckpt_folder}\"\n",
        "\n",
        "latest_ckpt = tf.train.latest_checkpoint(ckpt_folderp)\n",
        "print(f\"Using checkpoint:{latest_ckpt}\")\n",
        "\n",
        "cmd = f\"gsutil -m cp {latest_ckpt}.data-00000-of-00001 {latest_ckpt}.index {latest_ckpt}.meta {destination_folderp}\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXCiVdYbm-vt"
      },
      "source": [
        "##Check trainable variable values in model checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used to check the values of the trainable variables in a checkpoint. Currently, reads in the latest checkpoint from the specified \"ckpt_folder,\" then saves the values of all the trainable variables into a text file on the local system titled \"ckpt_details.txt\""
      ],
      "metadata": {
        "id": "e8HjnMUGjye-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6Fgn1E3nFvx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python import pywrap_tensorflow\n",
        "import os\n",
        "\n",
        "#@markdown checkpoint folder path:\n",
        "ckpt_folder = \"\" #@param {type:\"string\"}\n",
        "\n",
        "ckpt_folderp = f\"gs://{BUCKET_NAME}/{ckpt_folder}\"\n",
        "latest_ckpt = tf.train.latest_checkpoint(ckpt_folderp)\n",
        "\n",
        "reader = pywrap_tensorflow.NewCheckpointReader(latest_ckpt)\n",
        "var_to_shape_map = reader.get_variable_to_shape_map()\n",
        "\n",
        "print(f\"\\n\\nReading from checkpoint: {latest_ckpt}\\n\\n\")\n",
        "\n",
        "with open(\"ckpt_details.txt\",\"w+\") as out:\n",
        "  for n,key in enumerate(var_to_shape_map):\n",
        "    print(f\"tensor number {n}: tensor_name: \", key)\n",
        "    out.write(f\"\\n{key}:\\n\")\n",
        "    out.write(\"\\n\".join([str(row) for row in reader.get_tensor(key)]))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "py37",
      "display_name": "Python 3.7"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}