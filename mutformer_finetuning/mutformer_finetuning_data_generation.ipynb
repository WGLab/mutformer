{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mutformer_finetuning_data_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "source": [
        "#@markdown ## General Config\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "#@markdown For the name of the data in GCS; for generating multiple sets of file i.e. for different sequence lengths, xxx is the placeholder for each identifier (if not generating multiple files, just put the plain name of the directory here)\n",
        "FINETUNING_DATA_DIR_format = \"MRPC_adding_preds_only_others_xxx\" #@param {type:\"string\"}\n",
        "#@markdown whether or not this script is being run in a GCP runtime (if more memory is required for large databases)\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Which task to perform: options are \"MRPC\" for paired sequence method, \"MRPC_w_preds\" for paired sequence method with external data, \"RE\" for single sequence method, or \"NER\" for single sequance per residue prediction (if you add more modes make sure to change the corresponding code segments)\n",
        "MODE = \"MRPC_w_preds\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA4ieYajd-Ht"
      },
      "source": [
        "#If using a GCP runtime to generate data (if database is large and more memory is needed), use these commands prior to running this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSUONYA5id9M"
      },
      "source": [
        "To ssh into the VM:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Make sure the port above matches the port below (in this case it's 8888)\n",
        "\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "\n",
        "(one command):sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "And then copy and paste the outputted link with \"locahost: ...\" into the colab connect to local runtime option\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1PvmBO8eR00"
      },
      "source": [
        "#Clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vKz_tKFeO0s"
      },
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git-all\n",
        "#@markdown ######where to clone the repo into (only value that it can't be is \"mutformer\"):\n",
        "REPO_DESTINATION_PATH = \"code/mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Authorize for GCS and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW"
      },
      "source": [
        "if not GCP_RUNTIME:\n",
        "  from google.colab import auth\n",
        "  print(\"Authorize for GCS:\")\n",
        "  auth.authenticate_user()\n",
        "  print(\"Authorize done\")\n",
        "\n",
        "  %tensorflow_version 1.x\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import importlib\n",
        "\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic\n",
        "from mutformer.modeling import BertModel,BertModelModified\n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor,MrpcWithPredsProcessor  ##change this part if you add more modes--\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor       ##--\n",
        "\n",
        "##reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_classifier,\n",
        "                  run_ner_for_pathogenic]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# create formatter and add it to the handlers\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "##Vocabulary for the model (B and J are markers for the beginning and ending of a protein sequence)\n",
        "vocab = \\\n",
        "'''[PAD]\n",
        "[UNK]\n",
        "[CLS]\n",
        "[SEP]\n",
        "[MASK]\n",
        "L\n",
        "S\n",
        "B\n",
        "J\n",
        "E\n",
        "A\n",
        "P\n",
        "T\n",
        "G\n",
        "V\n",
        "K\n",
        "R\n",
        "D\n",
        "Q\n",
        "I\n",
        "N\n",
        "F\n",
        "H\n",
        "Y\n",
        "C\n",
        "M\n",
        "W'''\n",
        "\n",
        "  \n",
        "with open(\"vocab.txt\", \"w\") as fo:\n",
        "  for token in vocab.split(\"\\n\"):\n",
        "    fo.write(token+\"\\n\")\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\": ##change this part if you added more modes\n",
        "  processor = MrpcProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"MRPC_w_preds\":\n",
        "  processor = MrpcWithPredsProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"RE\":\n",
        "  processor = REProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"NER\":\n",
        "  processor = NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\", \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=\"vocab.txt\", do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0TXw9GtCKz"
      },
      "source": [
        "#Specify Data location/Mount Drive if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYsYBUCJMTdz"
      },
      "source": [
        "if not GCP_RUNTIME:\n",
        "  from google.colab import drive,auth\n",
        "import os\n",
        "import shutil\n",
        "#@markdown input finetuning data folder (can be a GCS path if needed for large databases; cannot be a drive path if using GCP_RUNTIME): for generating multiple sets of file i.e. for different sequence lengths, xxx is the placeholder for each identifier (if not generating multiple files, just put the plain path of the directory here without xxx)\n",
        "data_folder_format = \"/content/drive/My Drive/BERT finetuning/MRPC/w_added_only_others_modified_bert_mrpc_512\" #@param {type: \"string\"}\n",
        "if \"/content/drive\" in data_folder_format:\n",
        "  !fusermount -u /content/drive\n",
        "  drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive', force_remount=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lXDH9WeQWGw"
      },
      "source": [
        "###General setup and definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUXoN_qYQZOA"
      },
      "source": [
        "#@markdown maximum batch size the training script can handle without OOM (must be divisible by NUM_TPU_CORES_WHEN_TESTING)\n",
        "MAX_BATCH_SIZE =  1024 #@param {type:\"integer\"}\n",
        "#@markdown if using PRECISE_TESTING, how many tpu cores will be used during testing (for colab runtimes, it's 8)\n",
        "NUM_TPU_CORES_WHEN_TESTING = 8 #@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "\n",
        "def generate_data(MAX_SEQ_LENGTH,\n",
        "                  DATA_GCS_DIR,\n",
        "                  ID,\n",
        "                  PRECISE_TESTING,\n",
        "                  USING_SHARDS,\n",
        "                  SHARD_SIZE):  \n",
        "  data_folder_current= data_folder_format.replace(\"xxx\",str(ID))\n",
        "\n",
        "  try:\n",
        "    print(\"\\nGenerating train set...\\n\")\n",
        "    if USING_SHARDS:\n",
        "      rd_rg = [0,SHARD_SIZE]\n",
        "      i=0\n",
        "    else:\n",
        "      rd_rg = None\n",
        "    while True:\n",
        "      train_examples = processor.get_train_examples(data_folder_current,read_range=rd_rg)\n",
        "      if len(train_examples) == 0:\n",
        "        break\n",
        "      train_file = os.path.join(DATA_GCS_DIR, \"train.tf_record\")\n",
        "      if USING_SHARDS:\n",
        "        train_file+=\"_\"+str(i)\n",
        "      script.file_based_convert_examples_to_features(\n",
        "          train_examples, label_list, MAX_SEQ_LENGTH, tokenizer, train_file)\n",
        "      if not USING_SHARDS:\n",
        "        break\n",
        "      else:\n",
        "        rd_rg = [pt+SHARD_SIZE for pt in rd_rg]\n",
        "        i+=1\n",
        "  except Exception as e:\n",
        "    print(\"training data generation failed. Error:\",e)\n",
        "\n",
        "  try:\n",
        "    print(\"\\nGenerating eval set...\\n\")\n",
        "    if USING_SHARDS:\n",
        "      rd_rg = [0,SHARD_SIZE]\n",
        "      i=0\n",
        "    else:\n",
        "      rd_rg = None\n",
        "    while True:\n",
        "      eval_examples = processor.get_dev_examples(data_folder_current,read_range=rd_rg)\n",
        "      if len(eval_examples) == 0:\n",
        "        break\n",
        "      eval_file = os.path.join(DATA_GCS_DIR, \"eval.tf_record\")\n",
        "      if USING_SHARDS:\n",
        "        eval_file+=\"_\"+str(i)\n",
        "      script.file_based_convert_examples_to_features(\n",
        "          eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer, eval_file)\n",
        "      if not USING_SHARDS:\n",
        "        break\n",
        "      else:\n",
        "        rd_rg = [pt+SHARD_SIZE for pt in rd_rg]\n",
        "        i+=1\n",
        "  except Exception as e:\n",
        "    print(\"eval data generation failed. Error:\",e)\n",
        "\n",
        "  try:\n",
        "    print(\"\\nGenerating test set...\\n\")\n",
        "    if USING_SHARDS:\n",
        "      rd_rg = [0,SHARD_SIZE]\n",
        "      i=0\n",
        "    else:\n",
        "      rd_rg = None\n",
        "    while True:\n",
        "      test_examples = processor.get_test_examples(data_folder_current,read_range=rd_rg)\n",
        "      if len(test_examples) == 0:\n",
        "        break\n",
        "      test_file = os.path.join(DATA_GCS_DIR, \"test.tf_record\")\n",
        "      if USING_SHARDS:\n",
        "        test_file+=\"_\"+str(i)\n",
        "      ## if using precise testing, the data will be split into two sets: \n",
        "      ## one set will be able to be predicted on the maximum possible batch \n",
        "      ## size, while the other will be predicted on a batch size of one, to \n",
        "      ##ensure the fastest prediction without leaving out any datapoints\n",
        "      if PRECISE_TESTING and len(test_examples)<SHARD_SIZE:\n",
        "        test_file_trailing = os.path.join(DATA_GCS_DIR, \"test_trailing.tf_record\")\n",
        "        def largest_mutiple_under_max(max,multiple_base):\n",
        "          return int(max/multiple_base)*multiple_base\n",
        "\n",
        "        split = largest_mutiple_under_max(len(test_examples),MAX_BATCH_SIZE)\n",
        "        test_examples_head = test_examples[:split]\n",
        "        test_examples_trailing = test_examples[split:]\n",
        "        script.file_based_convert_examples_to_features(\n",
        "            test_examples_head, label_list, MAX_SEQ_LENGTH, tokenizer, test_file)\n",
        "        script.file_based_convert_examples_to_features(\n",
        "            test_examples_trailing, label_list, MAX_SEQ_LENGTH, tokenizer, test_file_trailing)\n",
        "      else:\n",
        "        script.file_based_convert_examples_to_features(\n",
        "            test_examples, label_list, MAX_SEQ_LENGTH, tokenizer, test_file)\n",
        "      if not USING_SHARDS:\n",
        "        break\n",
        "      else:\n",
        "        rd_rg = [pt+SHARD_SIZE for pt in rd_rg]\n",
        "        i+=1\n",
        "  except Exception as e:\n",
        "    print(\"testing data generation failed. Error:\",e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TWmbWT5SJqg"
      },
      "source": [
        "###Varying sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "source": [
        "#@markdown list of maximum sequence lengths to generate data for\n",
        "lengths = [64,128,256,512,1024] #@param\n",
        "#@markdown whether or not to ensure all dataponts are predicted\n",
        "PRECISE_TESTING = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "for MAX_SEQ_LENGTH in lengths:\n",
        "  print(\"Generating data for seq length:\",MAX_SEQ_LENGTH)\n",
        "  DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, FINETUNING_DATA_DIR_format.replace(\"xxx\",str(MAX_SEQ_LENGTH)))\n",
        "  \n",
        "  generate_data(MAX_SEQ_LENGTH,\n",
        "                DATA_GCS_DIR,\n",
        "                MAX_SEQ_LENGTH,\n",
        "                PRECISE_TESTING,\n",
        "                USING_SHARDS,\n",
        "                SHARD_SIZE)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEOfXa4WiB2N"
      },
      "source": [
        "###Only one dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IytLW0VbgOZz"
      },
      "source": [
        "#@markdown maximum output data length (because using paired method, actual protein sequence length is half)\n",
        "MAX_SEQ_LENGTH = 512 #@param {type:\"integer\"}\n",
        "#@markdown whether or not to ensure all dataponts are predicted\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown whether or not to split the data processing into (for really large databases, since finetuning data typically isn't that large)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown if USING_SHARDS, what shard size to use (must be divisible by MAX_BATCH_SIZE)\n",
        "SHARD_SIZE = 1024000 #@param {type:\"integer\"}\n",
        "\n",
        "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, FINETUNING_DATA_DIR_format.replace(\"xxx\",str(MAX_SEQ_LENGTH)))\n",
        "\n",
        "generate_data(MAX_SEQ_LENGTH,\n",
        "              DATA_GCS_DIR,\n",
        "              \"\",\n",
        "              PRECISE_TESTING,\n",
        "              USING_SHARDS,\n",
        "              SHARD_SIZE)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY7yhjWJiH7i"
      },
      "source": [
        "###Varying identifiers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2uji0UfiHdy"
      },
      "source": [
        "#@markdown maximum batch size the training script can handle without OOM\n",
        "MAX_BATCH_SIZE =  1024 #@param {type:\"integer\"}\n",
        "#@markdown whether or not to ensure all dataponts are predicted\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown maximum output data length (because using paired method, actual protein sequence length is half)\n",
        "MAX_SEQ_LENGTH = 512 #@param {type:\"integer\"}\n",
        "#@markdown list of identifiers to generate data for\n",
        "identifiers = [\"a\",\"b\",\"c\"] #@param\n",
        "\n",
        "for id in identifiers:\n",
        "  DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, FINETUNING_DATA_DIR_format.replace(\"xxx\",id))\n",
        "  generate_data(MAX_SEQ_LENGTH,\n",
        "                DATA_GCS_DIR,\n",
        "                id,\n",
        "                PRECISE_TESTING,\n",
        "                USING_SHARDS,\n",
        "                SHARD_SIZE)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}