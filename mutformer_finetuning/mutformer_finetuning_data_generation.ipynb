{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5WVrOdzC3K1"
      },
      "source": [
        "#Finetuning Data Generation Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq31-sx6C-eM"
      },
      "source": [
        "This notebook processes tsv data and uploads the processed data to GCS to be used for finetuning MutFormer."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downgrade Python and Tensorflow \n",
        "\n",
        "(the default python version in Colab does not support Tensorflow 1.15)\n",
        "\n",
        "* **Note** that because the Python used in this notebook is not the default path, syntax highlighting most likely will not function."
      ],
      "metadata": {
        "id": "RE7RL-X21OzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. First, download and install Python version 3.7:"
      ],
      "metadata": {
        "id": "XUczf3xHRSJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py37_22.11.1-1-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py37\" --user"
      ],
      "metadata": {
        "id": "P_QFvdx0RTKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37774b80-6d4e-4bea-b183-9d46ecc52173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-04 21:18:55--  https://repo.anaconda.com/miniconda/Miniconda3-py37_22.11.1-1-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86308321 (82M) [application/x-sh]\n",
            "Saving to: ‘mini.sh’\n",
            "\n",
            "mini.sh             100%[===================>]  82.31M  94.9MB/s    in 0.9s    \n",
            "\n",
            "2023-04-04 21:18:56 (94.9 MB/s) - ‘mini.sh’ saved [86308321/86308321]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "\n",
            "Installing base environment...\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: ...working... failed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - jupyter\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    anyio-3.5.0                |   py37h06a4308_0         165 KB\n",
            "    argon2-cffi-21.3.0         |     pyhd3eb1b0_0          15 KB\n",
            "    argon2-cffi-bindings-21.2.0|   py37h7f8727e_0          33 KB\n",
            "    attrs-22.1.0               |   py37h06a4308_0          84 KB\n",
            "    babel-2.11.0               |   py37h06a4308_0         6.8 MB\n",
            "    backcall-0.2.0             |     pyhd3eb1b0_0          13 KB\n",
            "    beautifulsoup4-4.11.1      |   py37h06a4308_0         185 KB\n",
            "    bleach-4.1.0               |     pyhd3eb1b0_0         123 KB\n",
            "    ca-certificates-2023.01.10 |       h06a4308_0         120 KB\n",
            "    conda-23.1.0               |   py37h06a4308_0         937 KB\n",
            "    dbus-1.13.18               |       hb2f20db_0         504 KB\n",
            "    debugpy-1.5.1              |   py37h295c915_0         1.7 MB\n",
            "    decorator-5.1.1            |     pyhd3eb1b0_0          12 KB\n",
            "    defusedxml-0.7.1           |     pyhd3eb1b0_0          23 KB\n",
            "    entrypoints-0.4            |   py37h06a4308_0          16 KB\n",
            "    expat-2.4.9                |       h6a678d5_0         156 KB\n",
            "    fontconfig-2.14.1          |       h52c9d5c_1         281 KB\n",
            "    freetype-2.12.1            |       h4a9f257_0         626 KB\n",
            "    glib-2.69.1                |       he621ea3_2         1.9 MB\n",
            "    gst-plugins-base-1.14.1    |       h6a678d5_1         2.2 MB\n",
            "    gstreamer-1.14.1           |       h5eee18b_1         1.7 MB\n",
            "    icu-58.2                   |       he6710b0_3        10.5 MB\n",
            "    importlib_resources-5.2.0  |     pyhd3eb1b0_1          21 KB\n",
            "    ipykernel-6.15.2           |   py37h06a4308_0         189 KB\n",
            "    ipython-7.31.1             |   py37h06a4308_1        1002 KB\n",
            "    ipython_genutils-0.2.0     |     pyhd3eb1b0_1          27 KB\n",
            "    ipywidgets-7.6.5           |     pyhd3eb1b0_1         105 KB\n",
            "    jedi-0.18.1                |   py37h06a4308_1         980 KB\n",
            "    jinja2-3.1.2               |   py37h06a4308_0         209 KB\n",
            "    jpeg-9e                    |       h5eee18b_1         262 KB\n",
            "    json5-0.9.6                |     pyhd3eb1b0_0          21 KB\n",
            "    jsonschema-4.17.3          |   py37h06a4308_0         138 KB\n",
            "    jupyter-1.0.0              |   py37h06a4308_8           7 KB\n",
            "    jupyter_client-7.4.9       |   py37h06a4308_0         204 KB\n",
            "    jupyter_console-6.4.4      |   py37h06a4308_0          42 KB\n",
            "    jupyter_core-4.11.2        |   py37h06a4308_0          80 KB\n",
            "    jupyter_server-1.23.4      |   py37h06a4308_0         382 KB\n",
            "    jupyterlab-3.5.3           |   py37h06a4308_0         4.4 MB\n",
            "    jupyterlab_pygments-0.1.2  |             py_0           8 KB\n",
            "    jupyterlab_server-2.19.0   |   py37h06a4308_0          80 KB\n",
            "    jupyterlab_widgets-1.0.0   |     pyhd3eb1b0_1         109 KB\n",
            "    libpng-1.6.39              |       h5eee18b_0         304 KB\n",
            "    libsodium-1.0.18           |       h7b6447c_0         244 KB\n",
            "    libuuid-1.41.5             |       h5eee18b_0          27 KB\n",
            "    libxcb-1.15                |       h7f8727e_0         505 KB\n",
            "    libxml2-2.9.14             |       h74e7548_0         718 KB\n",
            "    markupsafe-2.1.1           |   py37h7f8727e_0          21 KB\n",
            "    matplotlib-inline-0.1.6    |   py37h06a4308_0          16 KB\n",
            "    mistune-0.8.4              |py37h14c3975_1001          54 KB\n",
            "    nbclassic-0.5.2            |   py37h06a4308_0         6.1 MB\n",
            "    nbclient-0.5.13            |   py37h06a4308_0          91 KB\n",
            "    nbconvert-6.4.4            |   py37h06a4308_0         493 KB\n",
            "    nbformat-5.7.0             |   py37h06a4308_0         133 KB\n",
            "    nest-asyncio-1.5.6         |   py37h06a4308_0          14 KB\n",
            "    notebook-6.5.2             |   py37h06a4308_0         508 KB\n",
            "    notebook-shim-0.2.2        |   py37h06a4308_0          22 KB\n",
            "    openssl-1.1.1t             |       h7f8727e_0         3.7 MB\n",
            "    packaging-22.0             |   py37h06a4308_0          68 KB\n",
            "    pandocfilters-1.5.0        |     pyhd3eb1b0_0          11 KB\n",
            "    parso-0.8.3                |     pyhd3eb1b0_0          70 KB\n",
            "    pcre-8.45                  |       h295c915_0         207 KB\n",
            "    pexpect-4.8.0              |     pyhd3eb1b0_3          53 KB\n",
            "    pickleshare-0.7.5          |  pyhd3eb1b0_1003          13 KB\n",
            "    pkgutil-resolve-name-1.3.10|   py37h06a4308_0           9 KB\n",
            "    prometheus_client-0.14.1   |   py37h06a4308_0          90 KB\n",
            "    prompt-toolkit-3.0.36      |   py37h06a4308_0         571 KB\n",
            "    prompt_toolkit-3.0.36      |       hd3eb1b0_0           5 KB\n",
            "    psutil-5.9.0               |   py37h5eee18b_0         328 KB\n",
            "    ptyprocess-0.7.0           |     pyhd3eb1b0_2          17 KB\n",
            "    pygments-2.11.2            |     pyhd3eb1b0_0         759 KB\n",
            "    pyqt-5.9.2                 |   py37h05f1152_2         4.5 MB\n",
            "    pyrsistent-0.18.0          |   py37heee7806_0          95 KB\n",
            "    python-dateutil-2.8.2      |     pyhd3eb1b0_0         233 KB\n",
            "    python-fastjsonschema-2.16.2|   py37h06a4308_0         230 KB\n",
            "    pytz-2022.7                |   py37h06a4308_0         207 KB\n",
            "    pyzmq-23.2.0               |   py37h6a678d5_0         438 KB\n",
            "    qt-5.9.7                   |       h5867ecd_1        68.5 MB\n",
            "    qtconsole-5.4.0            |   py37h06a4308_0         189 KB\n",
            "    qtpy-2.2.0                 |   py37h06a4308_0          84 KB\n",
            "    send2trash-1.8.0           |     pyhd3eb1b0_1          19 KB\n",
            "    sip-4.19.8                 |   py37hf484d3e_0         274 KB\n",
            "    sniffio-1.2.0              |   py37h06a4308_1          15 KB\n",
            "    soupsieve-2.3.2.post1      |   py37h06a4308_0          65 KB\n",
            "    terminado-0.17.1           |   py37h06a4308_0          31 KB\n",
            "    testpath-0.6.0             |   py37h06a4308_0          85 KB\n",
            "    tomli-2.0.1                |   py37h06a4308_0          24 KB\n",
            "    tornado-6.2                |   py37h5eee18b_0         584 KB\n",
            "    traitlets-5.7.1            |   py37h06a4308_0         199 KB\n",
            "    typing-extensions-4.4.0    |   py37h06a4308_0           8 KB\n",
            "    wcwidth-0.2.5              |     pyhd3eb1b0_0          26 KB\n",
            "    webencodings-0.5.1         |           py37_1          19 KB\n",
            "    websocket-client-0.58.0    |   py37h06a4308_4          66 KB\n",
            "    widgetsnbextension-3.5.2   |   py37h06a4308_0         645 KB\n",
            "    zeromq-4.3.4               |       h2531618_0         331 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       127.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  anyio              pkgs/main/linux-64::anyio-3.5.0-py37h06a4308_0 \n",
            "  argon2-cffi        pkgs/main/noarch::argon2-cffi-21.3.0-pyhd3eb1b0_0 \n",
            "  argon2-cffi-bindi~ pkgs/main/linux-64::argon2-cffi-bindings-21.2.0-py37h7f8727e_0 \n",
            "  attrs              pkgs/main/linux-64::attrs-22.1.0-py37h06a4308_0 \n",
            "  babel              pkgs/main/linux-64::babel-2.11.0-py37h06a4308_0 \n",
            "  backcall           pkgs/main/noarch::backcall-0.2.0-pyhd3eb1b0_0 \n",
            "  beautifulsoup4     pkgs/main/linux-64::beautifulsoup4-4.11.1-py37h06a4308_0 \n",
            "  bleach             pkgs/main/noarch::bleach-4.1.0-pyhd3eb1b0_0 \n",
            "  dbus               pkgs/main/linux-64::dbus-1.13.18-hb2f20db_0 \n",
            "  debugpy            pkgs/main/linux-64::debugpy-1.5.1-py37h295c915_0 \n",
            "  decorator          pkgs/main/noarch::decorator-5.1.1-pyhd3eb1b0_0 \n",
            "  defusedxml         pkgs/main/noarch::defusedxml-0.7.1-pyhd3eb1b0_0 \n",
            "  entrypoints        pkgs/main/linux-64::entrypoints-0.4-py37h06a4308_0 \n",
            "  expat              pkgs/main/linux-64::expat-2.4.9-h6a678d5_0 \n",
            "  fontconfig         pkgs/main/linux-64::fontconfig-2.14.1-h52c9d5c_1 \n",
            "  freetype           pkgs/main/linux-64::freetype-2.12.1-h4a9f257_0 \n",
            "  glib               pkgs/main/linux-64::glib-2.69.1-he621ea3_2 \n",
            "  gst-plugins-base   pkgs/main/linux-64::gst-plugins-base-1.14.1-h6a678d5_1 \n",
            "  gstreamer          pkgs/main/linux-64::gstreamer-1.14.1-h5eee18b_1 \n",
            "  icu                pkgs/main/linux-64::icu-58.2-he6710b0_3 \n",
            "  importlib_resourc~ pkgs/main/noarch::importlib_resources-5.2.0-pyhd3eb1b0_1 \n",
            "  ipykernel          pkgs/main/linux-64::ipykernel-6.15.2-py37h06a4308_0 \n",
            "  ipython            pkgs/main/linux-64::ipython-7.31.1-py37h06a4308_1 \n",
            "  ipython_genutils   pkgs/main/noarch::ipython_genutils-0.2.0-pyhd3eb1b0_1 \n",
            "  ipywidgets         pkgs/main/noarch::ipywidgets-7.6.5-pyhd3eb1b0_1 \n",
            "  jedi               pkgs/main/linux-64::jedi-0.18.1-py37h06a4308_1 \n",
            "  jinja2             pkgs/main/linux-64::jinja2-3.1.2-py37h06a4308_0 \n",
            "  jpeg               pkgs/main/linux-64::jpeg-9e-h5eee18b_1 \n",
            "  json5              pkgs/main/noarch::json5-0.9.6-pyhd3eb1b0_0 \n",
            "  jsonschema         pkgs/main/linux-64::jsonschema-4.17.3-py37h06a4308_0 \n",
            "  jupyter            pkgs/main/linux-64::jupyter-1.0.0-py37h06a4308_8 \n",
            "  jupyter_client     pkgs/main/linux-64::jupyter_client-7.4.9-py37h06a4308_0 \n",
            "  jupyter_console    pkgs/main/linux-64::jupyter_console-6.4.4-py37h06a4308_0 \n",
            "  jupyter_core       pkgs/main/linux-64::jupyter_core-4.11.2-py37h06a4308_0 \n",
            "  jupyter_server     pkgs/main/linux-64::jupyter_server-1.23.4-py37h06a4308_0 \n",
            "  jupyterlab         pkgs/main/linux-64::jupyterlab-3.5.3-py37h06a4308_0 \n",
            "  jupyterlab_pygmen~ pkgs/main/noarch::jupyterlab_pygments-0.1.2-py_0 \n",
            "  jupyterlab_server  pkgs/main/linux-64::jupyterlab_server-2.19.0-py37h06a4308_0 \n",
            "  jupyterlab_widgets pkgs/main/noarch::jupyterlab_widgets-1.0.0-pyhd3eb1b0_1 \n",
            "  libpng             pkgs/main/linux-64::libpng-1.6.39-h5eee18b_0 \n",
            "  libsodium          pkgs/main/linux-64::libsodium-1.0.18-h7b6447c_0 \n",
            "  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0 \n",
            "  libxcb             pkgs/main/linux-64::libxcb-1.15-h7f8727e_0 \n",
            "  libxml2            pkgs/main/linux-64::libxml2-2.9.14-h74e7548_0 \n",
            "  markupsafe         pkgs/main/linux-64::markupsafe-2.1.1-py37h7f8727e_0 \n",
            "  matplotlib-inline  pkgs/main/linux-64::matplotlib-inline-0.1.6-py37h06a4308_0 \n",
            "  mistune            pkgs/main/linux-64::mistune-0.8.4-py37h14c3975_1001 \n",
            "  nbclassic          pkgs/main/linux-64::nbclassic-0.5.2-py37h06a4308_0 \n",
            "  nbclient           pkgs/main/linux-64::nbclient-0.5.13-py37h06a4308_0 \n",
            "  nbconvert          pkgs/main/linux-64::nbconvert-6.4.4-py37h06a4308_0 \n",
            "  nbformat           pkgs/main/linux-64::nbformat-5.7.0-py37h06a4308_0 \n",
            "  nest-asyncio       pkgs/main/linux-64::nest-asyncio-1.5.6-py37h06a4308_0 \n",
            "  notebook           pkgs/main/linux-64::notebook-6.5.2-py37h06a4308_0 \n",
            "  notebook-shim      pkgs/main/linux-64::notebook-shim-0.2.2-py37h06a4308_0 \n",
            "  packaging          pkgs/main/linux-64::packaging-22.0-py37h06a4308_0 \n",
            "  pandocfilters      pkgs/main/noarch::pandocfilters-1.5.0-pyhd3eb1b0_0 \n",
            "  parso              pkgs/main/noarch::parso-0.8.3-pyhd3eb1b0_0 \n",
            "  pcre               pkgs/main/linux-64::pcre-8.45-h295c915_0 \n",
            "  pexpect            pkgs/main/noarch::pexpect-4.8.0-pyhd3eb1b0_3 \n",
            "  pickleshare        pkgs/main/noarch::pickleshare-0.7.5-pyhd3eb1b0_1003 \n",
            "  pkgutil-resolve-n~ pkgs/main/linux-64::pkgutil-resolve-name-1.3.10-py37h06a4308_0 \n",
            "  prometheus_client  pkgs/main/linux-64::prometheus_client-0.14.1-py37h06a4308_0 \n",
            "  prompt-toolkit     pkgs/main/linux-64::prompt-toolkit-3.0.36-py37h06a4308_0 \n",
            "  prompt_toolkit     pkgs/main/noarch::prompt_toolkit-3.0.36-hd3eb1b0_0 \n",
            "  psutil             pkgs/main/linux-64::psutil-5.9.0-py37h5eee18b_0 \n",
            "  ptyprocess         pkgs/main/noarch::ptyprocess-0.7.0-pyhd3eb1b0_2 \n",
            "  pygments           pkgs/main/noarch::pygments-2.11.2-pyhd3eb1b0_0 \n",
            "  pyqt               pkgs/main/linux-64::pyqt-5.9.2-py37h05f1152_2 \n",
            "  pyrsistent         pkgs/main/linux-64::pyrsistent-0.18.0-py37heee7806_0 \n",
            "  python-dateutil    pkgs/main/noarch::python-dateutil-2.8.2-pyhd3eb1b0_0 \n",
            "  python-fastjsonsc~ pkgs/main/linux-64::python-fastjsonschema-2.16.2-py37h06a4308_0 \n",
            "  pytz               pkgs/main/linux-64::pytz-2022.7-py37h06a4308_0 \n",
            "  pyzmq              pkgs/main/linux-64::pyzmq-23.2.0-py37h6a678d5_0 \n",
            "  qt                 pkgs/main/linux-64::qt-5.9.7-h5867ecd_1 \n",
            "  qtconsole          pkgs/main/linux-64::qtconsole-5.4.0-py37h06a4308_0 \n",
            "  qtpy               pkgs/main/linux-64::qtpy-2.2.0-py37h06a4308_0 \n",
            "  send2trash         pkgs/main/noarch::send2trash-1.8.0-pyhd3eb1b0_1 \n",
            "  sip                pkgs/main/linux-64::sip-4.19.8-py37hf484d3e_0 \n",
            "  sniffio            pkgs/main/linux-64::sniffio-1.2.0-py37h06a4308_1 \n",
            "  soupsieve          pkgs/main/linux-64::soupsieve-2.3.2.post1-py37h06a4308_0 \n",
            "  terminado          pkgs/main/linux-64::terminado-0.17.1-py37h06a4308_0 \n",
            "  testpath           pkgs/main/linux-64::testpath-0.6.0-py37h06a4308_0 \n",
            "  tomli              pkgs/main/linux-64::tomli-2.0.1-py37h06a4308_0 \n",
            "  tornado            pkgs/main/linux-64::tornado-6.2-py37h5eee18b_0 \n",
            "  traitlets          pkgs/main/linux-64::traitlets-5.7.1-py37h06a4308_0 \n",
            "  typing-extensions  pkgs/main/linux-64::typing-extensions-4.4.0-py37h06a4308_0 \n",
            "  wcwidth            pkgs/main/noarch::wcwidth-0.2.5-pyhd3eb1b0_0 \n",
            "  webencodings       pkgs/main/linux-64::webencodings-0.5.1-py37_1 \n",
            "  websocket-client   pkgs/main/linux-64::websocket-client-0.58.0-py37h06a4308_4 \n",
            "  widgetsnbextension pkgs/main/linux-64::widgetsnbextension-3.5.2-py37h06a4308_0 \n",
            "  zeromq             pkgs/main/linux-64::zeromq-4.3.4-h2531618_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                     2022.10.11-h06a4308_0 --> 2023.01.10-h06a4308_0 \n",
            "  conda                              22.11.1-py37h06a4308_4 --> 23.1.0-py37h06a4308_0 \n",
            "  openssl                                 1.1.1s-h7f8727e_0 --> 1.1.1t-h7f8727e_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - google-colab\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    aiohttp-3.8.1              |   py37h540881e_1         561 KB  conda-forge\n",
            "    aiosignal-1.3.1            |     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    async-timeout-4.0.2        |     pyhd8ed1ab_0           9 KB  conda-forge\n",
            "    asynctest-0.13.0           |             py_0          24 KB  conda-forge\n",
            "    ca-certificates-2022.12.7  |       ha878542_0         143 KB  conda-forge\n",
            "    cachetools-5.3.0           |     pyhd8ed1ab_0          14 KB  conda-forge\n",
            "    certifi-2022.12.7          |     pyhd8ed1ab_0         147 KB  conda-forge\n",
            "    frozenlist-1.3.3           |   py37h5eee18b_0          44 KB\n",
            "    google-auth-2.17.1         |     pyh1a96a4e_0          97 KB  conda-forge\n",
            "    google-colab-1.0.0         |     pyh44b312d_0          77 KB  conda-forge\n",
            "    libblas-3.9.0              |15_linux64_openblas          12 KB  conda-forge\n",
            "    libcblas-3.9.0             |15_linux64_openblas          12 KB  conda-forge\n",
            "    libgfortran-ng-12.2.0      |      h69a702a_19          22 KB  conda-forge\n",
            "    libgfortran5-12.2.0        |      h337968e_19         1.8 MB  conda-forge\n",
            "    liblapack-3.9.0            |15_linux64_openblas          12 KB  conda-forge\n",
            "    libopenblas-0.3.20         |pthreads_h78a6416_0        10.1 MB  conda-forge\n",
            "    multidict-6.0.2            |   py37h5eee18b_0          47 KB\n",
            "    numpy-1.21.6               |   py37h976b520_0         6.1 MB  conda-forge\n",
            "    pandas-1.2.3               |   py37hdc94413_0        11.8 MB  conda-forge\n",
            "    portpicker-1.5.2           |     pyhd8ed1ab_0          17 KB  conda-forge\n",
            "    pyasn1-0.4.8               |             py_0          53 KB  conda-forge\n",
            "    pyasn1-modules-0.2.7       |             py_0          60 KB  conda-forge\n",
            "    python_abi-3.7             |          2_cp37m           4 KB  conda-forge\n",
            "    pyu2f-0.1.5                |     pyhd8ed1ab_0          31 KB  conda-forge\n",
            "    rsa-4.9                    |     pyhd8ed1ab_0          29 KB  conda-forge\n",
            "    setuptools-59.8.0          |   py37h89c1867_1         1.0 MB  conda-forge\n",
            "    yarl-1.7.2                 |   py37h540881e_2         132 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        32.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  aiohttp            conda-forge/linux-64::aiohttp-3.8.1-py37h540881e_1 \n",
            "  aiosignal          conda-forge/noarch::aiosignal-1.3.1-pyhd8ed1ab_0 \n",
            "  async-timeout      conda-forge/noarch::async-timeout-4.0.2-pyhd8ed1ab_0 \n",
            "  asynctest          conda-forge/noarch::asynctest-0.13.0-py_0 \n",
            "  cachetools         conda-forge/noarch::cachetools-5.3.0-pyhd8ed1ab_0 \n",
            "  frozenlist         pkgs/main/linux-64::frozenlist-1.3.3-py37h5eee18b_0 \n",
            "  google-auth        conda-forge/noarch::google-auth-2.17.1-pyh1a96a4e_0 \n",
            "  google-colab       conda-forge/noarch::google-colab-1.0.0-pyh44b312d_0 \n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-15_linux64_openblas \n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-15_linux64_openblas \n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-12.2.0-h69a702a_19 \n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-12.2.0-h337968e_19 \n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-15_linux64_openblas \n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.20-pthreads_h78a6416_0 \n",
            "  multidict          pkgs/main/linux-64::multidict-6.0.2-py37h5eee18b_0 \n",
            "  numpy              conda-forge/linux-64::numpy-1.21.6-py37h976b520_0 \n",
            "  pandas             conda-forge/linux-64::pandas-1.2.3-py37hdc94413_0 \n",
            "  portpicker         conda-forge/noarch::portpicker-1.5.2-pyhd8ed1ab_0 \n",
            "  pyasn1             conda-forge/noarch::pyasn1-0.4.8-py_0 \n",
            "  pyasn1-modules     conda-forge/noarch::pyasn1-modules-0.2.7-py_0 \n",
            "  python_abi         conda-forge/linux-64::python_abi-3.7-2_cp37m \n",
            "  pyu2f              conda-forge/noarch::pyu2f-0.1.5-pyhd8ed1ab_0 \n",
            "  rsa                conda-forge/noarch::rsa-4.9-pyhd8ed1ab_0 \n",
            "  yarl               conda-forge/linux-64::yarl-1.7.2-py37h540881e_2 \n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2022.12.7-ha878542_0 \n",
            "  certifi            pkgs/main/linux-64::certifi-2022.12.7~ --> conda-forge/noarch::certifi-2022.12.7-pyhd8ed1ab_0 \n",
            "  setuptools         pkgs/main::setuptools-65.5.0-py37h06a~ --> conda-forge::setuptools-59.8.0-py37h89c1867_1 \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Then, reload the webpage (not restart runtime) to allow Colab to recognize the newly installed python\n",
        "####3. Finally, run the following commands to install tensorflow 1.15:"
      ],
      "metadata": {
        "id": "2oAnf7kcRUAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow==1.15\n",
        "!python3 -m pip install protobuf==3.20.1"
      ],
      "metadata": {
        "id": "bOlnFkJ11OEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55a638f6-5695-4d24-e754-3ad71dff8143"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-pasta>=0.1.6\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf>=3.6.1\n",
            "  Downloading protobuf-4.22.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.53.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.16.0)\n",
            "Collecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.4/503.4 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.37.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.21.6)\n",
            "Collecting keras-preprocessing>=1.0.5\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=0.7.0\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py\n",
            "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (59.8.0)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=7a97710aaaf145fc321c0903f2564ef28a7f408a26c6fa25631a23a5205b42e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/87/6f/3f34218ef184368cec9ee65bdfd65baf117811f0a0ce1263ff\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, wrapt, werkzeug, termcolor, protobuf, opt-einsum, keras-preprocessing, h5py, grpcio, google-pasta, gast, astor, absl-py, markdown, keras-applications, tensorboard, tensorflow\n",
            "Successfully installed absl-py-1.4.0 astor-0.8.1 gast-0.2.2 google-pasta-0.2.0 grpcio-1.53.0 h5py-3.8.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.4.3 opt-einsum-3.3.0 protobuf-4.22.1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 termcolor-2.2.0 werkzeug-2.2.3 wrapt-1.15.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20.1\n",
            "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.22.1\n",
            "    Uninstalling protobuf-4.22.1:\n",
            "      Successfully uninstalled protobuf-4.22.1\n",
            "Successfully installed protobuf-3.20.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings/Mount Drive if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "outputs": [],
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown Whether or not this script is being run in a GCP runtime (if more memory is required for large databases)\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "#@markdown Which mode to use (a different mode means a different finetuning task): options are:\n",
        "#@markdown * \"MRPC\" - paired sequence method\n",
        "#@markdown * \"MRPC_w_ex_data\" - paired sequence method with external data\n",
        "#@markdown * \"RE\" - single sequence method\n",
        "#@markdown * \"NER\" - single sequence per residue prediction\n",
        "#@markdown \n",
        "#@markdown You can add more modes by creating a new processor and/or a new model_fn inside of the \"mutformer_model_code\" folder downloaded from github, then changing the corresponding code snippets in the code segment named \"Authorize for GCS, Imports, and General Setup\" (also edit the dropdown below).\n",
        "MODE = \"MRPC_w_ex_data\" #@param   [\"MRPC_w_ex_data\", \"MRPC\", \"RE\", \"NER\"]   {type:\"string\"} \n",
        "            ####      ^^^^^ dropdown list for all modes ^^^^^\n",
        "\n",
        "#@markdown Name of the GCS bucket to use (Make sure to set this to the name of your own GCS  bucket):\n",
        "BUCKET_NAME = \"\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ## IO Config\n",
        "#@markdown Input finetuning data folder: data will be read from here to be processed and uploaded to GCS (can be a drive path, or a GCS path if needed for large databases; must be a GCS path if using GCP_RUNTIME):\n",
        "#@markdown \n",
        "#@markdown * For processing multiple sets i.e. for multiple sequence lengths, simply store these sets into separate subfolders inside of the folder listed below, with each subfolder being named as specified in the following section.\n",
        "#@markdown \n",
        "#@markdown * For processing a single set, this folder should directly contain one dataset.\n",
        "#@markdown\n",
        "INPUT_DATA_DIR = \"\" #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "if not GCP_RUNTIME:                    ##if INPUT_DATA_DIR is a drive path,\n",
        "  if \"/content/drive\" in INPUT_DATA_DIR:   ##mount google drive\n",
        "    from google.colab import drive\n",
        "    if GCP_RUNTIME:\n",
        "      raise Exception(\"if GCP_RUNTIME, a GCS path must be used, since Google's cloud TPUs can only communicate with GCS and not drive\")\n",
        "    !fusermount -u /content/drive\n",
        "    drive.flush_and_unmount()\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "#@markdown Name of the folder in GCS to put processed data into: \n",
        "#@markdown * For generating multiple datasets i.e. for different sequence lengths, they will be written as individual subfolders inside of this folder.\n",
        "OUTPUT_DATA_DIR = \"all_snp_prediction_data_loaded\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "DATA_INFO = {      ##dictionary that will be uploaded alongside \n",
        "    \"mode\":MODE    ##each dataset to indicate its parameters\n",
        "}\n",
        "\n",
        "\n",
        "#### Vocabulary for the model (MutFormer uses the vocabulary below) ([PAD]\n",
        "#### [UNK],[CLS],[SEP], and [MASK] are necessary default tokens; B and J\n",
        "#### are markers for the beginning and ending of a protein sequence,\n",
        "#### respectively; the rest are all amino acids possible, ranked \n",
        "#### approximately by frequency of occurence in human population)\n",
        "#### vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
        "vocab = \\\n",
        "'''[PAD]\n",
        "[UNK]\n",
        "[CLS]\n",
        "[SEP]\n",
        "[MASK]\n",
        "L\n",
        "S\n",
        "B\n",
        "J\n",
        "E\n",
        "A\n",
        "P\n",
        "T\n",
        "G\n",
        "V\n",
        "K\n",
        "R\n",
        "D\n",
        "Q\n",
        "I\n",
        "N\n",
        "F\n",
        "H\n",
        "Y\n",
        "C\n",
        "M\n",
        "W'''\n",
        "with open(\"vocab.txt\", \"w\") as fo:\n",
        "  for token in vocab.split(\"\\n\"):\n",
        "    fo.write(token+\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA4ieYajd-Ht"
      },
      "source": [
        "#If running on a GCP runtime, follow these instructions to set it up:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSUONYA5id9M"
      },
      "source": [
        "###1) Create a VM from the GCP website\n",
        "###2) Open a command prompt on your computer and perform the following steps\"\n",
        "To ssh into the VM, run:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Note: Make sure the port above matches the port below (in this case it's 8888)\n",
        "\\\n",
        "\\\n",
        "In the new command prompt that popped out, either run each of the commands below individually, or copy and paste the one liner below:\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "One command:\n",
        "```\n",
        "sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "###3) In this notebook, click the \"connect to local runtime\" option under the connect button, and copy and paste the link outputted by command prompt with \"locahost: ...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1PvmBO8eR00"
      },
      "source": [
        "#Clone the MutFormer repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2vKz_tKFeO0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f22f94b1-ae84-4b8a-9a74-4e18c0d349c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mutformer'...\n",
            "remote: Enumerating objects: 1574, done.\u001b[K\n",
            "remote: Counting objects: 100% (454/454), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 1574 (delta 313), reused 364 (delta 256), pack-reused 1120\u001b[K\n",
            "Receiving objects: 100% (1574/1574), 5.93 MiB | 10.76 MiB/s, done.\n",
            "Resolving deltas: 100% (1102/1102), done.\n"
          ]
        }
      ],
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git-all\n",
        "#@markdown Where to clone the repo into:\n",
        "REPO_DESTINATION_PATH = \"mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Authorize for GCS, Imports, and General Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9S4CiOh3RzFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e94df3a6-cbf2-494f-c006-fd7883888afd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authorize for GCS:\n",
            "Authorize done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/mutformer/optimization.py:105: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@markdown Whether to use link authorization for GCS (link authorization allows connection to another account other than the one running the script, while normal authorization disables connecting to different accounts):\n",
        "LINK_AUTHORIZATION = False #@param {type:\"boolean\"}\n",
        "\n",
        "if not GCP_RUNTIME:\n",
        "  from google.colab import auth\n",
        "  print(\"Authorize for GCS:\")\n",
        "  if not LINK_AUTHORIZATION: \n",
        "    auth.authenticate_user()\n",
        "  else: \n",
        "    !gcloud auth login --no-launch-browser\n",
        "  print(\"Authorize done\")\n",
        "  \n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import importlib\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "if REPO_DESTINATION_PATH == \"mutformer\":\n",
        "  if os.path.exists(\"mutformer_code\"):\n",
        "    shutil.rmtree(\"mutformer_code\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH,\"mutformer_code\")\n",
        "  REPO_DESTINATION_PATH = \"mutformer_code\"\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic  #### <<<<< if you added more modes, change these imports to import the correct processors         \n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor,MrpcWithExDataProcessor            #### <<<<< and correct training scripts (i.e. run_classifier and run_ner_for_pathogenic)\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor                                \n",
        "\n",
        "##reload modules so that you don't need to restart the runtime to reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_classifier,\n",
        "                  run_ner_for_pathogenic]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\":      ####       vvvvv if you added more modes, change this part to set the processors and training scripts correctly vvvvv\n",
        "  processor = run_classifier.MrpcProcessor()\n",
        "  script = run_classifier\n",
        "  USE_EX_DATA = False\n",
        "elif MODE==\"MRPC_w_ex_data\":\n",
        "  processor = run_classifier.MrpcWithExDataProcessor()\n",
        "  script = run_classifier\n",
        "  USE_EX_DATA = True\n",
        "elif MODE==\"RE\":\n",
        "  processor = run_classifier.REProcessor()\n",
        "  script = run_classifier\n",
        "  USE_EX_DATA = False\n",
        "elif MODE==\"NER\":\n",
        "  processor = run_ner_for_pathogenic.NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "  USE_EX_DATA = False\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\", \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=\"vocab.txt\", do_lower_case=False)\n",
        "                      ####       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lXDH9WeQWGw"
      },
      "source": [
        "###General setup and definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nUXoN_qYQZOA"
      },
      "outputs": [],
      "source": [
        "#@markdown Maximum batch size the finetuning_benchmark script can handle without OOM (must be divisible by NUM_TPU_CORES_WHEN_TESTING):\n",
        "MAX_BATCH_SIZE =  1024 #@param {type:\"integer\"}\n",
        "#@markdown How many tpu cores will be used during evaluation and prediction (for colab runtimes, it's 8):\n",
        "NUM_TPU_CORES_WHEN_TESTING = 8 #@param {type:\"integer\"}\n",
        "\n",
        "def generate_data(MAX_SEQ_LENGTH,\n",
        "                  data_folder_current,\n",
        "                  DATA_GCS_DIR,\n",
        "                  PRECISE_TESTING,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  AUGMENT_COPIES_TRAIN,\n",
        "                  SHARD_SIZE,\n",
        "                  GENERATE_SETS):\n",
        "\n",
        "  try:\n",
        "    print(\"\\nUpdating and uploading data info json...\\n\")\n",
        "    DATA_INFO[\"sequence_length\"] = MAX_SEQ_LENGTH    ##update data info with sequence length\n",
        "\n",
        "    if USE_EX_DATA:                         ##if using external data, update data \n",
        "      def get_ex_data_num(file):            ##info with the # of external datapoints being used\n",
        "        with tf.gfile.Open(file) as filein: \n",
        "          while True:\n",
        "            line = filein.readline().strip()\n",
        "            if line:\n",
        "              ex_data = line.split(\"\\t\")[3].split()\n",
        "              return len(ex_data)\n",
        "      DATA_INFO[\"ex_data_num\"] = get_ex_data_num(data_folder_current+\"/\"+tf.io.gfile.listdir(data_folder_current)[0])\n",
        "    \n",
        "    with tf.gfile.Open(DATA_GCS_DIR+\"/info.json\",\"w+\") as out: ##writes out a dictionary containing\n",
        "      json.dump(DATA_INFO,out,indent=2)                           ##the dataset's parameters\n",
        "    print(\"Data info json uploaded successfully\")\n",
        "  except Exception as e:\n",
        "    print(\"could not update and upload data info json. Error:\",e)\n",
        "\n",
        "              \n",
        "  def get_or_create_shards(infile, SHARD_SIZE, START_SHARD, END_SHARD):\n",
        "      shard_files = []\n",
        "      with tf.gfile.Open(infile) as filein:\n",
        "          shard_ind = START_SHARD\n",
        "          read_ind = -1\n",
        "          while True:\n",
        "              current_start_line = shard_ind * SHARD_SIZE\n",
        "              if shard_ind == END_SHARD: break\n",
        "              shard_file = f\"{infile}_(shardsize_{SHARD_SIZE})_shard_{shard_ind}\"\n",
        "              shard_files.append([shard_file, shard_ind])\n",
        "              if not tf.io.gfile.exists(shard_file):\n",
        "                  with tf.gfile.Open(shard_file, \"w+\") as shardout:\n",
        "                      wroteout = 0\n",
        "                      for line in tqdm(filein, f\"creating shard number {shard_ind}\"):\n",
        "                          if not line.strip():\n",
        "                              continue\n",
        "                          read_ind += 1\n",
        "                          if read_ind < current_start_line:\n",
        "                              continue\n",
        "                          shardout.write(line)\n",
        "                          wroteout += 1\n",
        "                          if wroteout == SHARD_SIZE:\n",
        "                              break\n",
        "                      if wroteout == 0:\n",
        "                          shardout.close()\n",
        "                          del shard_files[-1]\n",
        "                          break\n",
        "                      if wroteout < SHARD_SIZE:\n",
        "                          break\n",
        "              shard_ind += 1\n",
        "      return shard_files\n",
        "  \n",
        "\n",
        "  DO_TRAIN, DO_DEV, DO_TEST = GENERATE_SETS\n",
        "  \n",
        "\n",
        "  if DO_TRAIN:\n",
        "    try:\n",
        "      print(\"\\nGenerating train set...\\n\")\n",
        "      train_data_input_file = processor.get_train_file(data_folder_current)\n",
        "      if USING_SHARDS:\n",
        "        shards = get_or_create_shards(train_data_input_file,SHARD_SIZE//(AUGMENT_COPIES_TRAIN+1),START_SHARD,END_SHARD)\n",
        "      else:\n",
        "        shards = [train_data_input_file,None]\n",
        "      for shard,shard_ind in shards:\n",
        "        if USING_SHARDS: print(f\"generating data for shard number {shard_ind}\")\n",
        "        train_examples = processor._create_examples(processor._read_tsv(shard),\"train\")\n",
        "        if len(train_examples) == 0:\n",
        "          raise Exception(\"no data present in the train dataset\")\n",
        "        train_file = os.path.join(DATA_GCS_DIR, \"train.tf_record\")\n",
        "        if USING_SHARDS:\n",
        "          train_file+=\"_\"+str(shard_ind)\n",
        "        script.file_based_convert_examples_to_features(train_examples, \n",
        "                                                      label_list, \n",
        "                                                      MAX_SEQ_LENGTH, \n",
        "                                                      tokenizer, \n",
        "                                                      train_file,\n",
        "                                                      augmented_data_copies=AUGMENT_COPIES_TRAIN,\n",
        "                                                      shuffle_data=True)\n",
        "    except Exception as e:\n",
        "      print(\"train dataset generation failed. Error:\",e)\n",
        "\n",
        "  if DO_DEV:\n",
        "    try:\n",
        "      print(\"\\nGenerating dev set...\\n\")\n",
        "      dev_data_input_file = processor.get_dev_file(data_folder_current)\n",
        "      if USING_SHARDS:\n",
        "        shards = get_or_create_shards(dev_data_input_file,SHARD_SIZE,START_SHARD,END_SHARD)\n",
        "      else:\n",
        "        shards = [dev_data_input_file,None]\n",
        "      for shard,shard_ind in shards:\n",
        "        if USING_SHARDS: print(f\"generating data for shard number {shard_ind}\")\n",
        "        dev_examples = processor._create_examples(processor._read_tsv(shard),\"dev\")\n",
        "        if len(dev_examples) == 0:\n",
        "          raise Exception(\"no data present in the dev dataset\")\n",
        "        dev_file = os.path.join(DATA_GCS_DIR, \"dev.tf_record\")\n",
        "        if USING_SHARDS:\n",
        "          dev_file+=\"_\"+str(shard_ind)\n",
        "        script.file_based_convert_examples_to_features(dev_examples, \n",
        "                                                      label_list, \n",
        "                                                      MAX_SEQ_LENGTH, \n",
        "                                                      tokenizer, \n",
        "                                                      dev_file)\n",
        "    except Exception as e:\n",
        "      print(\"dev dataset generation failed. Error:\",e)\n",
        "\n",
        "  if DO_TEST:\n",
        "    try:\n",
        "      print(\"\\nGenerating test set...\\n\")\n",
        "      datasets = [re.match(\"test_(\\w+).tsv\",file).groups()[0] for file in tf.io.gfile.listdir(data_folder_current) if re.match(\"test_(\\w+).tsv\",file)]\n",
        "      if not datasets:\n",
        "        datasets = [None]\n",
        "      for dataset in datasets:\n",
        "        if dataset: print(f\"Processing dataset: {dataset}\")\n",
        "        test_data_input_file = processor.get_test_file(data_folder_current,dataset=dataset)\n",
        "        if USING_SHARDS:\n",
        "          shards = get_or_create_shards(test_data_input_file,SHARD_SIZE,START_SHARD,END_SHARD)\n",
        "        else:\n",
        "          shards = [test_data_input_file,None]\n",
        "        for n,(shard,shard_ind) in enumerate(shards):\n",
        "          if USING_SHARDS: print(f\"generating data for shard number {shard_ind}\")\n",
        "          test_examples = processor._create_examples(processor._read_tsv(shard),\"test\")\n",
        "          if len(test_examples) == 0:\n",
        "            raise Exception(\"no data present in the test dataset\")\n",
        "          test_file = os.path.join(DATA_GCS_DIR, f\"test_{dataset}.tf_record\" if dataset else \"test.tf_record\")\n",
        "          if USING_SHARDS:\n",
        "            test_file+=\"_\"+str(shard_ind)\n",
        "          ## if using precise testing, the data will be split into two sets: \n",
        "          ## one set will be able to be predicted on the maximum possible batch \n",
        "          ## size, while the other will be predicted on a batch size of 1, to \n",
        "          ## ensure the fastest prediction without leaving out any datapoints\n",
        "          if PRECISE_TESTING and n==len(shards)-1:\n",
        "            test_file_trailing = os.path.join(DATA_GCS_DIR, f\"test_trailing_{dataset}.tf_record\" if dataset else \"test_trailing.tf_record\")\n",
        "            def largest_mutiple_under_max(max,multiple_base):\n",
        "              return int(max/multiple_base)*multiple_base\n",
        "\n",
        "            split = largest_mutiple_under_max(len(test_examples),MAX_BATCH_SIZE)\n",
        "            test_examples_head = test_examples[:split]\n",
        "            test_examples_trailing = test_examples[split:]\n",
        "            script.file_based_convert_examples_to_features(test_examples_head, \n",
        "                                                           label_list, \n",
        "                                                           MAX_SEQ_LENGTH, \n",
        "                                                           tokenizer, \n",
        "                                                           test_file)\n",
        "            if test_examples_trailing:\n",
        "              script.file_based_convert_examples_to_features(test_examples_trailing, \n",
        "                                                            label_list, \n",
        "                                                            MAX_SEQ_LENGTH, \n",
        "                                                            tokenizer, \n",
        "                                                            test_file_trailing)\n",
        "          else:\n",
        "            script.file_based_convert_examples_to_features(test_examples, \n",
        "                                                           label_list, \n",
        "                                                           MAX_SEQ_LENGTH, \n",
        "                                                           tokenizer, \n",
        "                                                           test_file)\n",
        "    except Exception as e:\n",
        "      print(\"test dataset generation failed. Error:\",e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED2rhitTCdjm"
      },
      "source": [
        "###Data Generation ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhUajg5kClz2"
      },
      "source": [
        "There are currently two data generations loops/ops (more can be added using a similar format to these two examples):\n",
        "1. Varying sequence lengths: multiple sets of different sequence lengths will be generated\n",
        "  * Store multiple individual datasets as subfolders inside of Input finetuning data folder, with each folder named its corresponding sequence length.\n",
        "2. Only one dataset: a single dataset with a specified set of parameters will be generated \n",
        "  * Directly store only the files train.tsv, dev.tsv, and test.tsv for one dataset inside Input finetuning data folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TWmbWT5SJqg"
      },
      "source": [
        "####Varying sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "outputs": [],
      "source": [
        "#@markdown List of maximum sequence lengths to generate data for\n",
        "MAX_SEQ_LENGTHS = [1024] #@param\n",
        "#@markdown Whether or not to ensure all datapoints are used during prediction by using an extra trailing test dataset so no datapoints will be skipped due to the batch size. (This option should be used unless an extra trailing test dataset is a large problem)\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to split the data processing into shards (only for really large databases, since finetuning data typically isn't that large)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown If USING_SHARDS, what shard size to use (how many lines/datapoints should be in each shard) (MUST BE DIVISIBLE BY \"MAX_BATCH_SIZE\") (if using data augmentation, size indicates the size of augmented data)\n",
        "SHARD_SIZE = 1024000 #@param {type:\"integer\"}\n",
        "#@markdown If USING_SHARDS, which shard to start at (default start at shard 0)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Which sets to generate out of train, dev, and test\n",
        "TRAIN = False #@param {type:\"boolean\"}\n",
        "DEV = False #@param {type:\"boolean\"}\n",
        "TEST = True #@param {type:\"boolean\"}\n",
        "#@markdown How many additional augmented copies to load (augmented copies refer to duplicates of the same sequence but with different clip locations. This parameter is defined by the \"run_classifier.py\" file in the \"mutformer_model_code\" folder):\n",
        "AUGMENT_COPIES_TRAIN =  0#@param{type:\"integer\"}\n",
        "\n",
        "for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS:\n",
        "  print(\"\\n\\nGenerating data for seq length:\",MAX_SEQ_LENGTH,\"\\n\\n\")\n",
        "  DATA_GCS_DIR = BUCKET_PATH+\"/\"+OUTPUT_DATA_DIR +\"/\"+ str(MAX_SEQ_LENGTH)\n",
        "  data_folder_current= INPUT_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "  generate_data(MAX_SEQ_LENGTH,\n",
        "                data_folder_current,\n",
        "                DATA_GCS_DIR,\n",
        "                PRECISE_TESTING,\n",
        "                USING_SHARDS,\n",
        "                START_SHARD,\n",
        "                AUGMENT_COPIES_TRAIN,\n",
        "                SHARD_SIZE,\n",
        "                [TRAIN,DEV,TEST])\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEOfXa4WiB2N"
      },
      "source": [
        "###Only one dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IytLW0VbgOZz"
      },
      "outputs": [],
      "source": [
        "#@markdown Maximum output data length (when using paired method, actual protein sequence length is about half of this value):\n",
        "MAX_SEQ_LENGTH = 1024 #@param {type:\"integer\"}\n",
        "#@markdown Whether or not to ensure all datapoints are used during prediction by using an extra trailing test dataset so no datapoints will be skipped due to the batch size. (This option should be used most of the time unless an extra trailing test dataset is a large problem)\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to split the data processing into shards (only for really large databases, since finetuning data typically isn't that large)\n",
        "USING_SHARDS = True #@param {type:\"boolean\"}\n",
        "#@markdown If USING_SHARDS, what shard size to use (how many lines/datapoints should be in each shard) (MUST BE DIVISIBLE BY \"MAX_BATCH_SIZE\")\n",
        "SHARD_SIZE = 1024000 #@param {type:\"integer\"}\n",
        "#@markdown * If USING_SHARDS, set this value to indicate which shard to start processing at (defualt 0 for first shard)\n",
        "START_SHARD =  53#@param {type:\"integer\"}\n",
        "#@markdown * If USING_SHARDS, set this value to indicate which shard to process until (not inclusive) (defualt -1 for last shard)\n",
        "END_SHARD =  54#@param {type:\"integer\"}\n",
        "#@markdown Which sets to generate out of train, dev, and test\n",
        "TRAIN = False #@param {type:\"boolean\"}\n",
        "DEV = False #@param {type:\"boolean\"}\n",
        "TEST = True #@param {type:\"boolean\"}\n",
        "#@markdown How many additional augmented copies to load (augmented copies refer to duplicates of the same sequence but with different clip locations. This parameter is defined by the \"run_classifier.py\" file in the \"mutformer_model_code\" folder):\n",
        "AUGMENT_COPIES_TRAIN =  0#@param{type:\"integer\"}\n",
        "\n",
        "DATA_GCS_DIR = BUCKET_PATH+\"/\"+OUTPUT_DATA_DIR\n",
        "data_folder_current = INPUT_DATA_DIR\n",
        "\n",
        "generate_data(MAX_SEQ_LENGTH,\n",
        "              data_folder_current,\n",
        "              DATA_GCS_DIR,\n",
        "              PRECISE_TESTING,\n",
        "              USING_SHARDS,\n",
        "              START_SHARD,\n",
        "              AUGMENT_COPIES_TRAIN,\n",
        "              SHARD_SIZE,\n",
        "              [TRAIN,DEV,TEST])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_TWmbWT5SJqg"
      ],
      "provenance": []
    },
    "kernelspec": {
      "name": "py37",
      "display_name": "Python 3.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}