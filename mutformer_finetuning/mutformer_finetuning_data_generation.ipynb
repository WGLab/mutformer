{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5WVrOdzC3K1"
      },
      "source": [
        "#Finetuning Data Generation Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq31-sx6C-eM"
      },
      "source": [
        "This notebook processes tsv data and uploads the processed data to GCS to be used for finetuning MutFormer."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downgrade TensorFlow (most likely requires runtime restart if using Colab runtime)"
      ],
      "metadata": {
        "id": "RE7RL-X21OzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "metadata": {
        "id": "bOlnFkJ11OEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings/Mount Drive if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "outputs": [],
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown Whether or not this script is being run in a GCP runtime (if more memory is required for large databases)\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "#@markdown Which mode to use (a different mode means a different finetuning task): options are:\n",
        "#@markdown * \"MRPC\" - paired sequence method\n",
        "#@markdown * \"MRPC_w_ex_data\" - paired sequence method with external data\n",
        "#@markdown * \"RE\" - single sequence method\n",
        "#@markdown * \"NER\" - single sequence per residue prediction\n",
        "#@markdown \n",
        "#@markdown You can add more modes by creating a new processor and/or a new model_fn inside of the \"mutformer_model_code\" folder downloaded from github, then changing the corresponding code snippets in the code segment named \"Authorize for GCS, Imports, and General Setup\" (also edit the dropdown below).\n",
        "MODE = \"MRPC_w_ex_data\" #@param   [\"MRPC_w_ex_data\", \"MRPC\", \"RE\", \"NER\"]   {type:\"string\"} \n",
        "            ####      ^^^^^ dropdown list for all modes ^^^^^\n",
        "\n",
        "#@markdown Name of the GCS bucket to use:\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ## IO Config\n",
        "#@markdown Input finetuning data folder: data will be read from here to be processed and uploaded to GCS (can be a drive path, or a GCS path if needed for large databases; must be a GCS path if using GCP_RUNTIME):\n",
        "#@markdown \n",
        "#@markdown * For processing multiple sets i.e. for multiple sequence lengths, simply store these sets into separate subfolders inside of the folder listed below, with each subfolder being named as specified in the following section.\n",
        "#@markdown \n",
        "#@markdown * For processing a single set, this folder should directly contain one dataset.\n",
        "#@markdown\n",
        "INPUT_DATA_DIR = \"gs://theodore_jiang/updated_all_snp_prediction_data\" #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "if not GCP_RUNTIME:                    ##if INPUT_DATA_DIR is a drive path,\n",
        "  if \"/content/drive\" in INPUT_DATA_DIR:   ##mount google drive\n",
        "    from google.colab import drive\n",
        "    if GCP_RUNTIME:\n",
        "      raise Exception(\"if GCP_RUNTIME, a GCS path must be used, since Google's cloud TPUs can only communicate with GCS and not drive\")\n",
        "    !fusermount -u /content/drive\n",
        "    drive.flush_and_unmount()\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "#@markdown Name of the folder in GCS to put processed data into: \n",
        "#@markdown * For generating multiple datasets i.e. for different sequence lengths, they will be written as individual subfolders inside of this folder.\n",
        "OUTPUT_DATA_DIR = \"all_snp_prediction_data_loaded\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "DATA_INFO = {      ##dictionary that will be uploaded alongside \n",
        "    \"mode\":MODE    ##each dataset to indicate its parameters\n",
        "}\n",
        "\n",
        "\n",
        "#### Vocabulary for the model (MutFormer uses the vocabulary below) ([PAD]\n",
        "#### [UNK],[CLS],[SEP], and [MASK] are necessary default tokens; B and J\n",
        "#### are markers for the beginning and ending of a protein sequence,\n",
        "#### respectively; the rest are all amino acids possible, ranked \n",
        "#### approximately by frequency of occurence in human population)\n",
        "#### vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
        "vocab = \\\n",
        "'''[PAD]\n",
        "[UNK]\n",
        "[CLS]\n",
        "[SEP]\n",
        "[MASK]\n",
        "L\n",
        "S\n",
        "B\n",
        "J\n",
        "E\n",
        "A\n",
        "P\n",
        "T\n",
        "G\n",
        "V\n",
        "K\n",
        "R\n",
        "D\n",
        "Q\n",
        "I\n",
        "N\n",
        "F\n",
        "H\n",
        "Y\n",
        "C\n",
        "M\n",
        "W'''\n",
        "with open(\"vocab.txt\", \"w\") as fo:\n",
        "  for token in vocab.split(\"\\n\"):\n",
        "    fo.write(token+\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA4ieYajd-Ht"
      },
      "source": [
        "#If running on a GCP runtime, follow these instructions to set it up:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSUONYA5id9M"
      },
      "source": [
        "###1) Create a VM from the GCP website\n",
        "###2) Open a command prompt on your computer and perform the following steps\"\n",
        "To ssh into the VM, run:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Note: Make sure the port above matches the port below (in this case it's 8888)\n",
        "\\\n",
        "\\\n",
        "In the new command prompt that popped out, either run each of the commands below individually, or copy and paste the one liner below:\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "One command:\n",
        "```\n",
        "sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "###3) In this notebook, click the \"connect to local runtime\" option under the connect button, and copy and paste the link outputted by command prompt with \"locahost: ...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1PvmBO8eR00"
      },
      "source": [
        "#Clone the MutFormer repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vKz_tKFeO0s",
        "outputId": "b8304093-75bd-44f8-baee-2443f5253c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mutformer'...\n",
            "remote: Enumerating objects: 1358, done.\u001b[K\n",
            "remote: Counting objects: 100% (238/238), done.\u001b[K\n",
            "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
            "remote: Total 1358 (delta 184), reused 142 (delta 98), pack-reused 1120\u001b[K\n",
            "Receiving objects: 100% (1358/1358), 2.33 MiB | 9.67 MiB/s, done.\n",
            "Resolving deltas: 100% (973/973), done.\n"
          ]
        }
      ],
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git-all\n",
        "#@markdown Where to clone the repo into:\n",
        "REPO_DESTINATION_PATH = \"mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Authorize for GCS, Imports, and General Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S4CiOh3RzFW",
        "outputId": "d9fb7fd2-3234-4daa-ee77-f2e5ff494f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authorize for GCS:\n",
            "Authorize done\n",
            "WARNING: Tensorflow 1 is deprecated, and support will be removed on August 1, 2022.\n",
            "After that, `%tensorflow_version 1.x` will throw an error.\n",
            "\n",
            "Your notebook should be updated to use Tensorflow 2.\n",
            "See the guide at https://www.tensorflow.org/guide/migrate#migrate-from-tensorflow-1x-to-tensorflow-2.\n",
            "\n",
            "TensorFlow 1.x selected.\n",
            "WARNING:tensorflow:From /content/mutformer/optimization.py:105: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@markdown Whether to use link authorization for GCS (link authorization allows connection to another account other than the one running the script, while normal authorization disables connecting to different accounts):\n",
        "LINK_AUTHORIZATION = False #@param {type:\"boolean\"}\n",
        "\n",
        "if not GCP_RUNTIME:\n",
        "  from google.colab import auth\n",
        "  print(\"Authorize for GCS:\")\n",
        "  if not LINK_AUTHORIZATION: \n",
        "    auth.authenticate_user()\n",
        "  else: \n",
        "    !gcloud auth login --no-launch-browser\n",
        "  print(\"Authorize done\")\n",
        "  \n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import importlib\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "if REPO_DESTINATION_PATH == \"mutformer\":\n",
        "  if os.path.exists(\"mutformer_code\"):\n",
        "    shutil.rmtree(\"mutformer_code\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH,\"mutformer_code\")\n",
        "  REPO_DESTINATION_PATH = \"mutformer_code\"\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic  #### <<<<< if you added more modes, change these imports to import the correct processors         \n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor,MrpcWithExDataProcessor            #### <<<<< and correct training scripts (i.e. run_classifier and run_ner_for_pathogenic)\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor                                \n",
        "\n",
        "##reload modules so that you don't need to restart the runtime to reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_classifier,\n",
        "                  run_ner_for_pathogenic]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\":      ####       vvvvv if you added more modes, change this part to set the processors and training scripts correctly vvvvv\n",
        "  processor = run_classifier.MrpcProcessor()\n",
        "  script = run_classifier\n",
        "  USE_EX_DATA = False\n",
        "elif MODE==\"MRPC_w_ex_data\":\n",
        "  processor = run_classifier.MrpcWithExDataProcessor()\n",
        "  script = run_classifier\n",
        "  USE_EX_DATA = True\n",
        "elif MODE==\"RE\":\n",
        "  processor = run_classifier.REProcessor()\n",
        "  script = run_classifier\n",
        "  USE_EX_DATA = False\n",
        "elif MODE==\"NER\":\n",
        "  processor = run_ner_for_pathogenic.NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "  USE_EX_DATA = False\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\", \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=\"vocab.txt\", do_lower_case=False)\n",
        "                      ####       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lXDH9WeQWGw"
      },
      "source": [
        "###General setup and definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUXoN_qYQZOA"
      },
      "outputs": [],
      "source": [
        "#@markdown Maximum batch size the finetuning_benchmark script can handle without OOM (must be divisible by NUM_TPU_CORES_WHEN_TESTING):\n",
        "MAX_BATCH_SIZE =  1024 #@param {type:\"integer\"}\n",
        "#@markdown How many tpu cores will be used during evaluation and prediction (for colab runtimes, it's 8):\n",
        "NUM_TPU_CORES_WHEN_TESTING = 8 #@param {type:\"integer\"}\n",
        "\n",
        "def generate_data(MAX_SEQ_LENGTH,\n",
        "                  data_folder_current,\n",
        "                  DATA_GCS_DIR,\n",
        "                  PRECISE_TESTING,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  AUGMENT_COPIES_TRAIN,\n",
        "                  SHARD_SIZE,\n",
        "                  GENERATE_SETS):\n",
        "\n",
        "  try:\n",
        "    print(\"\\nUpdating and uploading data info json...\\n\")\n",
        "    DATA_INFO[\"sequence_length\"] = MAX_SEQ_LENGTH    ##update data info with sequence length\n",
        "\n",
        "    if USE_EX_DATA:                         ##if using external data, update data \n",
        "      def get_ex_data_num(file):            ##info with the # of external datapoints being used\n",
        "        with tf.gfile.Open(file) as filein: \n",
        "          while True:\n",
        "            line = filein.readline().strip()\n",
        "            if line:\n",
        "              ex_data = line.split(\"\\t\")[3].split()\n",
        "              return len(ex_data)\n",
        "      DATA_INFO[\"ex_data_num\"] = get_ex_data_num(data_folder_current+\"/\"+tf.io.gfile.listdir(data_folder_current)[0])\n",
        "    \n",
        "    with tf.gfile.Open(DATA_GCS_DIR+\"/info.json\",\"w+\") as out: ##writes out a dictionary containing\n",
        "      json.dump(DATA_INFO,out,indent=2)                           ##the dataset's parameters\n",
        "    print(\"Data info json uploaded successfully\")\n",
        "  except Exception as e:\n",
        "    print(\"could not update and upload data info json. Error:\",e)\n",
        "\n",
        "              \n",
        "  def get_or_create_shards(infile, SHARD_SIZE, START_SHARD, END_SHARD):\n",
        "      shard_files = []\n",
        "      with tf.gfile.Open(infile) as filein:\n",
        "          shard_ind = START_SHARD\n",
        "          read_ind = -1\n",
        "          while True:\n",
        "              current_start_line = shard_ind * SHARD_SIZE\n",
        "              if shard_ind == END_SHARD: break\n",
        "              shard_file = f\"{infile}_(shardsize_{SHARD_SIZE})_shard_{shard_ind}\"\n",
        "              shard_files.append([shard_file, shard_ind])\n",
        "              if not tf.io.gfile.exists(shard_file):\n",
        "                  with tf.gfile.Open(shard_file, \"w+\") as shardout:\n",
        "                      wroteout = 0\n",
        "                      for line in tqdm(filein, f\"creating shard number {shard_ind}\"):\n",
        "                          if not line.strip():\n",
        "                              continue\n",
        "                          read_ind += 1\n",
        "                          if read_ind < current_start_line:\n",
        "                              continue\n",
        "                          shardout.write(line)\n",
        "                          wroteout += 1\n",
        "                          if wroteout == SHARD_SIZE:\n",
        "                              break\n",
        "                      if wroteout == 0:\n",
        "                          shardout.close()\n",
        "                          del shard_files[-1]\n",
        "                          break\n",
        "                      if wroteout < SHARD_SIZE:\n",
        "                          break\n",
        "              shard_ind += 1\n",
        "      return shard_files\n",
        "  \n",
        "\n",
        "  DO_TRAIN, DO_DEV, DO_TEST = GENERATE_SETS\n",
        "  \n",
        "\n",
        "  if DO_TRAIN:\n",
        "    try:\n",
        "      print(\"\\nGenerating train set...\\n\")\n",
        "      train_data_input_file = processor.get_train_file(data_folder_current)\n",
        "      if USING_SHARDS:\n",
        "        shards = get_or_create_shards(train_data_input_file,SHARD_SIZE//(AUGMENT_COPIES_TRAIN+1),START_SHARD,END_SHARD)\n",
        "      else:\n",
        "        shards = [train_data_input_file,None]\n",
        "      for shard,shard_ind in shards:\n",
        "        if USING_SHARDS: print(f\"generating data for shard number {shard_ind}\")\n",
        "        train_examples = processor._create_examples(processor._read_tsv(shard),\"train\")\n",
        "        if len(train_examples) == 0:\n",
        "          raise Exception(\"no data present in the train dataset\")\n",
        "        train_file = os.path.join(DATA_GCS_DIR, \"train.tf_record\")\n",
        "        if USING_SHARDS:\n",
        "          train_file+=\"_\"+str(shard_ind)\n",
        "        script.file_based_convert_examples_to_features(train_examples, \n",
        "                                                      label_list, \n",
        "                                                      MAX_SEQ_LENGTH, \n",
        "                                                      tokenizer, \n",
        "                                                      train_file,\n",
        "                                                      augmented_data_copies=AUGMENT_COPIES_TRAIN,\n",
        "                                                      shuffle_data=True)\n",
        "    except Exception as e:\n",
        "      print(\"train dataset generation failed. Error:\",e)\n",
        "\n",
        "  if DO_DEV:\n",
        "    try:\n",
        "      print(\"\\nGenerating dev set...\\n\")\n",
        "      dev_data_input_file = processor.get_dev_file(data_folder_current)\n",
        "      if USING_SHARDS:\n",
        "        shards = get_or_create_shards(dev_data_input_file,SHARD_SIZE,START_SHARD,END_SHARD)\n",
        "      else:\n",
        "        shards = [dev_data_input_file,None]\n",
        "      for shard,shard_ind in shards:\n",
        "        if USING_SHARDS: print(f\"generating data for shard number {shard_ind}\")\n",
        "        dev_examples = processor._create_examples(processor._read_tsv(shard),\"dev\")\n",
        "        if len(dev_examples) == 0:\n",
        "          raise Exception(\"no data present in the dev dataset\")\n",
        "        dev_file = os.path.join(DATA_GCS_DIR, \"dev.tf_record\")\n",
        "        if USING_SHARDS:\n",
        "          dev_file+=\"_\"+str(shard_ind)\n",
        "        script.file_based_convert_examples_to_features(dev_examples, \n",
        "                                                      label_list, \n",
        "                                                      MAX_SEQ_LENGTH, \n",
        "                                                      tokenizer, \n",
        "                                                      dev_file)\n",
        "    except Exception as e:\n",
        "      print(\"dev dataset generation failed. Error:\",e)\n",
        "\n",
        "  if DO_TEST:\n",
        "    try:\n",
        "      print(\"\\nGenerating test set...\\n\")\n",
        "      datasets = [re.match(\"test_(\\w+).tsv\",file).groups()[0] for file in tf.io.gfile.listdir(data_folder_current) if re.match(\"test_(\\w+).tsv\",file)]\n",
        "      if not datasets:\n",
        "        datasets = [None]\n",
        "      for dataset in datasets:\n",
        "        if dataset: print(f\"Processing dataset: {dataset}\")\n",
        "        test_data_input_file = processor.get_test_file(data_folder_current,dataset=dataset)\n",
        "        if USING_SHARDS:\n",
        "          shards = get_or_create_shards(test_data_input_file,SHARD_SIZE,START_SHARD,END_SHARD)\n",
        "        else:\n",
        "          shards = [test_data_input_file,None]\n",
        "        for n,(shard,shard_ind) in enumerate(shards):\n",
        "          if USING_SHARDS: print(f\"generating data for shard number {shard_ind}\")\n",
        "          test_examples = processor._create_examples(processor._read_tsv(shard),\"test\")\n",
        "          if len(test_examples) == 0:\n",
        "            raise Exception(\"no data present in the test dataset\")\n",
        "          test_file = os.path.join(DATA_GCS_DIR, f\"test_{dataset}.tf_record\" if dataset else \"test.tf_record\")\n",
        "          if USING_SHARDS:\n",
        "            test_file+=\"_\"+str(shard_ind)\n",
        "          ## if using precise testing, the data will be split into two sets: \n",
        "          ## one set will be able to be predicted on the maximum possible batch \n",
        "          ## size, while the other will be predicted on a batch size of 1, to \n",
        "          ## ensure the fastest prediction without leaving out any datapoints\n",
        "          if PRECISE_TESTING and n==len(shards)-1:\n",
        "            test_file_trailing = os.path.join(DATA_GCS_DIR, f\"test_trailing_{dataset}.tf_record\" if dataset else \"test_trailing.tf_record\")\n",
        "            def largest_mutiple_under_max(max,multiple_base):\n",
        "              return int(max/multiple_base)*multiple_base\n",
        "\n",
        "            split = largest_mutiple_under_max(len(test_examples),MAX_BATCH_SIZE)\n",
        "            test_examples_head = test_examples[:split]\n",
        "            test_examples_trailing = test_examples[split:]\n",
        "            script.file_based_convert_examples_to_features(test_examples_head, \n",
        "                                                           label_list, \n",
        "                                                           MAX_SEQ_LENGTH, \n",
        "                                                           tokenizer, \n",
        "                                                           test_file)\n",
        "            if test_examples_trailing:\n",
        "              script.file_based_convert_examples_to_features(test_examples_trailing, \n",
        "                                                            label_list, \n",
        "                                                            MAX_SEQ_LENGTH, \n",
        "                                                            tokenizer, \n",
        "                                                            test_file_trailing)\n",
        "          else:\n",
        "            script.file_based_convert_examples_to_features(test_examples, \n",
        "                                                           label_list, \n",
        "                                                           MAX_SEQ_LENGTH, \n",
        "                                                           tokenizer, \n",
        "                                                           test_file)\n",
        "    except Exception as e:\n",
        "      print(\"test dataset generation failed. Error:\",e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED2rhitTCdjm"
      },
      "source": [
        "###Data Generation ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhUajg5kClz2"
      },
      "source": [
        "There are currently two data generations loops/ops (more can be added using a similar format to these two examples):\n",
        "1. Varying sequence lengths: multiple sets of different sequence lengths will be generated\n",
        "  * Store multiple individual datasets as subfolders inside of Input finetuning data folder, with each folder named its corresponding sequence length.\n",
        "2. Only one dataset: a single dataset with a specified set of parameters will be generated \n",
        "  * Directly store only the files train.tsv, dev.tsv, and test.tsv for one dataset inside Input finetuning data folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TWmbWT5SJqg"
      },
      "source": [
        "####Varying sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "outputs": [],
      "source": [
        "#@markdown List of maximum sequence lengths to generate data for\n",
        "MAX_SEQ_LENGTHS = [1024] #@param\n",
        "#@markdown Whether or not to ensure all datapoints are used during prediction by using an extra trailing test dataset so no datapoints will be skipped due to the batch size. (This option should be used unless an extra trailing test dataset is a large problem)\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to split the data processing into shards (only for really large databases, since finetuning data typically isn't that large)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown If USING_SHARDS, what shard size to use (how many lines/datapoints should be in each shard) (MUST BE DIVISIBLE BY \"MAX_BATCH_SIZE\") (if using data augmentation, size indicates the size of augmented data)\n",
        "SHARD_SIZE = 1024000 #@param {type:\"integer\"}\n",
        "#@markdown If USING_SHARDS, which shard to start at (default start at shard 0)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Which sets to generate out of train, dev, and test\n",
        "TRAIN = False #@param {type:\"boolean\"}\n",
        "DEV = False #@param {type:\"boolean\"}\n",
        "TEST = True #@param {type:\"boolean\"}\n",
        "#@markdown How many additional augmented copies to load (augmented copies refer to duplicates of the same sequence but with different clip locations. This parameter is defined by the \"run_classifier.py\" file in the \"mutformer_model_code\" folder):\n",
        "AUGMENT_COPIES_TRAIN =  0#@param{type:\"integer\"}\n",
        "\n",
        "for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS:\n",
        "  print(\"\\n\\nGenerating data for seq length:\",MAX_SEQ_LENGTH,\"\\n\\n\")\n",
        "  DATA_GCS_DIR = BUCKET_PATH+\"/\"+OUTPUT_DATA_DIR +\"/\"+ str(MAX_SEQ_LENGTH)\n",
        "  data_folder_current= INPUT_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "  generate_data(MAX_SEQ_LENGTH,\n",
        "                data_folder_current,\n",
        "                DATA_GCS_DIR,\n",
        "                PRECISE_TESTING,\n",
        "                USING_SHARDS,\n",
        "                START_SHARD,\n",
        "                AUGMENT_COPIES_TRAIN,\n",
        "                SHARD_SIZE,\n",
        "                [TRAIN,DEV,TEST])\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEOfXa4WiB2N"
      },
      "source": [
        "###Only one dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IytLW0VbgOZz",
        "outputId": "680e3097-0e4f-45de-c5ab-2fd506de7736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Updating and uploading data info json...\n",
            "\n",
            "Data info json uploaded successfully\n",
            "\n",
            "Generating test set...\n",
            "\n",
            "generating data for shard number 53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "reading tsv: 1024000it [01:27, 11670.80it/s]\n",
            "creating_examples: 100%|██████████| 1024000/1024000 [00:05<00:00, 187778.34it/s]\n",
            "2022-07-18 01:56:28,514 - tensorflow - WARNING - From /content/mutformer/run_classifier.py:366: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "2022-07-18 01:56:30,648 - tensorflow - WARNING - From /content/mutformer/run_classifier.py:376: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "2022-07-18 01:56:30,655 - tensorflow - INFO - Writing example 0 of 1024000\n",
            "2022-07-18 01:56:30,671 - tensorflow - INFO - *** Example ***\n",
            "2022-07-18 01:56:30,674 - tensorflow - INFO - guid: test-0\n",
            "2022-07-18 01:56:30,678 - tensorflow - INFO - tokens (length = 1023): [CLS] E E E E E E D L I D G F A I A S F A T L E A L Q K D A S L Q P P E R L E H R L K H S G K R K R G G S S G A T G E P G D S S D R E P G R P P G D R A R K W P N K R R R K E A S S R H S L E A G Y I C D A E S D L D E R V S D D D L D P S F T V S T S K A S G P H G A F N G N C E A K L S V V P K V S G L E R S Q E Q P P G P D P L L V P F P P K E P P P P P V P R P P V S P P A P L P A T P S L P P P P Q P Q L Q L R V S P F G L R T S P Y G S S L D L S T G S S S R P P P K A P A P P V A Q P P P S S S S S S S S S S S A S S S S A Q L T H R P P T P S L P L P L S T H S F P P P G L R P P P P P H H P S L F S P G P T L P P P P P L L Q V P G H P G A S A A N A L S E Q D L I G Q D L N S R Y L N A Q G G P E V V G A G G S A R P L A F Q F H Q H N H Q H Q H T H Q H T H Q H F T P Y P P G L L P P H G P H M F E K Y P G K M E G L F R H N P Y T A F P P A V P G L P P G L P P A V S F G S L Q G A F Q P K S T N P E L P P R L G P V P S G L S Q K G T Q I P D H F R P P L R K P G K W C A M H V R V A Y M I L R H Q E K M K G D S H K L D F R N D L L P [SEP] E E E E E E D L I D G F A I A S F A T L E A L Q K D A S L Q P P E R L E H R L K H S G K R K R G G S S G A T G E P G D S S D R E P G R P P G D R A R K W P N K R R R K E A S S R H S L E A G Y I C D A E S D L D E R V S D D D L D P S F T V S T S K A S G P H G A F N G N C E A K L S V V P K V S G L E R S Q E Q P P G P D P L L V P F P P K E P P P P P V P R P P V S P P A P L P A T P S L P P P P Q P Q L Q L R V S P F G L R T S P Y G S S L D L S T G S S S R P P P K A P A P P V A Q P P P S S S S S S S S S S S A S S S P A Q L T H R P P T P S L P L P L S T H S F P P P G L R P P P P P H H P S L F S P G P T L P P P P P L L Q V P G H P G A S A A N A L S E Q D L I G Q D L N S R Y L N A Q G G P E V V G A G G S A R P L A F Q F H Q H N H Q H Q H T H Q H T H Q H F T P Y P P G L L P P H G P H M F E K Y P G K M E G L F R H N P Y T A F P P A V P G L P P G L P P A V S F G S L Q G A F Q P K S T N P E L P P R L G P V P S G L S Q K G T Q I P D H F R P P L R K P G K W C A M H V R V A Y M I L R H Q E K M K G D S H K L D F R N D L L P [SEP]\n",
            "2022-07-18 01:56:30,682 - tensorflow - INFO - input_ids (length = 1024): 2 9 9 9 9 9 9 17 5 19 17 13 21 10 19 10 6 21 10 12 5 9 10 5 18 15 17 10 6 5 18 11 11 9 16 5 9 22 16 5 15 22 6 13 15 16 15 16 13 13 6 6 13 10 12 13 9 11 13 17 6 6 17 16 9 11 13 16 11 11 13 17 16 10 16 15 26 11 20 15 16 16 16 15 9 10 6 6 16 22 6 5 9 10 13 23 19 24 17 10 9 6 17 5 17 9 16 14 6 17 17 17 5 17 11 6 21 12 14 6 12 6 15 10 6 13 11 22 13 10 21 20 13 20 24 9 10 15 5 6 14 14 11 15 14 6 13 5 9 16 6 18 9 18 11 11 13 11 17 11 5 5 14 11 21 11 11 15 9 11 11 11 11 11 14 11 16 11 11 14 6 11 11 10 11 5 11 10 12 11 6 5 11 11 11 11 18 11 18 5 18 5 16 14 6 11 21 13 5 16 12 6 11 23 13 6 6 5 17 5 6 12 13 6 6 6 16 11 11 11 15 10 11 10 11 11 14 10 18 11 11 11 6 6 6 6 6 6 6 6 6 6 6 10 6 6 6 6 10 18 5 12 22 16 11 11 12 11 6 5 11 5 11 5 6 12 22 6 21 11 11 11 13 5 16 11 11 11 11 11 22 22 11 6 5 21 6 11 13 11 12 5 11 11 11 11 11 5 5 18 14 11 13 22 11 13 10 6 10 10 20 10 5 6 9 18 17 5 19 13 18 17 5 20 6 16 23 5 20 10 18 13 13 11 9 14 14 13 10 13 13 6 10 16 11 5 10 21 18 21 22 18 22 20 22 18 22 18 22 12 22 18 22 12 22 18 22 21 12 11 23 11 11 13 5 5 11 11 22 13 11 22 25 21 9 15 23 11 13 15 25 9 13 5 21 16 22 20 11 23 12 10 21 11 11 10 14 11 13 5 11 11 13 5 11 11 10 14 6 21 13 6 5 18 13 10 21 18 11 15 6 12 20 11 9 5 11 11 16 5 13 11 14 11 6 13 5 6 18 15 13 12 18 19 11 17 22 21 16 11 11 5 16 15 11 13 15 26 24 10 25 22 14 16 14 10 23 25 19 5 16 22 18 9 15 25 15 13 17 6 22 15 5 17 21 16 20 17 5 5 11 3 9 9 9 9 9 9 17 5 19 17 13 21 10 19 10 6 21 10 12 5 9 10 5 18 15 17 10 6 5 18 11 11 9 16 5 9 22 16 5 15 22 6 13 15 16 15 16 13 13 6 6 13 10 12 13 9 11 13 17 6 6 17 16 9 11 13 16 11 11 13 17 16 10 16 15 26 11 20 15 16 16 16 15 9 10 6 6 16 22 6 5 9 10 13 23 19 24 17 10 9 6 17 5 17 9 16 14 6 17 17 17 5 17 11 6 21 12 14 6 12 6 15 10 6 13 11 22 13 10 21 20 13 20 24 9 10 15 5 6 14 14 11 15 14 6 13 5 9 16 6 18 9 18 11 11 13 11 17 11 5 5 14 11 21 11 11 15 9 11 11 11 11 11 14 11 16 11 11 14 6 11 11 10 11 5 11 10 12 11 6 5 11 11 11 11 18 11 18 5 18 5 16 14 6 11 21 13 5 16 12 6 11 23 13 6 6 5 17 5 6 12 13 6 6 6 16 11 11 11 15 10 11 10 11 11 14 10 18 11 11 11 6 6 6 6 6 6 6 6 6 6 6 10 6 6 6 11 10 18 5 12 22 16 11 11 12 11 6 5 11 5 11 5 6 12 22 6 21 11 11 11 13 5 16 11 11 11 11 11 22 22 11 6 5 21 6 11 13 11 12 5 11 11 11 11 11 5 5 18 14 11 13 22 11 13 10 6 10 10 20 10 5 6 9 18 17 5 19 13 18 17 5 20 6 16 23 5 20 10 18 13 13 11 9 14 14 13 10 13 13 6 10 16 11 5 10 21 18 21 22 18 22 20 22 18 22 18 22 12 22 18 22 12 22 18 22 21 12 11 23 11 11 13 5 5 11 11 22 13 11 22 25 21 9 15 23 11 13 15 25 9 13 5 21 16 22 20 11 23 12 10 21 11 11 10 14 11 13 5 11 11 13 5 11 11 10 14 6 21 13 6 5 18 13 10 21 18 11 15 6 12 20 11 9 5 11 11 16 5 13 11 14 11 6 13 5 6 18 15 13 12 18 19 11 17 22 21 16 11 11 5 16 15 11 13 15 26 24 10 25 22 14 16 14 10 23 25 19 5 16 22 18 9 15 25 15 13 17 6 22 15 5 17 21 16 20 17 5 5 11 3 0\n",
            "2022-07-18 01:56:30,684 - tensorflow - INFO - input_mask (length = 1024): 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            "2022-07-18 01:56:30,686 - tensorflow - INFO - segment_ids (length = 1024): 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            "2022-07-18 01:56:30,688 - tensorflow - INFO - ex_data (length = 27): 1.221 0.0 0.0 0.0 2.0 0.0 1.6227119910347403 1.5578571428571428 1.214 1.2075754777952175 1.1322222222222222 1.935910478128179 1.851 1.1892588188664288 1.067 1.8416666666666668 1.8289117487818083 1.9339108116778956 1.982390718873006 1.998 2.0 1.1812586742231714 0.0 0.0 0.0 0.0 1.1950288035069412\n",
            "2022-07-18 01:56:30,689 - tensorflow - INFO - label: 0 (id = 0)\n",
            "2022-07-18 01:56:30,715 - tensorflow - INFO - *** Example ***\n",
            "2022-07-18 01:56:30,722 - tensorflow - INFO - guid: test-1\n",
            "2022-07-18 01:56:30,724 - tensorflow - INFO - tokens (length = 1023): [CLS] E E E E E E D L I D G F A I A S F A T L E A L Q K D A S L Q P P E R L E H R L K H S G K R K R G G S S G A T G E P G D S S D R E P G R P P G D R A R K W P N K R R R K E A S S R H S L E A G Y I C D A E S D L D E R V S D D D L D P S F T V S T S K A S G P H G A F N G N C E A K L S V V P K V S G L E R S Q E Q P P G P D P L L V P F P P K E P P P P P V P R P P V S P P A P L P A T P S L P P P P Q P Q L Q L R V S P F G L R T S P Y G S S L D L S T G S S S R P P P K A P A P P V A Q P P P S S S S S S S S S S S A S S S S A Q L T H R P P T P S L P L P L S T H S F P P P G L R P P P P P H H P S L F S P G P T L P P P P P L L Q V P G H P G A S A A N A L S E Q D L I G Q D L N S R Y L N A Q G G P E V V G A G G S A R P L A F Q F H Q H N H Q H Q H T H Q H T H Q H F T P Y P P G L L P P H G P H M F E K Y P G K M E G L F R H N P Y T A F P P A V P G L P P G L P P A V S F G S L Q G A F Q P K S T N P E L P P R L G P V P S G L S Q K G T Q I P D H F R P P L R K P G K W C A M H V R V A Y M I L R H Q E K M K G D S H K L D F R N D L L P [SEP] E E E E E E D L I D G F A I A S F A T L E A L Q K D A S L Q P P E R L E H R L K H S G K R K R G G S S G A T G E P G D S S D R E P G R P P G D R A R K W P N K R R R K E A S S R H S L E A G Y I C D A E S D L D E R V S D D D L D P S F T V S T S K A S G P H G A F N G N C E A K L S V V P K V S G L E R S Q E Q P P G P D P L L V P F P P K E P P P P P V P R P P V S P P A P L P A T P S L P P P P Q P Q L Q L R V S P F G L R T S P Y G S S L D L S T G S S S R P P P K A P A P P V A Q P P P S S S S S S S S S S S A S S S A A Q L T H R P P T P S L P L P L S T H S F P P P G L R P P P P P H H P S L F S P G P T L P P P P P L L Q V P G H P G A S A A N A L S E Q D L I G Q D L N S R Y L N A Q G G P E V V G A G G S A R P L A F Q F H Q H N H Q H Q H T H Q H T H Q H F T P Y P P G L L P P H G P H M F E K Y P G K M E G L F R H N P Y T A F P P A V P G L P P G L P P A V S F G S L Q G A F Q P K S T N P E L P P R L G P V P S G L S Q K G T Q I P D H F R P P L R K P G K W C A M H V R V A Y M I L R H Q E K M K G D S H K L D F R N D L L P [SEP]\n",
            "2022-07-18 01:56:30,727 - tensorflow - INFO - input_ids (length = 1024): 2 9 9 9 9 9 9 17 5 19 17 13 21 10 19 10 6 21 10 12 5 9 10 5 18 15 17 10 6 5 18 11 11 9 16 5 9 22 16 5 15 22 6 13 15 16 15 16 13 13 6 6 13 10 12 13 9 11 13 17 6 6 17 16 9 11 13 16 11 11 13 17 16 10 16 15 26 11 20 15 16 16 16 15 9 10 6 6 16 22 6 5 9 10 13 23 19 24 17 10 9 6 17 5 17 9 16 14 6 17 17 17 5 17 11 6 21 12 14 6 12 6 15 10 6 13 11 22 13 10 21 20 13 20 24 9 10 15 5 6 14 14 11 15 14 6 13 5 9 16 6 18 9 18 11 11 13 11 17 11 5 5 14 11 21 11 11 15 9 11 11 11 11 11 14 11 16 11 11 14 6 11 11 10 11 5 11 10 12 11 6 5 11 11 11 11 18 11 18 5 18 5 16 14 6 11 21 13 5 16 12 6 11 23 13 6 6 5 17 5 6 12 13 6 6 6 16 11 11 11 15 10 11 10 11 11 14 10 18 11 11 11 6 6 6 6 6 6 6 6 6 6 6 10 6 6 6 6 10 18 5 12 22 16 11 11 12 11 6 5 11 5 11 5 6 12 22 6 21 11 11 11 13 5 16 11 11 11 11 11 22 22 11 6 5 21 6 11 13 11 12 5 11 11 11 11 11 5 5 18 14 11 13 22 11 13 10 6 10 10 20 10 5 6 9 18 17 5 19 13 18 17 5 20 6 16 23 5 20 10 18 13 13 11 9 14 14 13 10 13 13 6 10 16 11 5 10 21 18 21 22 18 22 20 22 18 22 18 22 12 22 18 22 12 22 18 22 21 12 11 23 11 11 13 5 5 11 11 22 13 11 22 25 21 9 15 23 11 13 15 25 9 13 5 21 16 22 20 11 23 12 10 21 11 11 10 14 11 13 5 11 11 13 5 11 11 10 14 6 21 13 6 5 18 13 10 21 18 11 15 6 12 20 11 9 5 11 11 16 5 13 11 14 11 6 13 5 6 18 15 13 12 18 19 11 17 22 21 16 11 11 5 16 15 11 13 15 26 24 10 25 22 14 16 14 10 23 25 19 5 16 22 18 9 15 25 15 13 17 6 22 15 5 17 21 16 20 17 5 5 11 3 9 9 9 9 9 9 17 5 19 17 13 21 10 19 10 6 21 10 12 5 9 10 5 18 15 17 10 6 5 18 11 11 9 16 5 9 22 16 5 15 22 6 13 15 16 15 16 13 13 6 6 13 10 12 13 9 11 13 17 6 6 17 16 9 11 13 16 11 11 13 17 16 10 16 15 26 11 20 15 16 16 16 15 9 10 6 6 16 22 6 5 9 10 13 23 19 24 17 10 9 6 17 5 17 9 16 14 6 17 17 17 5 17 11 6 21 12 14 6 12 6 15 10 6 13 11 22 13 10 21 20 13 20 24 9 10 15 5 6 14 14 11 15 14 6 13 5 9 16 6 18 9 18 11 11 13 11 17 11 5 5 14 11 21 11 11 15 9 11 11 11 11 11 14 11 16 11 11 14 6 11 11 10 11 5 11 10 12 11 6 5 11 11 11 11 18 11 18 5 18 5 16 14 6 11 21 13 5 16 12 6 11 23 13 6 6 5 17 5 6 12 13 6 6 6 16 11 11 11 15 10 11 10 11 11 14 10 18 11 11 11 6 6 6 6 6 6 6 6 6 6 6 10 6 6 6 10 10 18 5 12 22 16 11 11 12 11 6 5 11 5 11 5 6 12 22 6 21 11 11 11 13 5 16 11 11 11 11 11 22 22 11 6 5 21 6 11 13 11 12 5 11 11 11 11 11 5 5 18 14 11 13 22 11 13 10 6 10 10 20 10 5 6 9 18 17 5 19 13 18 17 5 20 6 16 23 5 20 10 18 13 13 11 9 14 14 13 10 13 13 6 10 16 11 5 10 21 18 21 22 18 22 20 22 18 22 18 22 12 22 18 22 12 22 18 22 21 12 11 23 11 11 13 5 5 11 11 22 13 11 22 25 21 9 15 23 11 13 15 25 9 13 5 21 16 22 20 11 23 12 10 21 11 11 10 14 11 13 5 11 11 13 5 11 11 10 14 6 21 13 6 5 18 13 10 21 18 11 15 6 12 20 11 9 5 11 11 16 5 13 11 14 11 6 13 5 6 18 15 13 12 18 19 11 17 22 21 16 11 11 5 16 15 11 13 15 26 24 10 25 22 14 16 14 10 23 25 19 5 16 22 18 9 15 25 15 13 17 6 22 15 5 17 21 16 20 17 5 5 11 3 0\n",
            "2022-07-18 01:56:30,729 - tensorflow - INFO - input_mask (length = 1024): 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            "2022-07-18 01:56:30,731 - tensorflow - INFO - segment_ids (length = 1024): 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            "2022-07-18 01:56:30,734 - tensorflow - INFO - ex_data (length = 27): 1.362 0.0 0.0 0.0 2.0 0.0 1.6301830407172206 1.490357142857143 1.121 1.1959883667251408 1.1051515151515152 1.9023397761953205 1.857 1.1965913594926674 1.139 1.8416666666666668 1.8289117487818083 1.9339108116778956 1.982390718873006 1.998 2.0 1.1812586742231714 0.0 0.0 0.0 0.0 1.267880259614083\n",
            "2022-07-18 01:56:30,737 - tensorflow - INFO - label: 0 (id = 0)\n",
            "2022-07-18 01:56:30,760 - tensorflow - INFO - *** Example ***\n",
            "2022-07-18 01:56:30,761 - tensorflow - INFO - guid: test-2\n",
            "2022-07-18 01:56:30,766 - tensorflow - INFO - tokens (length = 1023): [CLS] E E E E E E D L I D G F A I A S F A T L E A L Q K D A S L Q P P E R L E H R L K H S G K R K R G G S S G A T G E P G D S S D R E P G R P P G D R A R K W P N K R R R K E A S S R H S L E A G Y I C D A E S D L D E R V S D D D L D P S F T V S T S K A S G P H G A F N G N C E A K L S V V P K V S G L E R S Q E Q P P G P D P L L V P F P P K E P P P P P V P R P P V S P P A P L P A T P S L P P P P Q P Q L Q L R V S P F G L R T S P Y G S S L D L S T G S S S R P P P K A P A P P V A Q P P P S S S S S S S S S S S A S S S S A Q L T H R P P T P S L P L P L S T H S F P P P G L R P P P P P H H P S L F S P G P T L P P P P P L L Q V P G H P G A S A A N A L S E Q D L I G Q D L N S R Y L N A Q G G P E V V G A G G S A R P L A F Q F H Q H N H Q H Q H T H Q H T H Q H F T P Y P P G L L P P H G P H M F E K Y P G K M E G L F R H N P Y T A F P P A V P G L P P G L P P A V S F G S L Q G A F Q P K S T N P E L P P R L G P V P S G L S Q K G T Q I P D H F R P P L R K P G K W C A M H V R V A Y M I L R H Q E K M K G D S H K L D F R N D L L P [SEP] E E E E E E D L I D G F A I A S F A T L E A L Q K D A S L Q P P E R L E H R L K H S G K R K R G G S S G A T G E P G D S S D R E P G R P P G D R A R K W P N K R R R K E A S S R H S L E A G Y I C D A E S D L D E R V S D D D L D P S F T V S T S K A S G P H G A F N G N C E A K L S V V P K V S G L E R S Q E Q P P G P D P L L V P F P P K E P P P P P V P R P P V S P P A P L P A T P S L P P P P Q P Q L Q L R V S P F G L R T S P Y G S S L D L S T G S S S R P P P K A P A P P V A Q P P P S S S S S S S S S S S A S S S Y A Q L T H R P P T P S L P L P L S T H S F P P P G L R P P P P P H H P S L F S P G P T L P P P P P L L Q V P G H P G A S A A N A L S E Q D L I G Q D L N S R Y L N A Q G G P E V V G A G G S A R P L A F Q F H Q H N H Q H Q H T H Q H T H Q H F T P Y P P G L L P P H G P H M F E K Y P G K M E G L F R H N P Y T A F P P A V P G L P P G L P P A V S F G S L Q G A F Q P K S T N P E L P P R L G P V P S G L S Q K G T Q I P D H F R P P L R K P G K W C A M H V R V A Y M I L R H Q E K M K G D S H K L D F R N D L L P [SEP]\n",
            "2022-07-18 01:56:30,771 - tensorflow - INFO - input_ids (length = 1024): 2 9 9 9 9 9 9 17 5 19 17 13 21 10 19 10 6 21 10 12 5 9 10 5 18 15 17 10 6 5 18 11 11 9 16 5 9 22 16 5 15 22 6 13 15 16 15 16 13 13 6 6 13 10 12 13 9 11 13 17 6 6 17 16 9 11 13 16 11 11 13 17 16 10 16 15 26 11 20 15 16 16 16 15 9 10 6 6 16 22 6 5 9 10 13 23 19 24 17 10 9 6 17 5 17 9 16 14 6 17 17 17 5 17 11 6 21 12 14 6 12 6 15 10 6 13 11 22 13 10 21 20 13 20 24 9 10 15 5 6 14 14 11 15 14 6 13 5 9 16 6 18 9 18 11 11 13 11 17 11 5 5 14 11 21 11 11 15 9 11 11 11 11 11 14 11 16 11 11 14 6 11 11 10 11 5 11 10 12 11 6 5 11 11 11 11 18 11 18 5 18 5 16 14 6 11 21 13 5 16 12 6 11 23 13 6 6 5 17 5 6 12 13 6 6 6 16 11 11 11 15 10 11 10 11 11 14 10 18 11 11 11 6 6 6 6 6 6 6 6 6 6 6 10 6 6 6 6 10 18 5 12 22 16 11 11 12 11 6 5 11 5 11 5 6 12 22 6 21 11 11 11 13 5 16 11 11 11 11 11 22 22 11 6 5 21 6 11 13 11 12 5 11 11 11 11 11 5 5 18 14 11 13 22 11 13 10 6 10 10 20 10 5 6 9 18 17 5 19 13 18 17 5 20 6 16 23 5 20 10 18 13 13 11 9 14 14 13 10 13 13 6 10 16 11 5 10 21 18 21 22 18 22 20 22 18 22 18 22 12 22 18 22 12 22 18 22 21 12 11 23 11 11 13 5 5 11 11 22 13 11 22 25 21 9 15 23 11 13 15 25 9 13 5 21 16 22 20 11 23 12 10 21 11 11 10 14 11 13 5 11 11 13 5 11 11 10 14 6 21 13 6 5 18 13 10 21 18 11 15 6 12 20 11 9 5 11 11 16 5 13 11 14 11 6 13 5 6 18 15 13 12 18 19 11 17 22 21 16 11 11 5 16 15 11 13 15 26 24 10 25 22 14 16 14 10 23 25 19 5 16 22 18 9 15 25 15 13 17 6 22 15 5 17 21 16 20 17 5 5 11 3 9 9 9 9 9 9 17 5 19 17 13 21 10 19 10 6 21 10 12 5 9 10 5 18 15 17 10 6 5 18 11 11 9 16 5 9 22 16 5 15 22 6 13 15 16 15 16 13 13 6 6 13 10 12 13 9 11 13 17 6 6 17 16 9 11 13 16 11 11 13 17 16 10 16 15 26 11 20 15 16 16 16 15 9 10 6 6 16 22 6 5 9 10 13 23 19 24 17 10 9 6 17 5 17 9 16 14 6 17 17 17 5 17 11 6 21 12 14 6 12 6 15 10 6 13 11 22 13 10 21 20 13 20 24 9 10 15 5 6 14 14 11 15 14 6 13 5 9 16 6 18 9 18 11 11 13 11 17 11 5 5 14 11 21 11 11 15 9 11 11 11 11 11 14 11 16 11 11 14 6 11 11 10 11 5 11 10 12 11 6 5 11 11 11 11 18 11 18 5 18 5 16 14 6 11 21 13 5 16 12 6 11 23 13 6 6 5 17 5 6 12 13 6 6 6 16 11 11 11 15 10 11 10 11 11 14 10 18 11 11 11 6 6 6 6 6 6 6 6 6 6 6 10 6 6 6 23 10 18 5 12 22 16 11 11 12 11 6 5 11 5 11 5 6 12 22 6 21 11 11 11 13 5 16 11 11 11 11 11 22 22 11 6 5 21 6 11 13 11 12 5 11 11 11 11 11 5 5 18 14 11 13 22 11 13 10 6 10 10 20 10 5 6 9 18 17 5 19 13 18 17 5 20 6 16 23 5 20 10 18 13 13 11 9 14 14 13 10 13 13 6 10 16 11 5 10 21 18 21 22 18 22 20 22 18 22 18 22 12 22 18 22 12 22 18 22 21 12 11 23 11 11 13 5 5 11 11 22 13 11 22 25 21 9 15 23 11 13 15 25 9 13 5 21 16 22 20 11 23 12 10 21 11 11 10 14 11 13 5 11 11 13 5 11 11 10 14 6 21 13 6 5 18 13 10 21 18 11 15 6 12 20 11 9 5 11 11 16 5 13 11 14 11 6 13 5 6 18 15 13 12 18 19 11 17 22 21 16 11 11 5 16 15 11 13 15 26 24 10 25 22 14 16 14 10 23 25 19 5 16 22 18 9 15 25 15 13 17 6 22 15 5 17 21 16 20 17 5 5 11 3 0\n",
            "2022-07-18 01:56:30,777 - tensorflow - INFO - input_mask (length = 1024): 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            "2022-07-18 01:56:30,778 - tensorflow - INFO - segment_ids (length = 1024): 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            "2022-07-18 01:56:30,781 - tensorflow - INFO - ex_data (length = 27): 1.094 0.0 0.0 0.0 2.0 0.0 1.6189764661935002 1.4539285714285715 1.102 1.209145046625427 1.135858585858586 1.2207527975584944 1.891 1.2217598097502973 1.187 1.8416666666666668 1.8364916080129943 1.9222008341353867 1.9747255023824322 1.9969999999999999 2.0 1.2413560625413136 0.0 0.0 0.0 0.0 1.1910830790090952\n",
            "2022-07-18 01:56:30,783 - tensorflow - INFO - label: 0 (id = 0)\n",
            "2022-07-18 01:56:30,809 - tensorflow - INFO - *** Example ***\n",
            "2022-07-18 01:56:30,813 - tensorflow - INFO - guid: test-3\n",
            "2022-07-18 01:56:30,816 - tensorflow - INFO - tokens (length = 1023): [CLS] E E E E E E D L I D G F A I A S F A T L E A L Q K D A S L Q P P E R L E H R L K H S G K R K R G G S S G A T G E P G D S S D R E P G R P P G D R A R K W P N K R R R K E A S S R H S L E A G Y I C D A E S D L D E R V S D D D L D P S F T V S T S K A S G P H G A F N G N C E A K L S V V P K V S G L E R S Q E Q P P G P D P L L V P F P P K E P P P P P V P R P P V S P P A P L P A T P S L P P P P Q P Q L Q L R V S P F G L R T S P Y G S S L D L S T G S S S R P P P K A P A P P V A Q P P P S S S S S S S S S S S A S S S S A Q L T H R P P T P S L P L P L S T H S F P P P G L R P P P P P H H P S L F S P G P T L P P P P P L L Q V P G H P G A S A A N A L S E Q D L I G Q D L N S R Y L N A Q G G P E V V G A G G S A R P L A F Q F H Q H N H Q H Q H T H Q H T H Q H F T P Y P P G L L P P H G P H M F E K Y P G K M E G L F R H N P Y T A F P P A V P G L P P G L P P A V S F G S L Q G A F Q P K S T N P E L P P R L G P V P S G L S Q K G T Q I P D H F R P P L R K P G K W C A M H V R V A Y M I L R H Q E K M K G D S H K L D F R N D L L P [SEP] E E E E E E D L I D G F A I A S F A T L E A L Q K D A S L Q P P E R L E H R L K H S G K R K R G G S S G A T G E P G D S S D R E P G R P P G D R A R K W P N K R R R K E A S S R H S L E A G Y I C D A E S D L D E R V S D D D L D P S F T V S T S K A S G P H G A F N G N C E A K L S V V P K V S G L E R S Q E Q P P G P D P L L V P F P P K E P P P P P V P R P P V S P P A P L P A T P S L P P P P Q P Q L Q L R V S P F G L R T S P Y G S S L D L S T G S S S R P P P K A P A P P V A Q P P P S S S S S S S S S S S A S S S C A Q L T H R P P T P S L P L P L S T H S F P P P G L R P P P P P H H P S L F S P G P T L P P P P P L L Q V P G H P G A S A A N A L S E Q D L I G Q D L N S R Y L N A Q G G P E V V G A G G S A R P L A F Q F H Q H N H Q H Q H T H Q H T H Q H F T P Y P P G L L P P H G P H M F E K Y P G K M E G L F R H N P Y T A F P P A V P G L P P G L P P A V S F G S L Q G A F Q P K S T N P E L P P R L G P V P S G L S Q K G T Q I P D H F R P P L R K P G K W C A M H V R V A Y M I L R H Q E K M K G D S H K L D F R N D L L P [SEP]\n",
            "2022-07-18 01:56:30,818 - tensorflow - INFO - input_ids (length = 1024): 2 9 9 9 9 9 9 17 5 19 17 13 21 10 19 10 6 21 10 12 5 9 10 5 18 15 17 10 6 5 18 11 11 9 16 5 9 22 16 5 15 22 6 13 15 16 15 16 13 13 6 6 13 10 12 13 9 11 13 17 6 6 17 16 9 11 13 16 11 11 13 17 16 10 16 15 26 11 20 15 16 16 16 15 9 10 6 6 16 22 6 5 9 10 13 23 19 24 17 10 9 6 17 5 17 9 16 14 6 17 17 17 5 17 11 6 21 12 14 6 12 6 15 10 6 13 11 22 13 10 21 20 13 20 24 9 10 15 5 6 14 14 11 15 14 6 13 5 9 16 6 18 9 18 11 11 13 11 17 11 5 5 14 11 21 11 11 15 9 11 11 11 11 11 14 11 16 11 11 14 6 11 11 10 11 5 11 10 12 11 6 5 11 11 11 11 18 11 18 5 18 5 16 14 6 11 21 13 5 16 12 6 11 23 13 6 6 5 17 5 6 12 13 6 6 6 16 11 11 11 15 10 11 10 11 11 14 10 18 11 11 11 6 6 6 6 6 6 6 6 6 6 6 10 6 6 6 6 10 18 5 12 22 16 11 11 12 11 6 5 11 5 11 5 6 12 22 6 21 11 11 11 13 5 16 11 11 11 11 11 22 22 11 6 5 21 6 11 13 11 12 5 11 11 11 11 11 5 5 18 14 11 13 22 11 13 10 6 10 10 20 10 5 6 9 18 17 5 19 13 18 17 5 20 6 16 23 5 20 10 18 13 13 11 9 14 14 13 10 13 13 6 10 16 11 5 10 21 18 21 22 18 22 20 22 18 22 18 22 12 22 18 22 12 22 18 22 21 12 11 23 11 11 13 5 5 11 11 22 13 11 22 25 21 9 15 23 11 13 15 25 9 13 5 21 16 22 20 11 23 12 10 21 11 11 10 14 11 13 5 11 11 13 5 11 11 10 14 6 21 13 6 5 18 13 10 21 18 11 15 6 12 20 11 9 5 11 11 16 5 13 11 14 11 6 13 5 6 18 15 13 12 18 19 11 17 22 21 16 11 11 5 16 15 11 13 15 26 24 10 25 22 14 16 14 10 23 25 19 5 16 22 18 9 15 25 15 13 17 6 22 15 5 17 21 16 20 17 5 5 11 3 9 9 9 9 9 9 17 5 19 17 13 21 10 19 10 6 21 10 12 5 9 10 5 18 15 17 10 6 5 18 11 11 9 16 5 9 22 16 5 15 22 6 13 15 16 15 16 13 13 6 6 13 10 12 13 9 11 13 17 6 6 17 16 9 11 13 16 11 11 13 17 16 10 16 15 26 11 20 15 16 16 16 15 9 10 6 6 16 22 6 5 9 10 13 23 19 24 17 10 9 6 17 5 17 9 16 14 6 17 17 17 5 17 11 6 21 12 14 6 12 6 15 10 6 13 11 22 13 10 21 20 13 20 24 9 10 15 5 6 14 14 11 15 14 6 13 5 9 16 6 18 9 18 11 11 13 11 17 11 5 5 14 11 21 11 11 15 9 11 11 11 11 11 14 11 16 11 11 14 6 11 11 10 11 5 11 10 12 11 6 5 11 11 11 11 18 11 18 5 18 5 16 14 6 11 21 13 5 16 12 6 11 23 13 6 6 5 17 5 6 12 13 6 6 6 16 11 11 11 15 10 11 10 11 11 14 10 18 11 11 11 6 6 6 6 6 6 6 6 6 6 6 10 6 6 6 24 10 18 5 12 22 16 11 11 12 11 6 5 11 5 11 5 6 12 22 6 21 11 11 11 13 5 16 11 11 11 11 11 22 22 11 6 5 21 6 11 13 11 12 5 11 11 11 11 11 5 5 18 14 11 13 22 11 13 10 6 10 10 20 10 5 6 9 18 17 5 19 13 18 17 5 20 6 16 23 5 20 10 18 13 13 11 9 14 14 13 10 13 13 6 10 16 11 5 10 21 18 21 22 18 22 20 22 18 22 18 22 12 22 18 22 12 22 18 22 21 12 11 23 11 11 13 5 5 11 11 22 13 11 22 25 21 9 15 23 11 13 15 25 9 13 5 21 16 22 20 11 23 12 10 21 11 11 10 14 11 13 5 11 11 13 5 11 11 10 14 6 21 13 6 5 18 13 10 21 18 11 15 6 12 20 11 9 5 11 11 16 5 13 11 14 11 6 13 5 6 18 15 13 12 18 19 11 17 22 21 16 11 11 5 16 15 11 13 15 26 24 10 25 22 14 16 14 10 23 25 19 5 16 22 18 9 15 25 15 13 17 6 22 15 5 17 21 16 20 17 5 5 11 3 0\n",
            "2022-07-18 01:56:30,820 - tensorflow - INFO - input_mask (length = 1024): 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            "2022-07-18 01:56:30,822 - tensorflow - INFO - segment_ids (length = 1024): 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            "2022-07-18 01:56:30,824 - tensorflow - INFO - ex_data (length = 27): 1.041 0.0 0.0 0.0 2.0 0.0 1.6189764661935002 1.4546428571428571 1.21 1.21184562828917 1.1423232323232324 1.7884028484231944 1.9060000000000001 1.25465715418153 1.224 1.8416666666666668 1.8364916080129943 1.9222008341353867 1.9747255023824322 1.9969999999999999 2.0 1.2413560625413136 0.0 0.0 0.0 0.0 1.2468879447627887\n",
            "2022-07-18 01:56:30,826 - tensorflow - INFO - label: 0 (id = 0)\n",
            "2022-07-18 01:56:30,851 - tensorflow - INFO - *** Example ***\n",
            "2022-07-18 01:56:30,852 - tensorflow - INFO - guid: test-4\n",
            "2022-07-18 01:56:30,854 - tensorflow - INFO - tokens (length = 1023): [CLS] E E E E E E D L I D G F A I A S F A T L E A L Q K D A S L Q P P E R L E H R L K H S G K R K R G G S S G A T G E P G D S S D R E P G R P P G D R A R K W P N K R R R K E A S S R H S L E A G Y I C D A E S D L D E R V S D D D L D P S F T V S T S K A S G P H G A F N G N C E A K L S V V P K V S G L E R S Q E Q P P G P D P L L V P F P P K E P P P P P V P R P P V S P P A P L P A T P S L P P P P Q P Q L Q L R V S P F G L R T S P Y G S S L D L S T G S S S R P P P K A P A P P V A Q P P P S S S S S S S S S S S A S S S S A Q L T H R P P T P S L P L P L S T H S F P P P G L R P P P P P H H P S L F S P G P T L P P P P P L L Q V P G H P G A S A A N A L S E Q D L I G Q D L N S R Y L N A Q G G P E V V G A G G S A R P L A F Q F H Q H N H Q H Q H T H Q H T H Q H F T P Y P P G L L P P H G P H M F E K Y P G K M E G L F R H N P Y T A F P P A V P G L P P G L P P A V S F G S L Q G A F Q P K S T N P E L P P R L G P V P S G L S Q K G T Q I P D H F R P P L R K P G K W C A M H V R V A Y M I L R H Q E K M K G D S H K L D F R N D L L P [SEP] E E E E E E D L I D G F A I A S F A T L E A L Q K D A S L Q P P E R L E H R L K H S G K R K R G G S S G A T G E P G D S S D R E P G R P P G D R A R K W P N K R R R K E A S S R H S L E A G Y I C D A E S D L D E R V S D D D L D P S F T V S T S K A S G P H G A F N G N C E A K L S V V P K V S G L E R S Q E Q P P G P D P L L V P F P P K E P P P P P V P R P P V S P P A P L P A T P S L P P P P Q P Q L Q L R V S P F G L R T S P Y G S S L D L S T G S S S R P P P K A P A P P V A Q P P P S S S S S S S S S S S A S S S F A Q L T H R P P T P S L P L P L S T H S F P P P G L R P P P P P H H P S L F S P G P T L P P P P P L L Q V P G H P G A S A A N A L S E Q D L I G Q D L N S R Y L N A Q G G P E V V G A G G S A R P L A F Q F H Q H N H Q H Q H T H Q H T H Q H F T P Y P P G L L P P H G P H M F E K Y P G K M E G L F R H N P Y T A F P P A V P G L P P G L P P A V S F G S L Q G A F Q P K S T N P E L P P R L G P V P S G L S Q K G T Q I P D H F R P P L R K P G K W C A M H V R V A Y M I L R H Q E K M K G D S H K L D F R N D L L P [SEP]\n",
            "2022-07-18 01:56:30,858 - tensorflow - INFO - input_ids (length = 1024): 2 9 9 9 9 9 9 17 5 19 17 13 21 10 19 10 6 21 10 12 5 9 10 5 18 15 17 10 6 5 18 11 11 9 16 5 9 22 16 5 15 22 6 13 15 16 15 16 13 13 6 6 13 10 12 13 9 11 13 17 6 6 17 16 9 11 13 16 11 11 13 17 16 10 16 15 26 11 20 15 16 16 16 15 9 10 6 6 16 22 6 5 9 10 13 23 19 24 17 10 9 6 17 5 17 9 16 14 6 17 17 17 5 17 11 6 21 12 14 6 12 6 15 10 6 13 11 22 13 10 21 20 13 20 24 9 10 15 5 6 14 14 11 15 14 6 13 5 9 16 6 18 9 18 11 11 13 11 17 11 5 5 14 11 21 11 11 15 9 11 11 11 11 11 14 11 16 11 11 14 6 11 11 10 11 5 11 10 12 11 6 5 11 11 11 11 18 11 18 5 18 5 16 14 6 11 21 13 5 16 12 6 11 23 13 6 6 5 17 5 6 12 13 6 6 6 16 11 11 11 15 10 11 10 11 11 14 10 18 11 11 11 6 6 6 6 6 6 6 6 6 6 6 10 6 6 6 6 10 18 5 12 22 16 11 11 12 11 6 5 11 5 11 5 6 12 22 6 21 11 11 11 13 5 16 11 11 11 11 11 22 22 11 6 5 21 6 11 13 11 12 5 11 11 11 11 11 5 5 18 14 11 13 22 11 13 10 6 10 10 20 10 5 6 9 18 17 5 19 13 18 17 5 20 6 16 23 5 20 10 18 13 13 11 9 14 14 13 10 13 13 6 10 16 11 5 10 21 18 21 22 18 22 20 22 18 22 18 22 12 22 18 22 12 22 18 22 21 12 11 23 11 11 13 5 5 11 11 22 13 11 22 25 21 9 15 23 11 13 15 25 9 13 5 21 16 22 20 11 23 12 10 21 11 11 10 14 11 13 5 11 11 13 5 11 11 10 14 6 21 13 6 5 18 13 10 21 18 11 15 6 12 20 11 9 5 11 11 16 5 13 11 14 11 6 13 5 6 18 15 13 12 18 19 11 17 22 21 16 11 11 5 16 15 11 13 15 26 24 10 25 22 14 16 14 10 23 25 19 5 16 22 18 9 15 25 15 13 17 6 22 15 5 17 21 16 20 17 5 5 11 3 9 9 9 9 9 9 17 5 19 17 13 21 10 19 10 6 21 10 12 5 9 10 5 18 15 17 10 6 5 18 11 11 9 16 5 9 22 16 5 15 22 6 13 15 16 15 16 13 13 6 6 13 10 12 13 9 11 13 17 6 6 17 16 9 11 13 16 11 11 13 17 16 10 16 15 26 11 20 15 16 16 16 15 9 10 6 6 16 22 6 5 9 10 13 23 19 24 17 10 9 6 17 5 17 9 16 14 6 17 17 17 5 17 11 6 21 12 14 6 12 6 15 10 6 13 11 22 13 10 21 20 13 20 24 9 10 15 5 6 14 14 11 15 14 6 13 5 9 16 6 18 9 18 11 11 13 11 17 11 5 5 14 11 21 11 11 15 9 11 11 11 11 11 14 11 16 11 11 14 6 11 11 10 11 5 11 10 12 11 6 5 11 11 11 11 18 11 18 5 18 5 16 14 6 11 21 13 5 16 12 6 11 23 13 6 6 5 17 5 6 12 13 6 6 6 16 11 11 11 15 10 11 10 11 11 14 10 18 11 11 11 6 6 6 6 6 6 6 6 6 6 6 10 6 6 6 21 10 18 5 12 22 16 11 11 12 11 6 5 11 5 11 5 6 12 22 6 21 11 11 11 13 5 16 11 11 11 11 11 22 22 11 6 5 21 6 11 13 11 12 5 11 11 11 11 11 5 5 18 14 11 13 22 11 13 10 6 10 10 20 10 5 6 9 18 17 5 19 13 18 17 5 20 6 16 23 5 20 10 18 13 13 11 9 14 14 13 10 13 13 6 10 16 11 5 10 21 18 21 22 18 22 20 22 18 22 18 22 12 22 18 22 12 22 18 22 21 12 11 23 11 11 13 5 5 11 11 22 13 11 22 25 21 9 15 23 11 13 15 25 9 13 5 21 16 22 20 11 23 12 10 21 11 11 10 14 11 13 5 11 11 13 5 11 11 10 14 6 21 13 6 5 18 13 10 21 18 11 15 6 12 20 11 9 5 11 11 16 5 13 11 14 11 6 13 5 6 18 15 13 12 18 19 11 17 22 21 16 11 11 5 16 15 11 13 15 26 24 10 25 22 14 16 14 10 23 25 19 5 16 22 18 9 15 25 15 13 17 6 22 15 5 17 21 16 20 17 5 5 11 3 0\n",
            "2022-07-18 01:56:30,862 - tensorflow - INFO - input_mask (length = 1024): 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            "2022-07-18 01:56:30,864 - tensorflow - INFO - segment_ids (length = 1024): 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
            "2022-07-18 01:56:30,865 - tensorflow - INFO - ex_data (length = 27): 1.06 0.0 0.0 0.0 2.0 0.0 1.6189764661935002 1.4485714285714286 1.174 1.2187471147631799 1.16010101010101 1.6856561546286877 1.8860000000000001 1.229290527150218 1.198 1.8416666666666668 1.8364916080129943 1.9222008341353867 1.9747255023824322 1.9969999999999999 2.0 1.2413560625413136 0.0 0.0 0.0 0.0 1.2059195639442402\n",
            "2022-07-18 01:56:30,867 - tensorflow - INFO - label: 0 (id = 0)\n",
            "2022-07-18 01:57:55,145 - tensorflow - INFO - Writing example 10000 of 1024000\n",
            "2022-07-18 01:59:21,687 - tensorflow - INFO - Writing example 20000 of 1024000\n",
            "2022-07-18 02:00:27,316 - tensorflow - INFO - Writing example 30000 of 1024000\n",
            "2022-07-18 02:01:52,705 - tensorflow - INFO - Writing example 40000 of 1024000\n",
            "2022-07-18 02:02:58,179 - tensorflow - INFO - Writing example 50000 of 1024000\n",
            "2022-07-18 02:04:22,971 - tensorflow - INFO - Writing example 60000 of 1024000\n",
            "2022-07-18 02:05:30,906 - tensorflow - INFO - Writing example 70000 of 1024000\n",
            "2022-07-18 02:06:56,107 - tensorflow - INFO - Writing example 80000 of 1024000\n",
            "2022-07-18 02:08:13,732 - tensorflow - INFO - Writing example 90000 of 1024000\n",
            "2022-07-18 02:09:26,404 - tensorflow - INFO - Writing example 100000 of 1024000\n",
            "2022-07-18 02:10:39,279 - tensorflow - INFO - Writing example 110000 of 1024000\n",
            "2022-07-18 02:12:02,412 - tensorflow - INFO - Writing example 120000 of 1024000\n",
            "2022-07-18 02:13:27,492 - tensorflow - INFO - Writing example 130000 of 1024000\n",
            "2022-07-18 02:14:41,620 - tensorflow - INFO - Writing example 140000 of 1024000\n",
            "2022-07-18 02:16:03,357 - tensorflow - INFO - Writing example 150000 of 1024000\n",
            "2022-07-18 02:17:17,281 - tensorflow - INFO - Writing example 160000 of 1024000\n",
            "2022-07-18 02:18:32,753 - tensorflow - INFO - Writing example 170000 of 1024000\n",
            "2022-07-18 02:19:47,953 - tensorflow - INFO - Writing example 180000 of 1024000\n",
            "2022-07-18 02:21:03,632 - tensorflow - INFO - Writing example 190000 of 1024000\n",
            "2022-07-18 02:22:25,652 - tensorflow - INFO - Writing example 200000 of 1024000\n",
            "2022-07-18 02:23:50,790 - tensorflow - INFO - Writing example 210000 of 1024000\n",
            "2022-07-18 02:25:16,795 - tensorflow - INFO - Writing example 220000 of 1024000\n",
            "2022-07-18 02:26:36,772 - tensorflow - INFO - Writing example 230000 of 1024000\n",
            "2022-07-18 02:27:51,379 - tensorflow - INFO - Writing example 240000 of 1024000\n",
            "2022-07-18 02:29:11,878 - tensorflow - INFO - Writing example 250000 of 1024000\n",
            "2022-07-18 02:30:38,555 - tensorflow - INFO - Writing example 260000 of 1024000\n",
            "2022-07-18 02:32:01,745 - tensorflow - INFO - Writing example 270000 of 1024000\n",
            "2022-07-18 02:33:20,463 - tensorflow - INFO - Writing example 280000 of 1024000\n",
            "2022-07-18 02:34:45,972 - tensorflow - INFO - Writing example 290000 of 1024000\n",
            "2022-07-18 02:36:11,150 - tensorflow - INFO - Writing example 300000 of 1024000\n",
            "2022-07-18 02:37:36,503 - tensorflow - INFO - Writing example 310000 of 1024000\n",
            "2022-07-18 02:39:02,825 - tensorflow - INFO - Writing example 320000 of 1024000\n",
            "2022-07-18 02:40:22,524 - tensorflow - INFO - Writing example 330000 of 1024000\n",
            "2022-07-18 02:41:45,350 - tensorflow - INFO - Writing example 340000 of 1024000\n",
            "2022-07-18 02:43:07,391 - tensorflow - INFO - Writing example 350000 of 1024000\n",
            "2022-07-18 02:44:30,133 - tensorflow - INFO - Writing example 360000 of 1024000\n",
            "2022-07-18 02:45:59,492 - tensorflow - INFO - Writing example 370000 of 1024000\n",
            "2022-07-18 02:47:14,768 - tensorflow - INFO - Writing example 380000 of 1024000\n",
            "2022-07-18 02:48:29,409 - tensorflow - INFO - Writing example 390000 of 1024000\n",
            "2022-07-18 02:49:39,035 - tensorflow - INFO - Writing example 400000 of 1024000\n",
            "2022-07-18 02:50:59,977 - tensorflow - INFO - Writing example 410000 of 1024000\n",
            "2022-07-18 02:52:27,518 - tensorflow - INFO - Writing example 420000 of 1024000\n",
            "2022-07-18 02:53:48,331 - tensorflow - INFO - Writing example 430000 of 1024000\n",
            "2022-07-18 02:54:45,638 - tensorflow - INFO - Writing example 440000 of 1024000\n",
            "2022-07-18 02:55:53,291 - tensorflow - INFO - Writing example 450000 of 1024000\n",
            "2022-07-18 02:57:20,437 - tensorflow - INFO - Writing example 460000 of 1024000\n",
            "2022-07-18 02:58:47,725 - tensorflow - INFO - Writing example 470000 of 1024000\n",
            "2022-07-18 03:00:14,953 - tensorflow - INFO - Writing example 480000 of 1024000\n",
            "2022-07-18 03:01:35,713 - tensorflow - INFO - Writing example 490000 of 1024000\n",
            "2022-07-18 03:02:45,548 - tensorflow - INFO - Writing example 500000 of 1024000\n",
            "2022-07-18 03:03:48,396 - tensorflow - INFO - Writing example 510000 of 1024000\n",
            "2022-07-18 03:05:11,079 - tensorflow - INFO - Writing example 520000 of 1024000\n",
            "2022-07-18 03:06:37,935 - tensorflow - INFO - Writing example 530000 of 1024000\n",
            "2022-07-18 03:07:58,649 - tensorflow - INFO - Writing example 540000 of 1024000\n",
            "2022-07-18 03:09:24,904 - tensorflow - INFO - Writing example 550000 of 1024000\n",
            "2022-07-18 03:10:20,123 - tensorflow - INFO - Writing example 560000 of 1024000\n",
            "2022-07-18 03:11:22,234 - tensorflow - INFO - Writing example 570000 of 1024000\n",
            "2022-07-18 03:12:44,176 - tensorflow - INFO - Writing example 580000 of 1024000\n",
            "2022-07-18 03:14:03,152 - tensorflow - INFO - Writing example 590000 of 1024000\n",
            "2022-07-18 03:15:18,453 - tensorflow - INFO - Writing example 600000 of 1024000\n",
            "2022-07-18 03:16:22,497 - tensorflow - INFO - Writing example 610000 of 1024000\n",
            "2022-07-18 03:17:30,098 - tensorflow - INFO - Writing example 620000 of 1024000\n",
            "2022-07-18 03:18:50,816 - tensorflow - INFO - Writing example 630000 of 1024000\n",
            "2022-07-18 03:20:03,740 - tensorflow - INFO - Writing example 640000 of 1024000\n",
            "2022-07-18 03:21:28,497 - tensorflow - INFO - Writing example 650000 of 1024000\n",
            "2022-07-18 03:22:52,800 - tensorflow - INFO - Writing example 660000 of 1024000\n",
            "2022-07-18 03:24:11,594 - tensorflow - INFO - Writing example 670000 of 1024000\n",
            "2022-07-18 03:25:22,394 - tensorflow - INFO - Writing example 680000 of 1024000\n",
            "2022-07-18 03:26:49,021 - tensorflow - INFO - Writing example 690000 of 1024000\n",
            "2022-07-18 03:28:14,611 - tensorflow - INFO - Writing example 700000 of 1024000\n",
            "2022-07-18 03:29:21,399 - tensorflow - INFO - Writing example 710000 of 1024000\n",
            "2022-07-18 03:30:39,623 - tensorflow - INFO - Writing example 720000 of 1024000\n",
            "2022-07-18 03:31:57,738 - tensorflow - INFO - Writing example 730000 of 1024000\n",
            "2022-07-18 03:33:16,519 - tensorflow - INFO - Writing example 740000 of 1024000\n",
            "2022-07-18 03:34:33,791 - tensorflow - INFO - Writing example 750000 of 1024000\n",
            "2022-07-18 03:35:55,069 - tensorflow - INFO - Writing example 760000 of 1024000\n",
            "2022-07-18 03:37:13,440 - tensorflow - INFO - Writing example 770000 of 1024000\n",
            "2022-07-18 03:38:39,346 - tensorflow - INFO - Writing example 780000 of 1024000\n",
            "2022-07-18 03:39:55,934 - tensorflow - INFO - Writing example 790000 of 1024000\n",
            "2022-07-18 03:41:22,182 - tensorflow - INFO - Writing example 800000 of 1024000\n",
            "2022-07-18 03:42:45,901 - tensorflow - INFO - Writing example 810000 of 1024000\n",
            "2022-07-18 03:44:10,946 - tensorflow - INFO - Writing example 820000 of 1024000\n",
            "2022-07-18 03:45:30,408 - tensorflow - INFO - Writing example 830000 of 1024000\n",
            "2022-07-18 03:46:49,035 - tensorflow - INFO - Writing example 840000 of 1024000\n",
            "2022-07-18 03:47:58,391 - tensorflow - INFO - Writing example 850000 of 1024000\n",
            "2022-07-18 03:49:18,054 - tensorflow - INFO - Writing example 860000 of 1024000\n",
            "2022-07-18 03:50:31,885 - tensorflow - INFO - Writing example 870000 of 1024000\n",
            "2022-07-18 03:51:56,630 - tensorflow - INFO - Writing example 880000 of 1024000\n",
            "2022-07-18 03:53:14,534 - tensorflow - INFO - Writing example 890000 of 1024000\n",
            "2022-07-18 03:54:26,379 - tensorflow - INFO - Writing example 900000 of 1024000\n",
            "2022-07-18 03:55:50,737 - tensorflow - INFO - Writing example 910000 of 1024000\n",
            "2022-07-18 03:57:17,700 - tensorflow - INFO - Writing example 920000 of 1024000\n",
            "2022-07-18 03:58:37,237 - tensorflow - INFO - Writing example 930000 of 1024000\n",
            "2022-07-18 04:00:01,533 - tensorflow - INFO - Writing example 940000 of 1024000\n",
            "2022-07-18 04:01:26,595 - tensorflow - INFO - Writing example 950000 of 1024000\n",
            "2022-07-18 04:02:52,926 - tensorflow - INFO - Writing example 960000 of 1024000\n",
            "2022-07-18 04:04:21,370 - tensorflow - INFO - Writing example 970000 of 1024000\n",
            "2022-07-18 04:05:38,439 - tensorflow - INFO - Writing example 980000 of 1024000\n",
            "2022-07-18 04:06:55,043 - tensorflow - INFO - Writing example 990000 of 1024000\n",
            "2022-07-18 04:08:12,699 - tensorflow - INFO - Writing example 1000000 of 1024000\n",
            "2022-07-18 04:09:40,811 - tensorflow - INFO - Writing example 1010000 of 1024000\n",
            "2022-07-18 04:10:48,203 - tensorflow - INFO - Writing example 1020000 of 1024000\n"
          ]
        }
      ],
      "source": [
        "#@markdown Maximum output data length (when using paired method, actual protein sequence length is about half of this value):\n",
        "MAX_SEQ_LENGTH = 1024 #@param {type:\"integer\"}\n",
        "#@markdown Whether or not to ensure all datapoints are used during prediction by using an extra trailing test dataset so no datapoints will be skipped due to the batch size. (This option should be used most of the time unless an extra trailing test dataset is a large problem)\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to split the data processing into shards (only for really large databases, since finetuning data typically isn't that large)\n",
        "USING_SHARDS = True #@param {type:\"boolean\"}\n",
        "#@markdown If USING_SHARDS, what shard size to use (how many lines/datapoints should be in each shard) (MUST BE DIVISIBLE BY \"MAX_BATCH_SIZE\")\n",
        "SHARD_SIZE = 1024000 #@param {type:\"integer\"}\n",
        "#@markdown * If USING_SHARDS, set this value to indicate which shard to start processing at (defualt 0 for first shard)\n",
        "START_SHARD =  53#@param {type:\"integer\"}\n",
        "#@markdown * If USING_SHARDS, set this value to indicate which shard to process until (not inclusive) (defualt -1 for last shard)\n",
        "END_SHARD =  54#@param {type:\"integer\"}\n",
        "#@markdown Which sets to generate out of train, dev, and test\n",
        "TRAIN = False #@param {type:\"boolean\"}\n",
        "DEV = False #@param {type:\"boolean\"}\n",
        "TEST = True #@param {type:\"boolean\"}\n",
        "#@markdown How many additional augmented copies to load (augmented copies refer to duplicates of the same sequence but with different clip locations. This parameter is defined by the \"run_classifier.py\" file in the \"mutformer_model_code\" folder):\n",
        "AUGMENT_COPIES_TRAIN =  0#@param{type:\"integer\"}\n",
        "\n",
        "DATA_GCS_DIR = BUCKET_PATH+\"/\"+OUTPUT_DATA_DIR\n",
        "data_folder_current = INPUT_DATA_DIR\n",
        "\n",
        "generate_data(MAX_SEQ_LENGTH,\n",
        "              data_folder_current,\n",
        "              DATA_GCS_DIR,\n",
        "              PRECISE_TESTING,\n",
        "              USING_SHARDS,\n",
        "              START_SHARD,\n",
        "              AUGMENT_COPIES_TRAIN,\n",
        "              SHARD_SIZE,\n",
        "              [TRAIN,DEV,TEST])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_TWmbWT5SJqg"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}