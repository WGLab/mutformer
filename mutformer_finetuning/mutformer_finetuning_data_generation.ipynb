{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mutformer_finetuning_data_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Finetuning Data Generation Script"
      ],
      "metadata": {
        "id": "O5WVrOdzC3K1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook processes tsv data and uploads the processed data to GCS to be used for finetuning MutFormer."
      ],
      "metadata": {
        "id": "Nq31-sx6C-eM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings/Mount Drive if needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown Whether or not this script is being run in a GCP runtime (if more memory is required for large databases)\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "#@markdown Which mode to use (a different mode means a different finetuning task): options are:\n",
        "#@markdown * \"MRPC\" - paired sequence method\n",
        "#@markdown * \"MRPC_w_ex_data\" - paired sequence method with external data\n",
        "#@markdown * \"RE\" - single sequence method\n",
        "#@markdown * \"NER\" - single sequence per residue prediction\n",
        "#@markdown \n",
        "#@markdown You can add more modes by creating a new processor and/or a new model_fn inside of the \"mutformer_model_code\" folder downloaded from github, then changing the corresponding code snippets in the code segment named \"Authorize for GCS, Imports, and General Setup\" (also edit the dropdown below).\n",
        "MODE = \"MRPC\" #@param   [\"MRPC_w_ex_data\", \"MRPC\", \"RE\", \"NER\"]   {type:\"string\"} \n",
        "            ####      ^^^^^ dropdown list for all modes ^^^^^\n",
        "\n",
        "#@markdown Name of the GCS bucket to use:\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ## IO Config\n",
        "#@markdown Input finetuning data folder: data will be read from here to be processed and uploaded to GCS (can be a drive path, or a GCS path if needed for large databases; must be a GCS path if using GCP_RUNTIME):\n",
        "#@markdown \n",
        "#@markdown * For processing multiple sets i.e. for multiple sequence lengths, simply store these sets into separate subfolders inside of the folder listed below, with each subfolder being named as specified in the following section.\n",
        "#@markdown \n",
        "#@markdown * For processing a single set, this folder should directly contain one dataset. \n",
        "#@markdown\n",
        "INPUT_DATA_DIR = \"gs://theodore_jiang/finetune_updated_data/MRPC_wo_preds\" #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "if not GCP_RUNTIME:                    ##if INPUT_DATA_DIR is a drive path,\n",
        "  if \"/content/drive\" in INPUT_DATA_DIR:   ##mount google drive\n",
        "    from google.colab import drive\n",
        "    if GCP_RUNTIME:\n",
        "      raise Exception(\"if GCP_RUNTIME, a GCS path must be used, since Google's cloud TPUs can only communicate with GCS and not drive\")\n",
        "    !fusermount -u /content/drive\n",
        "    drive.flush_and_unmount()\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "#@markdown Name of the folder in GCS to put processed data into: \n",
        "#@markdown * For generating multiple datasets i.e. for different sequence lengths, they will be written as individual subfolders inside of this folder.\n",
        "OUTPUT_DATA_DIR = \"MRPC_finetune_update_loaded\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "DATA_INFO = {      ##dictionary that will be uploaded alongside \n",
        "    \"mode\":MODE    ##each dataset to indicate its parameters\n",
        "}\n",
        "\n",
        "\n",
        "#### Vocabulary for the model (MutFormer uses the vocabulary below) ([PAD]\n",
        "#### [UNK],[CLS],[SEP], and [MASK] are necessary default tokens; B and J\n",
        "#### are markers for the beginning and ending of a protein sequence,\n",
        "#### respectively; the rest are all amino acids possible, ranked \n",
        "#### approximately by frequency of occurence in human population)\n",
        "#### vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
        "vocab = \\\n",
        "'''[PAD]\n",
        "[UNK]\n",
        "[CLS]\n",
        "[SEP]\n",
        "[MASK]\n",
        "L\n",
        "S\n",
        "B\n",
        "J\n",
        "E\n",
        "A\n",
        "P\n",
        "T\n",
        "G\n",
        "V\n",
        "K\n",
        "R\n",
        "D\n",
        "Q\n",
        "I\n",
        "N\n",
        "F\n",
        "H\n",
        "Y\n",
        "C\n",
        "M\n",
        "W'''\n",
        "with open(\"vocab.txt\", \"w\") as fo:\n",
        "  for token in vocab.split(\"\\n\"):\n",
        "    fo.write(token+\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA4ieYajd-Ht"
      },
      "source": [
        "#If running on a GCP runtime, follow these instructions to set it up:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSUONYA5id9M"
      },
      "source": [
        "###1) Create a VM from the GCP website\n",
        "###2) Open a command prompt on your computer and perform the following steps\"\n",
        "To ssh into the VM, run:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Note: Make sure the port above matches the port below (in this case it's 8888)\n",
        "\\\n",
        "\\\n",
        "In the new command prompt that popped out, either run each of the commands below individually, or copy and paste the one liner below:\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "One command:\n",
        "```\n",
        "sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "###3) In this notebook, click the \"connect to local runtime\" option under the connect button, and copy and paste the link outputted by command prompt with \"locahost: ...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1PvmBO8eR00"
      },
      "source": [
        "#Clone the MutFormer repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vKz_tKFeO0s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b39dc47e-251b-49fc-9d97-94640b59be09"
      },
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git-all\n",
        "#@markdown Where to clone the repo into:\n",
        "REPO_DESTINATION_PATH = \"mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mutformer'...\n",
            "remote: Enumerating objects: 614, done.\u001b[K\n",
            "remote: Counting objects: 100% (415/415), done.\u001b[K\n",
            "remote: Compressing objects: 100% (335/335), done.\u001b[K\n",
            "remote: Total 614 (delta 299), reused 111 (delta 78), pack-reused 199\u001b[K\n",
            "Receiving objects: 100% (614/614), 2.14 MiB | 7.06 MiB/s, done.\n",
            "Resolving deltas: 100% (410/410), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Authorize for GCS, Imports, and General Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7605c017-0569-40e2-8d4f-9c2418ac03f0"
      },
      "source": [
        "if not GCP_RUNTIME:\n",
        "  from google.colab import auth\n",
        "  print(\"Authorize for GCS:\")\n",
        "  auth.authenticate_user()\n",
        "  print(\"Authorize done\")\n",
        "\n",
        "  %tensorflow_version 1.x\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import importlib\n",
        "\n",
        "if REPO_DESTINATION_PATH == \"mutformer\":\n",
        "  if os.path.exists(\"mutformer_code\"):\n",
        "    shutil.rmtree(\"mutformer_code\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH,\"mutformer_code\")\n",
        "  REPO_DESTINATION_PATH = \"mutformer_code\"\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic  #### <<<<< if you added more modes, change these imports to import the correct processors         \n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor,MrpcWithExDataProcessor            #### <<<<< and correct training scripts (i.e. run_classifier and run_ner_for_pathogenic)\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor                                \n",
        "\n",
        "##reload modules so that you don't need to restart the runtime to reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_classifier,\n",
        "                  run_ner_for_pathogenic]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\":      ####       vvvvv if you added more modes, change this part to set the processors and training scripts correctly vvvvv\n",
        "  processor = MrpcProcessor()\n",
        "  script = run_classifier\n",
        "  USE_EX_DATA = False\n",
        "elif MODE==\"MRPC_w_ex_data\":\n",
        "  processor = MrpcWithExDataProcessor()\n",
        "  script = run_classifier\n",
        "  USE_EX_DATA = True\n",
        "elif MODE==\"RE\":\n",
        "  processor = REProcessor()\n",
        "  script = run_classifier\n",
        "  USE_EX_DATA = False\n",
        "elif MODE==\"NER\":\n",
        "  processor = NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "  USE_EX_DATA = False\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\", \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=\"vocab.txt\", do_lower_case=False)\n",
        "                      ####       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authorize for GCS:\n",
            "Authorize done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lXDH9WeQWGw"
      },
      "source": [
        "###General setup and definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUXoN_qYQZOA"
      },
      "source": [
        "#@markdown Maximum batch size the finetuning_benchmark script can handle without OOM (must be divisible by NUM_TPU_CORES_WHEN_TESTING):\n",
        "MAX_BATCH_SIZE =  1024 #@param {type:\"integer\"}\n",
        "#@markdown How many tpu cores will be used during evaluation and prediction (for colab runtimes, it's 8):\n",
        "NUM_TPU_CORES_WHEN_TESTING = 8 #@param {type:\"integer\"}\n",
        "\n",
        "def generate_data(MAX_SEQ_LENGTH,\n",
        "                  data_folder_current,\n",
        "                  DATA_GCS_DIR,\n",
        "                  PRECISE_TESTING,\n",
        "                  USING_SHARDS,\n",
        "                  SHARD_SIZE):  \n",
        "\n",
        "  try:\n",
        "    print(\"\\nUpdating and uploading data info json...\\n\")\n",
        "    DATA_INFO[\"sequence_length\"] = MAX_SEQ_LENGTH    ##update data info with sequence length\n",
        "\n",
        "    if USE_EX_DATA:                         ##if using external data, update data \n",
        "      def get_ex_data_num(file):            ##info with the # of external datapoints being used\n",
        "        ex_data = tf.gfile.Open(file).read().split(\"\\n\")[0].split(\"\\t\")[3].split()\n",
        "        return len(ex_data)\n",
        "      DATA_INFO[\"ex_data_num\"] = get_ex_data_num(data_folder_current+\"/\"+\\\n",
        "                                                 [file for file in tf.io.gfile.listdir(data_folder_current) \n",
        "                                                 if file.endswith(\".tsv\")][0])            ##just get the first tsv in the folder\n",
        "    \n",
        "    with tf.gfile.Open(DATA_GCS_DIR+\"/info.json\",\"w+\") as out: ##writes out a dictionary containing\n",
        "      json.dump(DATA_INFO,out,indent=2)                           ##the dataset's parameters\n",
        "    print(\"Data info json uploaded successfully\")\n",
        "  except Exception as e:\n",
        "    print(\"could not update and upload data info json. Error:\",e)\n",
        "\n",
        "\n",
        "  try:\n",
        "    print(\"\\nGenerating train set...\\n\")\n",
        "    if USING_SHARDS:\n",
        "      rd_rg = [0,SHARD_SIZE]\n",
        "      i=0\n",
        "    else:\n",
        "      rd_rg = None\n",
        "    while True:\n",
        "      train_examples = processor.get_train_examples(data_folder_current,read_range=rd_rg)\n",
        "      if len(train_examples) == 0:\n",
        "        break\n",
        "      train_file = os.path.join(DATA_GCS_DIR, \"train.tf_record\")\n",
        "      if USING_SHARDS:\n",
        "        train_file+=\"_\"+str(i)\n",
        "      script.file_based_convert_examples_to_features(\n",
        "          train_examples, label_list, MAX_SEQ_LENGTH, tokenizer, train_file)\n",
        "      if not USING_SHARDS:\n",
        "        break\n",
        "      else:\n",
        "        rd_rg = [pt+SHARD_SIZE for pt in rd_rg]\n",
        "        i+=1\n",
        "  except Exception as e:\n",
        "    print(\"training data generation failed. Error:\",e)\n",
        "\n",
        "  try:\n",
        "    print(\"\\nGenerating eval set...\\n\")\n",
        "    if USING_SHARDS:\n",
        "      rd_rg = [0,SHARD_SIZE]\n",
        "      i=0\n",
        "    else:\n",
        "      rd_rg = None\n",
        "    while True:\n",
        "      eval_examples = processor.get_dev_examples(data_folder_current,read_range=rd_rg)\n",
        "      if len(eval_examples) == 0:\n",
        "        break\n",
        "      eval_file = os.path.join(DATA_GCS_DIR, \"eval.tf_record\")\n",
        "      if USING_SHARDS:\n",
        "        eval_file+=\"_\"+str(i)\n",
        "      script.file_based_convert_examples_to_features(\n",
        "          eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer, eval_file)\n",
        "      if not USING_SHARDS:\n",
        "        break\n",
        "      else:\n",
        "        rd_rg = [pt+SHARD_SIZE for pt in rd_rg]\n",
        "        i+=1\n",
        "  except Exception as e:\n",
        "    print(\"eval data generation failed. Error:\",e)\n",
        "\n",
        "  try:\n",
        "    print(\"\\nGenerating test set...\\n\")\n",
        "    if USING_SHARDS:\n",
        "      rd_rg = [0,SHARD_SIZE]\n",
        "      i=0\n",
        "    else:\n",
        "      rd_rg = None\n",
        "    while True:\n",
        "      test_examples = processor.get_test_examples(data_folder_current,read_range=rd_rg)\n",
        "      if len(test_examples) == 0:\n",
        "        break\n",
        "      test_file = os.path.join(DATA_GCS_DIR, \"test.tf_record\")\n",
        "      if USING_SHARDS:\n",
        "        test_file+=\"_\"+str(i)\n",
        "      ## if using precise testing, the data will be split into two sets: \n",
        "      ## one set will be able to be predicted on the maximum possible batch \n",
        "      ## size, while the other will be predicted on a batch size of one, to \n",
        "      ##ensure the fastest prediction without leaving out any datapoints\n",
        "      if PRECISE_TESTING and len(test_examples)<SHARD_SIZE:\n",
        "        test_file_trailing = os.path.join(DATA_GCS_DIR, \"test_trailing.tf_record\")\n",
        "        def largest_mutiple_under_max(max,multiple_base):\n",
        "          return int(max/multiple_base)*multiple_base\n",
        "\n",
        "        split = largest_mutiple_under_max(len(test_examples),MAX_BATCH_SIZE)\n",
        "        test_examples_head = test_examples[:split]\n",
        "        test_examples_trailing = test_examples[split:]\n",
        "        script.file_based_convert_examples_to_features(\n",
        "            test_examples_head, label_list, MAX_SEQ_LENGTH, tokenizer, test_file)\n",
        "        script.file_based_convert_examples_to_features(\n",
        "            test_examples_trailing, label_list, MAX_SEQ_LENGTH, tokenizer, test_file_trailing)\n",
        "      else:\n",
        "        script.file_based_convert_examples_to_features(\n",
        "            test_examples, label_list, MAX_SEQ_LENGTH, tokenizer, test_file)\n",
        "      if not USING_SHARDS:\n",
        "        break\n",
        "      else:\n",
        "        rd_rg = [pt+SHARD_SIZE for pt in rd_rg]\n",
        "        i+=1\n",
        "  except Exception as e:\n",
        "    print(\"testing data generation failed. Error:\",e)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generation ops"
      ],
      "metadata": {
        "id": "ED2rhitTCdjm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are currently two data generations ops (more can be added):\n",
        "1. Varying sequence lengths: multiple sets of different sequence lengths will be generated\n",
        "  * Store multiple individual datasets as subfolders inside of Input finetuning data folder, with each folder named its corresponding sequence length.\n",
        "2. Only one dataset: a single dataset with a specified set of parameters will be generated \n",
        "  * Directly store only the files train.tsv, dev.tsv, and test.tsv for one dataset inside Input finetuning data folder"
      ],
      "metadata": {
        "id": "rhUajg5kClz2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TWmbWT5SJqg"
      },
      "source": [
        "####Varying sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "source": [
        "#@markdown List of maximum sequence lengths to generate data for\n",
        "MAX_SEQ_LENGTHS = [1024,512,256,128,64] #@param\n",
        "#@markdown Whether or not to ensure all datapoints are used during prediction by using an extra trailing test dataset so no datapoints will be skipped due to the batch size. (This option should be used unless an extra trailing test dataset is a large problem)\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to split the data processing into (for really large databases, since finetuning data typically isn't that large)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown If USING_SHARDS, what shard size to use (how many lines/datapoints should be in each shard) (MUST BE DIVISIBLE BY \"MAX_BATCH_SIZE\")\n",
        "SHARD_SIZE = 1024000 #@param {type:\"integer\"}\n",
        "\n",
        "for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS:\n",
        "  print(\"\\n\\nGenerating data for seq length:\",MAX_SEQ_LENGTH,\"\\n\\n\")\n",
        "  DATA_GCS_DIR = BUCKET_PATH+\"/\"+OUTPUT_DATA_DIR +\"/\"+ str(MAX_SEQ_LENGTH)\n",
        "  data_folder_current= INPUT_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "  generate_data(MAX_SEQ_LENGTH,\n",
        "                data_folder_current,\n",
        "                DATA_GCS_DIR,\n",
        "                PRECISE_TESTING,\n",
        "                USING_SHARDS,\n",
        "                SHARD_SIZE)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEOfXa4WiB2N"
      },
      "source": [
        "###Only one dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IytLW0VbgOZz"
      },
      "source": [
        "#@markdown Maximum output data length (when using paired method, actual protein sequence length is about half of this value):\n",
        "MAX_SEQ_LENGTH = 512 #@param {type:\"integer\"}\n",
        "#@markdown Whether or not to ensure all datapoints are used during prediction by using an extra trailing test dataset so no datapoints will be skipped due to the batch size. (This option should be used most of the time unless an extra trailing test dataset is a large problem)\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to split the data processing into (for really large databases, since finetuning data typically isn't that large)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown If USING_SHARDS, what shard size to use (how many lines/datapoints should be in each shard) (MUST BE DIVISIBLE BY \"MAX_BATCH_SIZE\")\n",
        "SHARD_SIZE = 1024000 #@param {type:\"integer\"}\n",
        "\n",
        "DATA_GCS_DIR = BUCKET_PATH+\"/\"+OUTPUT_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "data_folder_current = INPUT_DATA_DIR\n",
        "\n",
        "generate_data(MAX_SEQ_LENGTH,\n",
        "              data_folder_current,\n",
        "              DATA_GCS_DIR,\n",
        "              PRECISE_TESTING,\n",
        "              USING_SHARDS,\n",
        "              SHARD_SIZE)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}