{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5WVrOdzC3K1"
      },
      "source": [
        "#Finetuning Data Generation Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq31-sx6C-eM"
      },
      "source": [
        "This notebook processes tsv data and uploads the processed data to GCS to be used for finetuning MutFormer."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downgrade Python and Tensorflow \n",
        "\n",
        "(the default python version in Colab does not support Tensorflow 1.15)\n",
        "\n",
        "* **Note** that because the Python used in this notebook is not the default path, syntax highlighting most likely will not function."
      ],
      "metadata": {
        "id": "RE7RL-X21OzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. First, download and install Python version 3.7:"
      ],
      "metadata": {
        "id": "XUczf3xHRSJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py37_22.11.1-1-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py37\" --user"
      ],
      "metadata": {
        "id": "P_QFvdx0RTKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Then, reload the webpage (not restart runtime) to allow Colab to recognize the newly installed python\n",
        "####3. Finally, run the following commands to install tensorflow 1.15:"
      ],
      "metadata": {
        "id": "2oAnf7kcRUAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow==1.15"
      ],
      "metadata": {
        "id": "bOlnFkJ11OEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings/Mount Drive if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "outputs": [],
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown Whether or not this script is being run in a GCP runtime (if more memory is required for large databases)\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "#@markdown Which mode to use (a different mode means a different finetuning task): options are:\n",
        "#@markdown * \"MRPC\" - paired sequence method\n",
        "#@markdown * \"MRPC_w_ex_data\" - paired sequence method with external data\n",
        "#@markdown * \"RE\" - single sequence method\n",
        "#@markdown * \"NER\" - single sequence per residue prediction\n",
        "#@markdown \n",
        "#@markdown You can add more modes by creating a new processor and/or a new model_fn inside of the \"mutformer_model_code\" folder downloaded from github, then changing the corresponding code snippets in the code segment named \"Authorize for GCS, Imports, and General Setup\" (also edit the dropdown below).\n",
        "MODE = \"MRPC_w_ex_data\" #@param   [\"MRPC_w_ex_data\", \"MRPC\", \"RE\", \"NER\"]   {type:\"string\"} \n",
        "            ####      ^^^^^ dropdown list for all modes ^^^^^\n",
        "\n",
        "#@markdown Name of the GCS bucket to use (Make sure to set this to the name of your own GCS  bucket):\n",
        "BUCKET_NAME = \"\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ## IO Config\n",
        "#@markdown Input finetuning data folder: data will be read from here to be processed and uploaded to GCS (can be a drive path, or a GCS path if needed for large databases; must be a GCS path if using GCP_RUNTIME):\n",
        "#@markdown \n",
        "#@markdown * For processing multiple sets i.e. for multiple sequence lengths, simply store these sets into separate subfolders inside of the folder listed below, with each subfolder being named as specified in the following section.\n",
        "#@markdown \n",
        "#@markdown * For processing a single set, this folder should directly contain one dataset.\n",
        "#@markdown\n",
        "INPUT_DATA_DIR = \"gs://theodore_jiang/updated_all_snp_prediction_data\" #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "if not GCP_RUNTIME:                    ##if INPUT_DATA_DIR is a drive path,\n",
        "  if \"/content/drive\" in INPUT_DATA_DIR:   ##mount google drive\n",
        "    from google.colab import drive\n",
        "    if GCP_RUNTIME:\n",
        "      raise Exception(\"if GCP_RUNTIME, a GCS path must be used, since Google's cloud TPUs can only communicate with GCS and not drive\")\n",
        "    !fusermount -u /content/drive\n",
        "    drive.flush_and_unmount()\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "\n",
        "#@markdown Name of the folder in GCS to put processed data into: \n",
        "#@markdown * For generating multiple datasets i.e. for different sequence lengths, they will be written as individual subfolders inside of this folder.\n",
        "OUTPUT_DATA_DIR = \"all_snp_prediction_data_loaded\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "DATA_INFO = {      ##dictionary that will be uploaded alongside \n",
        "    \"mode\":MODE    ##each dataset to indicate its parameters\n",
        "}\n",
        "\n",
        "\n",
        "#### Vocabulary for the model (MutFormer uses the vocabulary below) ([PAD]\n",
        "#### [UNK],[CLS],[SEP], and [MASK] are necessary default tokens; B and J\n",
        "#### are markers for the beginning and ending of a protein sequence,\n",
        "#### respectively; the rest are all amino acids possible, ranked \n",
        "#### approximately by frequency of occurence in human population)\n",
        "#### vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
        "vocab = \\\n",
        "'''[PAD]\n",
        "[UNK]\n",
        "[CLS]\n",
        "[SEP]\n",
        "[MASK]\n",
        "L\n",
        "S\n",
        "B\n",
        "J\n",
        "E\n",
        "A\n",
        "P\n",
        "T\n",
        "G\n",
        "V\n",
        "K\n",
        "R\n",
        "D\n",
        "Q\n",
        "I\n",
        "N\n",
        "F\n",
        "H\n",
        "Y\n",
        "C\n",
        "M\n",
        "W'''\n",
        "with open(\"vocab.txt\", \"w\") as fo:\n",
        "  for token in vocab.split(\"\\n\"):\n",
        "    fo.write(token+\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA4ieYajd-Ht"
      },
      "source": [
        "#If running on a GCP runtime, follow these instructions to set it up:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSUONYA5id9M"
      },
      "source": [
        "###1) Create a VM from the GCP website\n",
        "###2) Open a command prompt on your computer and perform the following steps\"\n",
        "To ssh into the VM, run:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Note: Make sure the port above matches the port below (in this case it's 8888)\n",
        "\\\n",
        "\\\n",
        "In the new command prompt that popped out, either run each of the commands below individually, or copy and paste the one liner below:\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "One command:\n",
        "```\n",
        "sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "###3) In this notebook, click the \"connect to local runtime\" option under the connect button, and copy and paste the link outputted by command prompt with \"locahost: ...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1PvmBO8eR00"
      },
      "source": [
        "#Clone the MutFormer repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vKz_tKFeO0s"
      },
      "outputs": [],
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git-all\n",
        "#@markdown Where to clone the repo into:\n",
        "REPO_DESTINATION_PATH = \"mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Authorize for GCS, Imports, and General Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S4CiOh3RzFW"
      },
      "outputs": [],
      "source": [
        "#@markdown Whether to use link authorization for GCS (link authorization allows connection to another account other than the one running the script, while normal authorization disables connecting to different accounts):\n",
        "LINK_AUTHORIZATION = False #@param {type:\"boolean\"}\n",
        "\n",
        "if not GCP_RUNTIME:\n",
        "  from google.colab import auth\n",
        "  print(\"Authorize for GCS:\")\n",
        "  if not LINK_AUTHORIZATION: \n",
        "    auth.authenticate_user()\n",
        "  else: \n",
        "    !gcloud auth login --no-launch-browser\n",
        "  print(\"Authorize done\")\n",
        "  \n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "import importlib\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "if REPO_DESTINATION_PATH == \"mutformer\":\n",
        "  if os.path.exists(\"mutformer_code\"):\n",
        "    shutil.rmtree(\"mutformer_code\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH,\"mutformer_code\")\n",
        "  REPO_DESTINATION_PATH = \"mutformer_code\"\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic  #### <<<<< if you added more modes, change these imports to import the correct processors         \n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor,MrpcWithExDataProcessor            #### <<<<< and correct training scripts (i.e. run_classifier and run_ner_for_pathogenic)\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor                                \n",
        "\n",
        "##reload modules so that you don't need to restart the runtime to reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_classifier,\n",
        "                  run_ner_for_pathogenic]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\":      ####       vvvvv if you added more modes, change this part to set the processors and training scripts correctly vvvvv\n",
        "  processor = run_classifier.MrpcProcessor()\n",
        "  script = run_classifier\n",
        "  USE_EX_DATA = False\n",
        "elif MODE==\"MRPC_w_ex_data\":\n",
        "  processor = run_classifier.MrpcWithExDataProcessor()\n",
        "  script = run_classifier\n",
        "  USE_EX_DATA = True\n",
        "elif MODE==\"RE\":\n",
        "  processor = run_classifier.REProcessor()\n",
        "  script = run_classifier\n",
        "  USE_EX_DATA = False\n",
        "elif MODE==\"NER\":\n",
        "  processor = run_ner_for_pathogenic.NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "  USE_EX_DATA = False\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\", \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=\"vocab.txt\", do_lower_case=False)\n",
        "                      ####       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lXDH9WeQWGw"
      },
      "source": [
        "###General setup and definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUXoN_qYQZOA"
      },
      "outputs": [],
      "source": [
        "#@markdown Maximum batch size the finetuning_benchmark script can handle without OOM (must be divisible by NUM_TPU_CORES_WHEN_TESTING):\n",
        "MAX_BATCH_SIZE =  1024 #@param {type:\"integer\"}\n",
        "#@markdown How many tpu cores will be used during evaluation and prediction (for colab runtimes, it's 8):\n",
        "NUM_TPU_CORES_WHEN_TESTING = 8 #@param {type:\"integer\"}\n",
        "\n",
        "def generate_data(MAX_SEQ_LENGTH,\n",
        "                  data_folder_current,\n",
        "                  DATA_GCS_DIR,\n",
        "                  PRECISE_TESTING,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  AUGMENT_COPIES_TRAIN,\n",
        "                  SHARD_SIZE,\n",
        "                  GENERATE_SETS):\n",
        "\n",
        "  try:\n",
        "    print(\"\\nUpdating and uploading data info json...\\n\")\n",
        "    DATA_INFO[\"sequence_length\"] = MAX_SEQ_LENGTH    ##update data info with sequence length\n",
        "\n",
        "    if USE_EX_DATA:                         ##if using external data, update data \n",
        "      def get_ex_data_num(file):            ##info with the # of external datapoints being used\n",
        "        with tf.gfile.Open(file) as filein: \n",
        "          while True:\n",
        "            line = filein.readline().strip()\n",
        "            if line:\n",
        "              ex_data = line.split(\"\\t\")[3].split()\n",
        "              return len(ex_data)\n",
        "      DATA_INFO[\"ex_data_num\"] = get_ex_data_num(data_folder_current+\"/\"+tf.io.gfile.listdir(data_folder_current)[0])\n",
        "    \n",
        "    with tf.gfile.Open(DATA_GCS_DIR+\"/info.json\",\"w+\") as out: ##writes out a dictionary containing\n",
        "      json.dump(DATA_INFO,out,indent=2)                           ##the dataset's parameters\n",
        "    print(\"Data info json uploaded successfully\")\n",
        "  except Exception as e:\n",
        "    print(\"could not update and upload data info json. Error:\",e)\n",
        "\n",
        "              \n",
        "  def get_or_create_shards(infile, SHARD_SIZE, START_SHARD, END_SHARD):\n",
        "      shard_files = []\n",
        "      with tf.gfile.Open(infile) as filein:\n",
        "          shard_ind = START_SHARD\n",
        "          read_ind = -1\n",
        "          while True:\n",
        "              current_start_line = shard_ind * SHARD_SIZE\n",
        "              if shard_ind == END_SHARD: break\n",
        "              shard_file = f\"{infile}_(shardsize_{SHARD_SIZE})_shard_{shard_ind}\"\n",
        "              shard_files.append([shard_file, shard_ind])\n",
        "              if not tf.io.gfile.exists(shard_file):\n",
        "                  with tf.gfile.Open(shard_file, \"w+\") as shardout:\n",
        "                      wroteout = 0\n",
        "                      for line in tqdm(filein, f\"creating shard number {shard_ind}\"):\n",
        "                          if not line.strip():\n",
        "                              continue\n",
        "                          read_ind += 1\n",
        "                          if read_ind < current_start_line:\n",
        "                              continue\n",
        "                          shardout.write(line)\n",
        "                          wroteout += 1\n",
        "                          if wroteout == SHARD_SIZE:\n",
        "                              break\n",
        "                      if wroteout == 0:\n",
        "                          shardout.close()\n",
        "                          del shard_files[-1]\n",
        "                          break\n",
        "                      if wroteout < SHARD_SIZE:\n",
        "                          break\n",
        "              shard_ind += 1\n",
        "      return shard_files\n",
        "  \n",
        "\n",
        "  DO_TRAIN, DO_DEV, DO_TEST = GENERATE_SETS\n",
        "  \n",
        "\n",
        "  if DO_TRAIN:\n",
        "    try:\n",
        "      print(\"\\nGenerating train set...\\n\")\n",
        "      train_data_input_file = processor.get_train_file(data_folder_current)\n",
        "      if USING_SHARDS:\n",
        "        shards = get_or_create_shards(train_data_input_file,SHARD_SIZE//(AUGMENT_COPIES_TRAIN+1),START_SHARD,END_SHARD)\n",
        "      else:\n",
        "        shards = [train_data_input_file,None]\n",
        "      for shard,shard_ind in shards:\n",
        "        if USING_SHARDS: print(f\"generating data for shard number {shard_ind}\")\n",
        "        train_examples = processor._create_examples(processor._read_tsv(shard),\"train\")\n",
        "        if len(train_examples) == 0:\n",
        "          raise Exception(\"no data present in the train dataset\")\n",
        "        train_file = os.path.join(DATA_GCS_DIR, \"train.tf_record\")\n",
        "        if USING_SHARDS:\n",
        "          train_file+=\"_\"+str(shard_ind)\n",
        "        script.file_based_convert_examples_to_features(train_examples, \n",
        "                                                      label_list, \n",
        "                                                      MAX_SEQ_LENGTH, \n",
        "                                                      tokenizer, \n",
        "                                                      train_file,\n",
        "                                                      augmented_data_copies=AUGMENT_COPIES_TRAIN,\n",
        "                                                      shuffle_data=True)\n",
        "    except Exception as e:\n",
        "      print(\"train dataset generation failed. Error:\",e)\n",
        "\n",
        "  if DO_DEV:\n",
        "    try:\n",
        "      print(\"\\nGenerating dev set...\\n\")\n",
        "      dev_data_input_file = processor.get_dev_file(data_folder_current)\n",
        "      if USING_SHARDS:\n",
        "        shards = get_or_create_shards(dev_data_input_file,SHARD_SIZE,START_SHARD,END_SHARD)\n",
        "      else:\n",
        "        shards = [dev_data_input_file,None]\n",
        "      for shard,shard_ind in shards:\n",
        "        if USING_SHARDS: print(f\"generating data for shard number {shard_ind}\")\n",
        "        dev_examples = processor._create_examples(processor._read_tsv(shard),\"dev\")\n",
        "        if len(dev_examples) == 0:\n",
        "          raise Exception(\"no data present in the dev dataset\")\n",
        "        dev_file = os.path.join(DATA_GCS_DIR, \"dev.tf_record\")\n",
        "        if USING_SHARDS:\n",
        "          dev_file+=\"_\"+str(shard_ind)\n",
        "        script.file_based_convert_examples_to_features(dev_examples, \n",
        "                                                      label_list, \n",
        "                                                      MAX_SEQ_LENGTH, \n",
        "                                                      tokenizer, \n",
        "                                                      dev_file)\n",
        "    except Exception as e:\n",
        "      print(\"dev dataset generation failed. Error:\",e)\n",
        "\n",
        "  if DO_TEST:\n",
        "    try:\n",
        "      print(\"\\nGenerating test set...\\n\")\n",
        "      datasets = [re.match(\"test_(\\w+).tsv\",file).groups()[0] for file in tf.io.gfile.listdir(data_folder_current) if re.match(\"test_(\\w+).tsv\",file)]\n",
        "      if not datasets:\n",
        "        datasets = [None]\n",
        "      for dataset in datasets:\n",
        "        if dataset: print(f\"Processing dataset: {dataset}\")\n",
        "        test_data_input_file = processor.get_test_file(data_folder_current,dataset=dataset)\n",
        "        if USING_SHARDS:\n",
        "          shards = get_or_create_shards(test_data_input_file,SHARD_SIZE,START_SHARD,END_SHARD)\n",
        "        else:\n",
        "          shards = [test_data_input_file,None]\n",
        "        for n,(shard,shard_ind) in enumerate(shards):\n",
        "          if USING_SHARDS: print(f\"generating data for shard number {shard_ind}\")\n",
        "          test_examples = processor._create_examples(processor._read_tsv(shard),\"test\")\n",
        "          if len(test_examples) == 0:\n",
        "            raise Exception(\"no data present in the test dataset\")\n",
        "          test_file = os.path.join(DATA_GCS_DIR, f\"test_{dataset}.tf_record\" if dataset else \"test.tf_record\")\n",
        "          if USING_SHARDS:\n",
        "            test_file+=\"_\"+str(shard_ind)\n",
        "          ## if using precise testing, the data will be split into two sets: \n",
        "          ## one set will be able to be predicted on the maximum possible batch \n",
        "          ## size, while the other will be predicted on a batch size of 1, to \n",
        "          ## ensure the fastest prediction without leaving out any datapoints\n",
        "          if PRECISE_TESTING and n==len(shards)-1:\n",
        "            test_file_trailing = os.path.join(DATA_GCS_DIR, f\"test_trailing_{dataset}.tf_record\" if dataset else \"test_trailing.tf_record\")\n",
        "            def largest_mutiple_under_max(max,multiple_base):\n",
        "              return int(max/multiple_base)*multiple_base\n",
        "\n",
        "            split = largest_mutiple_under_max(len(test_examples),MAX_BATCH_SIZE)\n",
        "            test_examples_head = test_examples[:split]\n",
        "            test_examples_trailing = test_examples[split:]\n",
        "            script.file_based_convert_examples_to_features(test_examples_head, \n",
        "                                                           label_list, \n",
        "                                                           MAX_SEQ_LENGTH, \n",
        "                                                           tokenizer, \n",
        "                                                           test_file)\n",
        "            if test_examples_trailing:\n",
        "              script.file_based_convert_examples_to_features(test_examples_trailing, \n",
        "                                                            label_list, \n",
        "                                                            MAX_SEQ_LENGTH, \n",
        "                                                            tokenizer, \n",
        "                                                            test_file_trailing)\n",
        "          else:\n",
        "            script.file_based_convert_examples_to_features(test_examples, \n",
        "                                                           label_list, \n",
        "                                                           MAX_SEQ_LENGTH, \n",
        "                                                           tokenizer, \n",
        "                                                           test_file)\n",
        "    except Exception as e:\n",
        "      print(\"test dataset generation failed. Error:\",e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED2rhitTCdjm"
      },
      "source": [
        "###Data Generation ops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhUajg5kClz2"
      },
      "source": [
        "There are currently two data generations loops/ops (more can be added using a similar format to these two examples):\n",
        "1. Varying sequence lengths: multiple sets of different sequence lengths will be generated\n",
        "  * Store multiple individual datasets as subfolders inside of Input finetuning data folder, with each folder named its corresponding sequence length.\n",
        "2. Only one dataset: a single dataset with a specified set of parameters will be generated \n",
        "  * Directly store only the files train.tsv, dev.tsv, and test.tsv for one dataset inside Input finetuning data folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TWmbWT5SJqg"
      },
      "source": [
        "####Varying sequence lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "outputs": [],
      "source": [
        "#@markdown List of maximum sequence lengths to generate data for\n",
        "MAX_SEQ_LENGTHS = [1024] #@param\n",
        "#@markdown Whether or not to ensure all datapoints are used during prediction by using an extra trailing test dataset so no datapoints will be skipped due to the batch size. (This option should be used unless an extra trailing test dataset is a large problem)\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to split the data processing into shards (only for really large databases, since finetuning data typically isn't that large)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown If USING_SHARDS, what shard size to use (how many lines/datapoints should be in each shard) (MUST BE DIVISIBLE BY \"MAX_BATCH_SIZE\") (if using data augmentation, size indicates the size of augmented data)\n",
        "SHARD_SIZE = 1024000 #@param {type:\"integer\"}\n",
        "#@markdown If USING_SHARDS, which shard to start at (default start at shard 0)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Which sets to generate out of train, dev, and test\n",
        "TRAIN = False #@param {type:\"boolean\"}\n",
        "DEV = False #@param {type:\"boolean\"}\n",
        "TEST = True #@param {type:\"boolean\"}\n",
        "#@markdown How many additional augmented copies to load (augmented copies refer to duplicates of the same sequence but with different clip locations. This parameter is defined by the \"run_classifier.py\" file in the \"mutformer_model_code\" folder):\n",
        "AUGMENT_COPIES_TRAIN =  0#@param{type:\"integer\"}\n",
        "\n",
        "for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS:\n",
        "  print(\"\\n\\nGenerating data for seq length:\",MAX_SEQ_LENGTH,\"\\n\\n\")\n",
        "  DATA_GCS_DIR = BUCKET_PATH+\"/\"+OUTPUT_DATA_DIR +\"/\"+ str(MAX_SEQ_LENGTH)\n",
        "  data_folder_current= INPUT_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "  generate_data(MAX_SEQ_LENGTH,\n",
        "                data_folder_current,\n",
        "                DATA_GCS_DIR,\n",
        "                PRECISE_TESTING,\n",
        "                USING_SHARDS,\n",
        "                START_SHARD,\n",
        "                AUGMENT_COPIES_TRAIN,\n",
        "                SHARD_SIZE,\n",
        "                [TRAIN,DEV,TEST])\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEOfXa4WiB2N"
      },
      "source": [
        "###Only one dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IytLW0VbgOZz"
      },
      "outputs": [],
      "source": [
        "#@markdown Maximum output data length (when using paired method, actual protein sequence length is about half of this value):\n",
        "MAX_SEQ_LENGTH = 1024 #@param {type:\"integer\"}\n",
        "#@markdown Whether or not to ensure all datapoints are used during prediction by using an extra trailing test dataset so no datapoints will be skipped due to the batch size. (This option should be used most of the time unless an extra trailing test dataset is a large problem)\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to split the data processing into shards (only for really large databases, since finetuning data typically isn't that large)\n",
        "USING_SHARDS = True #@param {type:\"boolean\"}\n",
        "#@markdown If USING_SHARDS, what shard size to use (how many lines/datapoints should be in each shard) (MUST BE DIVISIBLE BY \"MAX_BATCH_SIZE\")\n",
        "SHARD_SIZE = 1024000 #@param {type:\"integer\"}\n",
        "#@markdown * If USING_SHARDS, set this value to indicate which shard to start processing at (defualt 0 for first shard)\n",
        "START_SHARD =  53#@param {type:\"integer\"}\n",
        "#@markdown * If USING_SHARDS, set this value to indicate which shard to process until (not inclusive) (defualt -1 for last shard)\n",
        "END_SHARD =  54#@param {type:\"integer\"}\n",
        "#@markdown Which sets to generate out of train, dev, and test\n",
        "TRAIN = False #@param {type:\"boolean\"}\n",
        "DEV = False #@param {type:\"boolean\"}\n",
        "TEST = True #@param {type:\"boolean\"}\n",
        "#@markdown How many additional augmented copies to load (augmented copies refer to duplicates of the same sequence but with different clip locations. This parameter is defined by the \"run_classifier.py\" file in the \"mutformer_model_code\" folder):\n",
        "AUGMENT_COPIES_TRAIN =  0#@param{type:\"integer\"}\n",
        "\n",
        "DATA_GCS_DIR = BUCKET_PATH+\"/\"+OUTPUT_DATA_DIR\n",
        "data_folder_current = INPUT_DATA_DIR\n",
        "\n",
        "generate_data(MAX_SEQ_LENGTH,\n",
        "              data_folder_current,\n",
        "              DATA_GCS_DIR,\n",
        "              PRECISE_TESTING,\n",
        "              USING_SHARDS,\n",
        "              START_SHARD,\n",
        "              AUGMENT_COPIES_TRAIN,\n",
        "              SHARD_SIZE,\n",
        "              [TRAIN,DEV,TEST])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_TWmbWT5SJqg"
      ],
      "provenance": []
    },
    "kernelspec": {
      "name": "py37",
      "display_name": "Python 3.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
