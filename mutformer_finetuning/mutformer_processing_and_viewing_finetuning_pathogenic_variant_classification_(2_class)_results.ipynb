{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mutformer_processing_and_viewing_finetuning_pathogenic_variant_classification_(2_class)_results.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I-x6AH2RVjm"
      },
      "source": [
        "##Viewing evaluation finetuning curves"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For viewing finetuning evaluation results by graphing metrics versus time using matplotlib."
      ],
      "metadata": {
        "id": "6K6kM5uFFqft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###General config/authenticate GCS and mount drive if needed"
      ],
      "metadata": {
        "id": "mGlzrz1xIg7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "import shutil\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "#@markdown For GCS pathes, what the name of the bucket is:\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown Folder source where evaluation result files have been stored (should point to the EVALUATIONS_DIR variable in the evaluation/prediction script) (can be either a GCS path or a drive path, depending on where the evaluation results were written):\n",
        "SOURCE_PATH = \"gs://theodore_jiang/MutFormer_updated_finetuning_eval_results\" #@param {type:\"string\"}\n",
        "DEST_PATH = SOURCE_PATH.replace(BUCKET_PATH+\"/\",\"\")\n",
        "\n",
        "if \"gs://\" in SOURCE_PATH:\n",
        "  from google.colab import auth\n",
        "  print(\"Authenticate for GCS:\")\n",
        "  auth.authenticate_user()\n",
        "elif \"/content/drive\" in SOURCE_PATH: \n",
        "  from google.colab import drive\n",
        "  print(\"Mount drive:\")\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH=\"/content/drive/My Drive\"\n"
      ],
      "metadata": {
        "id": "IoIM-r_7IliG",
        "outputId": "5c0ed8d2-d499-416d-f681-f7275f69704f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Authenticate for GCS:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download and combine tfevent files into dictionaries"
      ],
      "metadata": {
        "id": "1sW48W1tXx70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloads and converts data into dictionary format to be used for graphing in the code segment below the following code segment. To avoid tfevent file clutter (loading tfevent files is also both expensive and slow), this file will delete the original tfevent files and create a json dictionary to take their place."
      ],
      "metadata": {
        "id": "eM0Dcvm-Y3AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tabulate_event(fpath):\n",
        "  stuff = {}\n",
        "  \n",
        "  ea = EventAccumulator(fpath).Reload()\n",
        "  tags = ea.Tags()['scalars']\n",
        "\n",
        "  for tag in tags:\n",
        "    for event in ea.Scalars(tag):\n",
        "      try:\n",
        "          stuff[tag].append((event.step,event.value))\n",
        "      except:\n",
        "          stuff[tag] = [(event.step,event.value)]\n",
        "  return stuff\n",
        "\n",
        "if os.path.exists(DEST_PATH): ##before downloading, clear the destination\n",
        "  shutil.rmtree(DEST_PATH)\n",
        "os.makedirs(DEST_PATH)\n",
        "if \"gs://\" in SOURCE_PATH:                ##download tfevent files into local system for processing\n",
        "  cmd = \"gsutil -m rsync -r \"+SOURCE_PATH+\" \"+DEST_PATH\n",
        "  !{cmd}\n",
        "  cmd = \"gsutil -m rm -r \"+SOURCE_PATH\n",
        "  !{cmd}\n",
        "else:\n",
        "  shutil.copytree(SOURCE_PATH,DEST_PATH)\n",
        "  shutil.rmtree(SOURCE_PATH)\n",
        "  os.makedirs(SOURCE_PATH)\n",
        "\n",
        "graph_datas = {}\n",
        "for run in os.listdir(DEST_PATH):          ##assumes each folder comtains multiple subfolders, with each folder denoting \n",
        "  runp = DEST_PATH+\"/\"+run                 ##a single run. Generates a different set of data to be graphed for each run.\n",
        "  try:\n",
        "    run_data = json.load(open(DEST_PATH+\"/\"+run+\"/compiled_data.json\"))\n",
        "  except:\n",
        "    run_data = {}\n",
        "  for path,dirs,files in os.walk(runp):\n",
        "    for file in files:\n",
        "      subrun = path.replace(runp+\"/\",\"\")\n",
        "      if not subrun in run_data.keys():\n",
        "        run_data[subrun] = {}\n",
        "      filep = path+\"/\"+file\n",
        "      if \"tfevents\" not in filep:\n",
        "        continue\n",
        "      metrics = tabulate_event(filep)\n",
        "      for k,v in metrics.items():\n",
        "        for event_data in v:\n",
        "          try:\n",
        "            if not event_data in run_data[subrun][k]:\n",
        "              run_data[subrun][k].append(event_data)\n",
        "          except:\n",
        "            run_data[subrun][k] = [event_data]\n",
        "  graph_datas[run] = run_data\n",
        "  json.dump(run_data,tf.gfile.Open(SOURCE_PATH+\"/\"+run+\"/compiled_data.json\",\"w+\")) ##upload a json to take the place of many tfevent files"
      ],
      "metadata": {
        "id": "meXWqrG9PUIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fmmecLvTp7S"
      },
      "source": [
        "###Plotting smoothed average curves using matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw4p4eBN3YTW"
      },
      "source": [
        "#@markdown Range of the local average for viewing training graphs (amount of steps to average into one datatpoint) (to disable local averaging, set it to 0):\n",
        "avg_range = 100 #@param {type:\"integer\"}\n",
        "#@markdown Whether or not to save graphs into files:\n",
        "save_figs = True #@param {type:\"boolean\"}\n",
        "#@markdown * If saving figs, destination path for saving them (can be either a drive path or local path):\n",
        "outfolder = \"evaluation_graphs\" #@param {type:\"string\"}\n",
        "\n",
        "if \"/content/drive\" in outfolder: \n",
        "  from google.colab import drive\n",
        "  print(\"Mount drive:\")\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH=\"/content/drive/My Drive\"\n",
        "\n",
        "for run,run_data in graph_datas.items():\n",
        "    print(\"\\n\\nGraphs for run:\",run,\"\\n\\n\")\n",
        "    graphs = {}\n",
        "    for subrun,subrun_data in run_data.items():\n",
        "      for metric,data in subrun_data.items():\n",
        "        try:\n",
        "          graphs[metric][subrun] = data\n",
        "        except:\n",
        "          graphs[metric] = {subrun:data}\n",
        "    for metric,metric_datas in graphs.items():\n",
        "      plt.figure(figsize=(10,5))\n",
        "      plt.title(metric+\" graph\")\n",
        "      plt.xlabel(\"steps\")\n",
        "      plt.ylabel(metric)\n",
        "      for subrun,data in metric_datas.items():\n",
        "        steps = []\n",
        "        values = []\n",
        "        nan = 0\n",
        "        for datapt in data:\n",
        "          step = int(float(datapt[0]))\n",
        "          value = float(datapt[1])\n",
        "          if not math.isnan(value):\n",
        "              values.append(value)\n",
        "              steps.append(step)\n",
        "          else:\n",
        "              nan+=1\n",
        "        print(\"found and deleted\",nan,\"nan values in subrun:\",subrun)\n",
        "        steps_values_sorted = sorted(zip(steps, values), key=lambda pair: pair[0])\n",
        "        steps = [x for x,_ in steps_values_sorted]\n",
        "        values = [x for _,x in steps_values_sorted]\n",
        "\n",
        "        avged_values = []\n",
        "        for step,value in steps_values_sorted:\n",
        "          min_value_in_range = step-avg_range\n",
        "          max_value_in_range = step+avg_range\n",
        "          values_within_avg_range = [value for step,value in steps_values_sorted if min_value_in_range<=step<=max_value_in_range]\n",
        "          avged_values.append(sum(values_within_avg_range)/len(values_within_avg_range))\n",
        "        plt.plot(steps,avged_values,label=subrun)\n",
        "                \n",
        "      plt.legend()\n",
        "      if save_figs:\n",
        "        figout_folder = outfolder+\"/\"+run\n",
        "        if not os.path.exists(figout_folder):\n",
        "          os.makedirs(figout_folder)\n",
        "        plt.savefig(figout_folder+\"/\"+metric.replace(\"/\",\"_\")+\".png\")\n",
        "      plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbXhWZ7tTm76"
      },
      "source": [
        "###Tensorboard viewing (If you wish to use tensorboard instead)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvVNtrI1cH-H"
      },
      "source": [
        "LOGS_DIR = \"/content/drive/My Drive\" #@param (type:\"string\")\n",
        "LOGS_DIR = \"\\\"\"+LOGS_DIR+\"\\\"\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir $LOGS_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op5ZqhKaR5ir"
      },
      "source": [
        "##Predictions Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5KuZA6dR857"
      },
      "source": [
        "###General config/authenticate GCS and mount drive if needed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "import shutil\n",
        "import tensorflow.compat.v1 as tf\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "#@markdown For GCS pathes, what the name of the bucket is:\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown Folder source where predictions have been stored (should point to the PREDICTIONS_DIR variable in the evaluation/prediction script) (can be either a GCS path or a drive path, depending on where the predictions were written):\n",
        "SOURCE_PATH = \"gs://theodore_jiang/MutFormer_updated_finetuning_predictions\" #@param {type:\"string\"}\n",
        "DEST_PATH = SOURCE_PATH.replace(BUCKET_PATH+\"/\",\"\")\n",
        "\n",
        "if \"gs://\" in SOURCE_PATH:\n",
        "  from google.colab import auth\n",
        "  print(\"Authenticate for GCS:\")\n",
        "  auth.authenticate_user()\n",
        "elif \"/content/drive\" in SOURCE_PATH: \n",
        "  from google.colab import drive\n",
        "  print(\"Mount drive:\")\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH=\"/content/drive/My Drive\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsjY5uzqm8uk",
        "outputId": "8c2d6b44-55cb-4c85-d13f-175e38e40335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticate for GCS:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Transfer predictions(optional)"
      ],
      "metadata": {
        "id": "kofpP15fnxvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If desired, prediction results can be copied to a new path, and processed from there (useful for downloading files into drive from GCS)."
      ],
      "metadata": {
        "id": "PjGDjYMdn2Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Where to write the predictions into (can be a drive path, in which case drive will be mounted if not mounted already)\n",
        "DESTINATION_PATH = \"/content/drive/My Drive/MutFormer_updated_finetuning_predictions\" #@param{type:\"string\"}\n",
        "\n",
        "if \"/content/drive\" in DEST_PATH: \n",
        "  from google.colab import drive\n",
        "  print(\"Mount drive:\")\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH=\"/content/drive/My Drive\"\n",
        "\n",
        "def tabulate_event(fpath):\n",
        "  stuff = {}\n",
        "  \n",
        "  ea = EventAccumulator(fpath).Reload()\n",
        "  tags = ea.Tags()['scalars']\n",
        "\n",
        "  for tag in tags:\n",
        "    for event in ea.Scalars(tag):\n",
        "      try:\n",
        "          stuff[tag].append((event.step,event.value))\n",
        "      except:\n",
        "          stuff[tag] = [(event.step,event.value)]\n",
        "  return stuff\n",
        "\n",
        "if \"gs://\" in SOURCE_PATH:                ##download tfevent files into local system for processing\n",
        "  cmd = \"gsutil -m rsync -r \"+SOURCE_PATH+\" \"+DESTINATION_PATH\n",
        "  !{cmd}\n",
        "elif \"/content/drive\" in SOURCE_PATH: \n",
        "  shutil.copytree(SOURCE_PATH,DESTINATION_PATH)\n",
        "\n",
        "SOURCE_PATH = DESTINATION_PATH"
      ],
      "metadata": {
        "id": "SMhTyizHn-0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Download files into local system"
      ],
      "metadata": {
        "id": "DgcdwJsyLe-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(DEST_PATH): ##before downloading, clear the local destination\n",
        "  shutil.rmtree(DEST_PATH)\n",
        "os.makedirs(DEST_PATH)\n",
        "if \"gs://\" in SOURCE_PATH:                ##download tfevent files into local system for processing\n",
        "  cmd = \"gsutil -m rsync -r \"+SOURCE_PATH+\" \"+DEST_PATH\n",
        "  !{cmd}\n",
        "else: \n",
        "  shutil.copytree(SOURCE_PATH,DEST_PATH)"
      ],
      "metadata": {
        "id": "pU8ZMwiMLh7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Convert tfevents into txts (If used EVALUATE_WHILE_PREDICT)"
      ],
      "metadata": {
        "id": "vS4CZqFotPnI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If during prediction, the EVALUATE_WHILE_PREDICT option was used, predictions will be written in the form of tfevent files. This script will convert these tfevent files into txts (There is no need to run this code segment if EVALUATE_WHILE_PREDICT was not used during prediction)."
      ],
      "metadata": {
        "id": "PaxobJKrtW_i"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eDrQ8MRpkP2"
      },
      "source": [
        "def tabulate_event(fpath):\n",
        "  stuff = {}\n",
        "  \n",
        "  ea = EventAccumulator(fpath).Reload()\n",
        "  tags = ea.Tags()['scalars']\n",
        "\n",
        "  for tag in tags:\n",
        "    for n,event in enumerate(ea.Scalars(tag)):\n",
        "      try:\n",
        "          stuff[tag].append((event.step,n,event.value))\n",
        "      except:\n",
        "          stuff[tag] = [(event.step,n,event.value)]\n",
        "  return stuff\n",
        "\n",
        "\n",
        "for run in os.listdir(DEST_PATH):          ##assumes each folder comtains multiple subfolders, with each folder denoting \n",
        "  runp = DEST_PATH+\"/\"+run                 ##a single run. Generates a different set of data to be graphed for each run.\n",
        "  run_data = {}\n",
        "  for path,dirs,files in os.walk(runp):\n",
        "    for file in files:\n",
        "      subrun = path.replace(runp+\"/\",\"\")\n",
        "      if not subrun in run_data.keys():\n",
        "        run_data[subrun] = {}\n",
        "      filep = path+\"/\"+file\n",
        "      if \"tfevents\" not in filep:\n",
        "        continue\n",
        "      preds_data = tabulate_event(filep)\n",
        "      for tag,v in preds_data.items():\n",
        "        tag=re.sub(\"\\_\\d+$\",\"\",tag)\n",
        "        for event_data in v:\n",
        "          try:\n",
        "            if not event_data in run_data[subrun][tag]:\n",
        "              run_data[subrun][tag].append(event_data)\n",
        "          except:\n",
        "            run_data[subrun][tag] = [event_data]\n",
        "  for subrun,subrun_data in run_data.items():\n",
        "    predp = DEST_PATH+\"/\"+run+\"/\"+subrun+\"/predictions.txt\"\n",
        "    lines = []\n",
        "    for tag,data in subrun_data.items():\n",
        "      sorted_data = [x for x in sorted(sorted(data,key=lambda x:x[0]),key=lambda x:x[1])]\n",
        "      for d,dp in enumerate(sorted_data):\n",
        "        try:\n",
        "          lines[d]+=\"\\t\"+tag+\":\"+str(dp[2])\n",
        "        except:\n",
        "          lines.append(tag+\":\"+str(dp[2]))\n",
        "    if len(lines)>0:\n",
        "      open(predp,\"w+\").write(\"\\n\".join(lines))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLZ-4p3-3lwU"
      },
      "source": [
        "###Plot ROC Curves using txts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OLsc3mH3pKN"
      },
      "source": [
        "#@markdown Whether or not to save ROC curves into files:\n",
        "save_figs = True #@param {type:\"boolean\"}\n",
        "#@markdown * If saving figs, destination path for saving them (can be either a drive path or local path):\n",
        "outfolder = \"ROC_graphs\" #@param {type:\"string\"}\n",
        "\n",
        "def str2list(string):\n",
        "    string = string.strip(\"[]\").replace(\",\",\" \")\n",
        "    return string.split()\n",
        "\n",
        "if \"/content/drive\" in outfolder: \n",
        "  from google.colab import drive\n",
        "  print(\"Mount drive:\")\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH=\"/content/drive/My Drive\"\n",
        "\n",
        "AUCs = {}\n",
        "\n",
        "for run in os.listdir(DEST_PATH):          ##assumes each folder comtains multiple subfolders, with each folder denoting \n",
        "  runp = DEST_PATH+\"/\"+run                 ##a single run. Generates a different set of data to be graphed for each run.\n",
        "  plt.figure(figsize=(20,10))\n",
        "  for path,dirs,files in os.walk(runp):\n",
        "    for file in files:\n",
        "      subrun = path.replace(runp+\"/\",\"\")\n",
        "      filep = path+\"/\"+file\n",
        "      if \"predictions\" not in file:\n",
        "        continue\n",
        "      labels = []\n",
        "\n",
        "      pred_probs = []\n",
        "      tp=0\n",
        "      tn=0\n",
        "      fp=0\n",
        "      fn=0\n",
        "      print(\"Stats for (run/subrun):\",run+\"/\"+subrun,\"\\n\")\n",
        "\n",
        "      for n,line in enumerate(open(filep).read().split(\"\\n\")[:-1]):\n",
        "        line_dict = {}\n",
        "        try:\n",
        "          for item in line.split(\"\\t\"):\n",
        "              line_dict[item.split(\":\")[0]] = item.split(\":\")[1]\n",
        "          label = float(line_dict[\"labels\"])\n",
        "          pred = float(str2list(line_dict[\"probabilities\"])[1])\n",
        "          if label == 1 and pred>0.5:\n",
        "            tp+=1\n",
        "          elif label == 0 and pred<0.5:\n",
        "            tn+=1\n",
        "          elif label == 0 and pred>0.5:\n",
        "            fp+=1\n",
        "          elif label == 1 and pred<0.5:\n",
        "            fn+=1\n",
        "          else:\n",
        "            continue ##probably invalid input, so slip\n",
        "          pred_probs.append(pred)\n",
        "          labels.append(label)\n",
        "        except Exception as e:\n",
        "          print(\"failed at line\",n, \"error:\",e)\n",
        "          print(\"full failed line:\",line,\"\\n\")\n",
        "\n",
        "\n",
        "      print(\"tp:\",tp,\n",
        "            \"tn:\",tn,\n",
        "            \"fp:\",fp,\n",
        "            \"fn:\",fn)\n",
        "      \n",
        "      try:\n",
        "        acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "        recall = tp/(tp+fn)\n",
        "        precision = tp/(tp+fp)\n",
        "        f1 = 2*precision*recall/(precision+recall)\n",
        "        print(\"acc:\",acc)\n",
        "        print(\"recall_total:\",recall)\n",
        "        print(\"precision_total:\",precision)\n",
        "        print(\"f1_total:\",f1,\"\\n\")\n",
        "      except:\n",
        "        pass\n",
        "      \n",
        "      ##calculate roc auc\n",
        "      pred_auc = roc_auc_score(labels, pred_probs)\n",
        "\n",
        "\n",
        "      labels = labels[:min(len(labels),len(pred_probs))]        ##trims both lists to the same length\n",
        "      pred_probs = pred_probs[:min(len(labels),len(pred_probs))]\n",
        "      print(\"Graphing ROC curve for\",len(labels),\"predictions\")\n",
        "      pred_fpr, pred_tpr, _ = roc_curve(labels, pred_probs)\n",
        "      plt.plot(pred_fpr, pred_tpr, linestyle=\"-\", label=subrun+': Area under curve: '+str(round(pred_auc,3)))\n",
        "      plt.xlabel('False Positive Rate')\n",
        "      plt.ylabel('True Positive Rate')\n",
        "      AUCs[run+\"/\"+subrun] = round(pred_auc,3)\n",
        "\n",
        "  plt.legend()\n",
        "  plt.title(\"ROC for run: \"+run)\n",
        "  if save_figs:\n",
        "    if not os.path.exists(outfolder):\n",
        "      os.makedirs(outfolder)\n",
        "    plt.savefig(outfolder+\"/\"+run+\"_ROC.png\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "AUCs = {k:v for k,v in sorted([(k,v) for k,v in AUCs.items()],key=lambda x:x[1])}\n",
        "print(\"Printing all AUCs...\")\n",
        "for k,v in AUCs.items():\n",
        "  print(\"run/subrun:\",k,\"\\tAUC:\",v)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
