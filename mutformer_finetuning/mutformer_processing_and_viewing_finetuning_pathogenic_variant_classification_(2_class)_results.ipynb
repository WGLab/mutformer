{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I-x6AH2RVjm"
      },
      "source": [
        "##Viewing evaluation finetuning curves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K6kM5uFFqft"
      },
      "source": [
        "For viewing finetuning evaluation results by graphing metrics versus time using matplotlib."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N4SVj1X12d4"
      },
      "source": [
        "###Downgrade Python and Tensorflow \n",
        "\n",
        "(the default python version in Colab does not support Tensorflow 1.15)\n",
        "\n",
        "* **Note** that because the Python used in this notebook is not the default path, syntax highlighting most likely will not function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eom4pUnKRguT"
      },
      "source": [
        "####1. First, download and install Python version 3.7:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WZZFi9cRhdp",
        "outputId": "ce1f02c8-fc33-44d1-bd9a-44231436605d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-04 20:11:28--  https://repo.anaconda.com/miniconda/Miniconda3-py37_22.11.1-1-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86308321 (82M) [application/x-sh]\n",
            "Saving to: ‘mini.sh’\n",
            "\n",
            "mini.sh             100%[===================>]  82.31M   130MB/s    in 0.6s    \n",
            "\n",
            "2023-04-04 20:11:29 (130 MB/s) - ‘mini.sh’ saved [86308321/86308321]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "                                                                                    \n",
            "Installing base environment...\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: ...working... failed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - jupyter\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    anyio-3.5.0                |   py37h06a4308_0         165 KB\n",
            "    argon2-cffi-21.3.0         |     pyhd3eb1b0_0          15 KB\n",
            "    argon2-cffi-bindings-21.2.0|   py37h7f8727e_0          33 KB\n",
            "    attrs-22.1.0               |   py37h06a4308_0          84 KB\n",
            "    babel-2.11.0               |   py37h06a4308_0         6.8 MB\n",
            "    backcall-0.2.0             |     pyhd3eb1b0_0          13 KB\n",
            "    beautifulsoup4-4.11.1      |   py37h06a4308_0         185 KB\n",
            "    bleach-4.1.0               |     pyhd3eb1b0_0         123 KB\n",
            "    ca-certificates-2023.01.10 |       h06a4308_0         120 KB\n",
            "    conda-23.1.0               |   py37h06a4308_0         937 KB\n",
            "    dbus-1.13.18               |       hb2f20db_0         504 KB\n",
            "    debugpy-1.5.1              |   py37h295c915_0         1.7 MB\n",
            "    decorator-5.1.1            |     pyhd3eb1b0_0          12 KB\n",
            "    defusedxml-0.7.1           |     pyhd3eb1b0_0          23 KB\n",
            "    entrypoints-0.4            |   py37h06a4308_0          16 KB\n",
            "    expat-2.4.9                |       h6a678d5_0         156 KB\n",
            "    fontconfig-2.14.1          |       h52c9d5c_1         281 KB\n",
            "    freetype-2.12.1            |       h4a9f257_0         626 KB\n",
            "    glib-2.69.1                |       he621ea3_2         1.9 MB\n",
            "    gst-plugins-base-1.14.1    |       h6a678d5_1         2.2 MB\n",
            "    gstreamer-1.14.1           |       h5eee18b_1         1.7 MB\n",
            "    icu-58.2                   |       he6710b0_3        10.5 MB\n",
            "    importlib_resources-5.2.0  |     pyhd3eb1b0_1          21 KB\n",
            "    ipykernel-6.15.2           |   py37h06a4308_0         189 KB\n",
            "    ipython-7.31.1             |   py37h06a4308_1        1002 KB\n",
            "    ipython_genutils-0.2.0     |     pyhd3eb1b0_1          27 KB\n",
            "    ipywidgets-7.6.5           |     pyhd3eb1b0_1         105 KB\n",
            "    jedi-0.18.1                |   py37h06a4308_1         980 KB\n",
            "    jinja2-3.1.2               |   py37h06a4308_0         209 KB\n",
            "    jpeg-9e                    |       h5eee18b_1         262 KB\n",
            "    json5-0.9.6                |     pyhd3eb1b0_0          21 KB\n",
            "    jsonschema-4.17.3          |   py37h06a4308_0         138 KB\n",
            "    jupyter-1.0.0              |   py37h06a4308_8           7 KB\n",
            "    jupyter_client-7.4.9       |   py37h06a4308_0         204 KB\n",
            "    jupyter_console-6.4.4      |   py37h06a4308_0          42 KB\n",
            "    jupyter_core-4.11.2        |   py37h06a4308_0          80 KB\n",
            "    jupyter_server-1.23.4      |   py37h06a4308_0         382 KB\n",
            "    jupyterlab-3.5.3           |   py37h06a4308_0         4.4 MB\n",
            "    jupyterlab_pygments-0.1.2  |             py_0           8 KB\n",
            "    jupyterlab_server-2.19.0   |   py37h06a4308_0          80 KB\n",
            "    jupyterlab_widgets-1.0.0   |     pyhd3eb1b0_1         109 KB\n",
            "    libpng-1.6.39              |       h5eee18b_0         304 KB\n",
            "    libsodium-1.0.18           |       h7b6447c_0         244 KB\n",
            "    libuuid-1.41.5             |       h5eee18b_0          27 KB\n",
            "    libxcb-1.15                |       h7f8727e_0         505 KB\n",
            "    libxml2-2.9.14             |       h74e7548_0         718 KB\n",
            "    markupsafe-2.1.1           |   py37h7f8727e_0          21 KB\n",
            "    matplotlib-inline-0.1.6    |   py37h06a4308_0          16 KB\n",
            "    mistune-0.8.4              |py37h14c3975_1001          54 KB\n",
            "    nbclassic-0.5.2            |   py37h06a4308_0         6.1 MB\n",
            "    nbclient-0.5.13            |   py37h06a4308_0          91 KB\n",
            "    nbconvert-6.4.4            |   py37h06a4308_0         493 KB\n",
            "    nbformat-5.7.0             |   py37h06a4308_0         133 KB\n",
            "    nest-asyncio-1.5.6         |   py37h06a4308_0          14 KB\n",
            "    notebook-6.5.2             |   py37h06a4308_0         508 KB\n",
            "    notebook-shim-0.2.2        |   py37h06a4308_0          22 KB\n",
            "    openssl-1.1.1t             |       h7f8727e_0         3.7 MB\n",
            "    packaging-22.0             |   py37h06a4308_0          68 KB\n",
            "    pandocfilters-1.5.0        |     pyhd3eb1b0_0          11 KB\n",
            "    parso-0.8.3                |     pyhd3eb1b0_0          70 KB\n",
            "    pcre-8.45                  |       h295c915_0         207 KB\n",
            "    pexpect-4.8.0              |     pyhd3eb1b0_3          53 KB\n",
            "    pickleshare-0.7.5          |  pyhd3eb1b0_1003          13 KB\n",
            "    pkgutil-resolve-name-1.3.10|   py37h06a4308_0           9 KB\n",
            "    prometheus_client-0.14.1   |   py37h06a4308_0          90 KB\n",
            "    prompt-toolkit-3.0.36      |   py37h06a4308_0         571 KB\n",
            "    prompt_toolkit-3.0.36      |       hd3eb1b0_0           5 KB\n",
            "    psutil-5.9.0               |   py37h5eee18b_0         328 KB\n",
            "    ptyprocess-0.7.0           |     pyhd3eb1b0_2          17 KB\n",
            "    pygments-2.11.2            |     pyhd3eb1b0_0         759 KB\n",
            "    pyqt-5.9.2                 |   py37h05f1152_2         4.5 MB\n",
            "    pyrsistent-0.18.0          |   py37heee7806_0          95 KB\n",
            "    python-dateutil-2.8.2      |     pyhd3eb1b0_0         233 KB\n",
            "    python-fastjsonschema-2.16.2|   py37h06a4308_0         230 KB\n",
            "    pytz-2022.7                |   py37h06a4308_0         207 KB\n",
            "    pyzmq-23.2.0               |   py37h6a678d5_0         438 KB\n",
            "    qt-5.9.7                   |       h5867ecd_1        68.5 MB\n",
            "    qtconsole-5.4.0            |   py37h06a4308_0         189 KB\n",
            "    qtpy-2.2.0                 |   py37h06a4308_0          84 KB\n",
            "    send2trash-1.8.0           |     pyhd3eb1b0_1          19 KB\n",
            "    sip-4.19.8                 |   py37hf484d3e_0         274 KB\n",
            "    sniffio-1.2.0              |   py37h06a4308_1          15 KB\n",
            "    soupsieve-2.3.2.post1      |   py37h06a4308_0          65 KB\n",
            "    terminado-0.17.1           |   py37h06a4308_0          31 KB\n",
            "    testpath-0.6.0             |   py37h06a4308_0          85 KB\n",
            "    tomli-2.0.1                |   py37h06a4308_0          24 KB\n",
            "    tornado-6.2                |   py37h5eee18b_0         584 KB\n",
            "    traitlets-5.7.1            |   py37h06a4308_0         199 KB\n",
            "    typing-extensions-4.4.0    |   py37h06a4308_0           8 KB\n",
            "    wcwidth-0.2.5              |     pyhd3eb1b0_0          26 KB\n",
            "    webencodings-0.5.1         |           py37_1          19 KB\n",
            "    websocket-client-0.58.0    |   py37h06a4308_4          66 KB\n",
            "    widgetsnbextension-3.5.2   |   py37h06a4308_0         645 KB\n",
            "    zeromq-4.3.4               |       h2531618_0         331 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       127.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  anyio              pkgs/main/linux-64::anyio-3.5.0-py37h06a4308_0 \n",
            "  argon2-cffi        pkgs/main/noarch::argon2-cffi-21.3.0-pyhd3eb1b0_0 \n",
            "  argon2-cffi-bindi~ pkgs/main/linux-64::argon2-cffi-bindings-21.2.0-py37h7f8727e_0 \n",
            "  attrs              pkgs/main/linux-64::attrs-22.1.0-py37h06a4308_0 \n",
            "  babel              pkgs/main/linux-64::babel-2.11.0-py37h06a4308_0 \n",
            "  backcall           pkgs/main/noarch::backcall-0.2.0-pyhd3eb1b0_0 \n",
            "  beautifulsoup4     pkgs/main/linux-64::beautifulsoup4-4.11.1-py37h06a4308_0 \n",
            "  bleach             pkgs/main/noarch::bleach-4.1.0-pyhd3eb1b0_0 \n",
            "  dbus               pkgs/main/linux-64::dbus-1.13.18-hb2f20db_0 \n",
            "  debugpy            pkgs/main/linux-64::debugpy-1.5.1-py37h295c915_0 \n",
            "  decorator          pkgs/main/noarch::decorator-5.1.1-pyhd3eb1b0_0 \n",
            "  defusedxml         pkgs/main/noarch::defusedxml-0.7.1-pyhd3eb1b0_0 \n",
            "  entrypoints        pkgs/main/linux-64::entrypoints-0.4-py37h06a4308_0 \n",
            "  expat              pkgs/main/linux-64::expat-2.4.9-h6a678d5_0 \n",
            "  fontconfig         pkgs/main/linux-64::fontconfig-2.14.1-h52c9d5c_1 \n",
            "  freetype           pkgs/main/linux-64::freetype-2.12.1-h4a9f257_0 \n",
            "  glib               pkgs/main/linux-64::glib-2.69.1-he621ea3_2 \n",
            "  gst-plugins-base   pkgs/main/linux-64::gst-plugins-base-1.14.1-h6a678d5_1 \n",
            "  gstreamer          pkgs/main/linux-64::gstreamer-1.14.1-h5eee18b_1 \n",
            "  icu                pkgs/main/linux-64::icu-58.2-he6710b0_3 \n",
            "  importlib_resourc~ pkgs/main/noarch::importlib_resources-5.2.0-pyhd3eb1b0_1 \n",
            "  ipykernel          pkgs/main/linux-64::ipykernel-6.15.2-py37h06a4308_0 \n",
            "  ipython            pkgs/main/linux-64::ipython-7.31.1-py37h06a4308_1 \n",
            "  ipython_genutils   pkgs/main/noarch::ipython_genutils-0.2.0-pyhd3eb1b0_1 \n",
            "  ipywidgets         pkgs/main/noarch::ipywidgets-7.6.5-pyhd3eb1b0_1 \n",
            "  jedi               pkgs/main/linux-64::jedi-0.18.1-py37h06a4308_1 \n",
            "  jinja2             pkgs/main/linux-64::jinja2-3.1.2-py37h06a4308_0 \n",
            "  jpeg               pkgs/main/linux-64::jpeg-9e-h5eee18b_1 \n",
            "  json5              pkgs/main/noarch::json5-0.9.6-pyhd3eb1b0_0 \n",
            "  jsonschema         pkgs/main/linux-64::jsonschema-4.17.3-py37h06a4308_0 \n",
            "  jupyter            pkgs/main/linux-64::jupyter-1.0.0-py37h06a4308_8 \n",
            "  jupyter_client     pkgs/main/linux-64::jupyter_client-7.4.9-py37h06a4308_0 \n",
            "  jupyter_console    pkgs/main/linux-64::jupyter_console-6.4.4-py37h06a4308_0 \n",
            "  jupyter_core       pkgs/main/linux-64::jupyter_core-4.11.2-py37h06a4308_0 \n",
            "  jupyter_server     pkgs/main/linux-64::jupyter_server-1.23.4-py37h06a4308_0 \n",
            "  jupyterlab         pkgs/main/linux-64::jupyterlab-3.5.3-py37h06a4308_0 \n",
            "  jupyterlab_pygmen~ pkgs/main/noarch::jupyterlab_pygments-0.1.2-py_0 \n",
            "  jupyterlab_server  pkgs/main/linux-64::jupyterlab_server-2.19.0-py37h06a4308_0 \n",
            "  jupyterlab_widgets pkgs/main/noarch::jupyterlab_widgets-1.0.0-pyhd3eb1b0_1 \n",
            "  libpng             pkgs/main/linux-64::libpng-1.6.39-h5eee18b_0 \n",
            "  libsodium          pkgs/main/linux-64::libsodium-1.0.18-h7b6447c_0 \n",
            "  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0 \n",
            "  libxcb             pkgs/main/linux-64::libxcb-1.15-h7f8727e_0 \n",
            "  libxml2            pkgs/main/linux-64::libxml2-2.9.14-h74e7548_0 \n",
            "  markupsafe         pkgs/main/linux-64::markupsafe-2.1.1-py37h7f8727e_0 \n",
            "  matplotlib-inline  pkgs/main/linux-64::matplotlib-inline-0.1.6-py37h06a4308_0 \n",
            "  mistune            pkgs/main/linux-64::mistune-0.8.4-py37h14c3975_1001 \n",
            "  nbclassic          pkgs/main/linux-64::nbclassic-0.5.2-py37h06a4308_0 \n",
            "  nbclient           pkgs/main/linux-64::nbclient-0.5.13-py37h06a4308_0 \n",
            "  nbconvert          pkgs/main/linux-64::nbconvert-6.4.4-py37h06a4308_0 \n",
            "  nbformat           pkgs/main/linux-64::nbformat-5.7.0-py37h06a4308_0 \n",
            "  nest-asyncio       pkgs/main/linux-64::nest-asyncio-1.5.6-py37h06a4308_0 \n",
            "  notebook           pkgs/main/linux-64::notebook-6.5.2-py37h06a4308_0 \n",
            "  notebook-shim      pkgs/main/linux-64::notebook-shim-0.2.2-py37h06a4308_0 \n",
            "  packaging          pkgs/main/linux-64::packaging-22.0-py37h06a4308_0 \n",
            "  pandocfilters      pkgs/main/noarch::pandocfilters-1.5.0-pyhd3eb1b0_0 \n",
            "  parso              pkgs/main/noarch::parso-0.8.3-pyhd3eb1b0_0 \n",
            "  pcre               pkgs/main/linux-64::pcre-8.45-h295c915_0 \n",
            "  pexpect            pkgs/main/noarch::pexpect-4.8.0-pyhd3eb1b0_3 \n",
            "  pickleshare        pkgs/main/noarch::pickleshare-0.7.5-pyhd3eb1b0_1003 \n",
            "  pkgutil-resolve-n~ pkgs/main/linux-64::pkgutil-resolve-name-1.3.10-py37h06a4308_0 \n",
            "  prometheus_client  pkgs/main/linux-64::prometheus_client-0.14.1-py37h06a4308_0 \n",
            "  prompt-toolkit     pkgs/main/linux-64::prompt-toolkit-3.0.36-py37h06a4308_0 \n",
            "  prompt_toolkit     pkgs/main/noarch::prompt_toolkit-3.0.36-hd3eb1b0_0 \n",
            "  psutil             pkgs/main/linux-64::psutil-5.9.0-py37h5eee18b_0 \n",
            "  ptyprocess         pkgs/main/noarch::ptyprocess-0.7.0-pyhd3eb1b0_2 \n",
            "  pygments           pkgs/main/noarch::pygments-2.11.2-pyhd3eb1b0_0 \n",
            "  pyqt               pkgs/main/linux-64::pyqt-5.9.2-py37h05f1152_2 \n",
            "  pyrsistent         pkgs/main/linux-64::pyrsistent-0.18.0-py37heee7806_0 \n",
            "  python-dateutil    pkgs/main/noarch::python-dateutil-2.8.2-pyhd3eb1b0_0 \n",
            "  python-fastjsonsc~ pkgs/main/linux-64::python-fastjsonschema-2.16.2-py37h06a4308_0 \n",
            "  pytz               pkgs/main/linux-64::pytz-2022.7-py37h06a4308_0 \n",
            "  pyzmq              pkgs/main/linux-64::pyzmq-23.2.0-py37h6a678d5_0 \n",
            "  qt                 pkgs/main/linux-64::qt-5.9.7-h5867ecd_1 \n",
            "  qtconsole          pkgs/main/linux-64::qtconsole-5.4.0-py37h06a4308_0 \n",
            "  qtpy               pkgs/main/linux-64::qtpy-2.2.0-py37h06a4308_0 \n",
            "  send2trash         pkgs/main/noarch::send2trash-1.8.0-pyhd3eb1b0_1 \n",
            "  sip                pkgs/main/linux-64::sip-4.19.8-py37hf484d3e_0 \n",
            "  sniffio            pkgs/main/linux-64::sniffio-1.2.0-py37h06a4308_1 \n",
            "  soupsieve          pkgs/main/linux-64::soupsieve-2.3.2.post1-py37h06a4308_0 \n",
            "  terminado          pkgs/main/linux-64::terminado-0.17.1-py37h06a4308_0 \n",
            "  testpath           pkgs/main/linux-64::testpath-0.6.0-py37h06a4308_0 \n",
            "  tomli              pkgs/main/linux-64::tomli-2.0.1-py37h06a4308_0 \n",
            "  tornado            pkgs/main/linux-64::tornado-6.2-py37h5eee18b_0 \n",
            "  traitlets          pkgs/main/linux-64::traitlets-5.7.1-py37h06a4308_0 \n",
            "  typing-extensions  pkgs/main/linux-64::typing-extensions-4.4.0-py37h06a4308_0 \n",
            "  wcwidth            pkgs/main/noarch::wcwidth-0.2.5-pyhd3eb1b0_0 \n",
            "  webencodings       pkgs/main/linux-64::webencodings-0.5.1-py37_1 \n",
            "  websocket-client   pkgs/main/linux-64::websocket-client-0.58.0-py37h06a4308_4 \n",
            "  widgetsnbextension pkgs/main/linux-64::widgetsnbextension-3.5.2-py37h06a4308_0 \n",
            "  zeromq             pkgs/main/linux-64::zeromq-4.3.4-h2531618_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                     2022.10.11-h06a4308_0 --> 2023.01.10-h06a4308_0 \n",
            "  conda                              22.11.1-py37h06a4308_4 --> 23.1.0-py37h06a4308_0 \n",
            "  openssl                                 1.1.1s-h7f8727e_0 --> 1.1.1t-h7f8727e_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - google-colab\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    aiohttp-3.8.1              |   py37h540881e_1         561 KB  conda-forge\n",
            "    aiosignal-1.3.1            |     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    async-timeout-4.0.2        |     pyhd8ed1ab_0           9 KB  conda-forge\n",
            "    asynctest-0.13.0           |             py_0          24 KB  conda-forge\n",
            "    ca-certificates-2022.12.7  |       ha878542_0         143 KB  conda-forge\n",
            "    cachetools-5.3.0           |     pyhd8ed1ab_0          14 KB  conda-forge\n",
            "    certifi-2022.12.7          |     pyhd8ed1ab_0         147 KB  conda-forge\n",
            "    frozenlist-1.3.3           |   py37h5eee18b_0          44 KB\n",
            "    google-auth-2.17.1         |     pyh1a96a4e_0          97 KB  conda-forge\n",
            "    google-colab-1.0.0         |     pyh44b312d_0          77 KB  conda-forge\n",
            "    libblas-3.9.0              |15_linux64_openblas          12 KB  conda-forge\n",
            "    libcblas-3.9.0             |15_linux64_openblas          12 KB  conda-forge\n",
            "    libgfortran-ng-12.2.0      |      h69a702a_19          22 KB  conda-forge\n",
            "    libgfortran5-12.2.0        |      h337968e_19         1.8 MB  conda-forge\n",
            "    liblapack-3.9.0            |15_linux64_openblas          12 KB  conda-forge\n",
            "    libopenblas-0.3.20         |pthreads_h78a6416_0        10.1 MB  conda-forge\n",
            "    multidict-6.0.2            |   py37h5eee18b_0          47 KB\n",
            "    numpy-1.21.6               |   py37h976b520_0         6.1 MB  conda-forge\n",
            "    pandas-1.2.3               |   py37hdc94413_0        11.8 MB  conda-forge\n",
            "    portpicker-1.5.2           |     pyhd8ed1ab_0          17 KB  conda-forge\n",
            "    pyasn1-0.4.8               |             py_0          53 KB  conda-forge\n",
            "    pyasn1-modules-0.2.7       |             py_0          60 KB  conda-forge\n",
            "    python_abi-3.7             |          2_cp37m           4 KB  conda-forge\n",
            "    pyu2f-0.1.5                |     pyhd8ed1ab_0          31 KB  conda-forge\n",
            "    rsa-4.9                    |     pyhd8ed1ab_0          29 KB  conda-forge\n",
            "    setuptools-59.8.0          |   py37h89c1867_1         1.0 MB  conda-forge\n",
            "    yarl-1.7.2                 |   py37h540881e_2         132 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        32.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  aiohttp            conda-forge/linux-64::aiohttp-3.8.1-py37h540881e_1 \n",
            "  aiosignal          conda-forge/noarch::aiosignal-1.3.1-pyhd8ed1ab_0 \n",
            "  async-timeout      conda-forge/noarch::async-timeout-4.0.2-pyhd8ed1ab_0 \n",
            "  asynctest          conda-forge/noarch::asynctest-0.13.0-py_0 \n",
            "  cachetools         conda-forge/noarch::cachetools-5.3.0-pyhd8ed1ab_0 \n",
            "  frozenlist         pkgs/main/linux-64::frozenlist-1.3.3-py37h5eee18b_0 \n",
            "  google-auth        conda-forge/noarch::google-auth-2.17.1-pyh1a96a4e_0 \n",
            "  google-colab       conda-forge/noarch::google-colab-1.0.0-pyh44b312d_0 \n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-15_linux64_openblas \n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-15_linux64_openblas \n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-12.2.0-h69a702a_19 \n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-12.2.0-h337968e_19 \n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-15_linux64_openblas \n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.20-pthreads_h78a6416_0 \n",
            "  multidict          pkgs/main/linux-64::multidict-6.0.2-py37h5eee18b_0 \n",
            "  numpy              conda-forge/linux-64::numpy-1.21.6-py37h976b520_0 \n",
            "  pandas             conda-forge/linux-64::pandas-1.2.3-py37hdc94413_0 \n",
            "  portpicker         conda-forge/noarch::portpicker-1.5.2-pyhd8ed1ab_0 \n",
            "  pyasn1             conda-forge/noarch::pyasn1-0.4.8-py_0 \n",
            "  pyasn1-modules     conda-forge/noarch::pyasn1-modules-0.2.7-py_0 \n",
            "  python_abi         conda-forge/linux-64::python_abi-3.7-2_cp37m \n",
            "  pyu2f              conda-forge/noarch::pyu2f-0.1.5-pyhd8ed1ab_0 \n",
            "  rsa                conda-forge/noarch::rsa-4.9-pyhd8ed1ab_0 \n",
            "  yarl               conda-forge/linux-64::yarl-1.7.2-py37h540881e_2 \n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2022.12.7-ha878542_0 \n",
            "  certifi            pkgs/main/linux-64::certifi-2022.12.7~ --> conda-forge/noarch::certifi-2022.12.7-pyhd8ed1ab_0 \n",
            "  setuptools         pkgs/main::setuptools-65.5.0-py37h06a~ --> conda-forge::setuptools-59.8.0-py37h89c1867_1 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Installed kernelspec py37 in /root/.local/share/jupyter/kernels/py37\n"
          ]
        }
      ],
      "source": [
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py37_22.11.1-1-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py37\" --user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSfSfnvRRii9"
      },
      "source": [
        "####2. Then, reload the webpage (not restart runtime) to allow Colab to recognize the newly installed python\n",
        "####3. Finally, run the following commands to install tensorflow 1.15:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgMYqiwJ12S0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "032494e4-1203-47bb-bd8a-a028d0028f8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.7/site-packages (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.16.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.53.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (2.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (3.20.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.21.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.37.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (59.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.2.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.3)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.4.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/site-packages (3.5.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib) (9.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/site-packages (from matplotlib) (4.38.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/site-packages (from matplotlib) (22.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (4.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: protobuf==3.20.1 in /usr/local/lib/python3.7/site-packages (3.20.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/site-packages (from scikit-learn) (1.21.6)\n",
            "Collecting joblib>=0.11\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.1.0\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "Successfully installed joblib-1.2.0 scikit-learn-1.0.2 scipy-1.7.3 threadpoolctl-3.1.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install tensorflow==1.15\n",
        "!python3 -m pip install matplotlib\n",
        "!python3 -m pip install protobuf==3.20.1\n",
        "!python3 -m pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGlzrz1xIg7Q"
      },
      "source": [
        "###General config/authenticate GCS and mount drive if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoIM-r_7IliG",
        "outputId": "30b54da1-c756-4741-d770-2aa26c1ef571",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authorize for GCS:\n",
            "Authorize done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "#@markdown Name of the GCS bucket to use (Make sure to set this to the name of your own GCS  bucket):\n",
        "BUCKET_NAME = \"\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown Folder source where evaluation result files have been stored (should be a full path pointing to the EVALUATIONS_DIR variable in the evaluation/prediction script; can also be left blank: if not using evaluation files, leave this item blank) (can be either a GCS path or a drive path, depending on where the evaluation results were written):\n",
        "SOURCE_PATH = \"\" #@param {type:\"string\"}\n",
        "#@markdown * If using a GCS path, whether to use link authorization for GCS (link authorization allows connection to another account other than the one running the script, while normal authorization disables connecting to different accounts):\n",
        "GCS_LINK_AUTHORIZATION = False #@param {type:\"boolean\"}\n",
        "\n",
        "DEST_PATH = SOURCE_PATH.replace(BUCKET_PATH+\"/\",\"\")\n",
        "\n",
        "if \"gs://\" in SOURCE_PATH or not SOURCE_PATH:\n",
        "  from google.colab import auth\n",
        "  print(\"Authorize for GCS:\")\n",
        "  if not GCS_LINK_AUTHORIZATION: \n",
        "    auth.authenticate_user()\n",
        "  else: \n",
        "    !gcloud auth login --no-launch-browser\n",
        "  print(\"Authorize done\")\n",
        "elif \"/content/drive\" in SOURCE_PATH: \n",
        "  from google.colab import drive\n",
        "  print(\"Mount drive:\")\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH=\"/content/drive/My Drive\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sW48W1tXx70"
      },
      "source": [
        "###Download and combine tfevent files into dictionaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM0Dcvm-Y3AI"
      },
      "source": [
        "Downloads and converts data into dictionary format to be used for graphing later. This code segment is used to avoid tfevent file clutter (loading tfevent files is also both expensive and slow); this file will delete the original tfevent files and create a json dictionary to take their place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meXWqrG9PUIJ"
      },
      "outputs": [],
      "source": [
        "def tabulate_event(fpath):\n",
        "  stuff = {}\n",
        "  \n",
        "  ea = EventAccumulator(fpath).Reload()\n",
        "  tags = ea.Tags()['scalars']\n",
        "\n",
        "  for tag in tags:\n",
        "    for event in ea.Scalars(tag):\n",
        "      try:\n",
        "          stuff[tag].append((event.step,event.value))\n",
        "      except:\n",
        "          stuff[tag] = [(event.step,event.value)]\n",
        "  return stuff\n",
        "\n",
        "if os.path.exists(DEST_PATH): ##before downloading, clear the destination\n",
        "  shutil.rmtree(DEST_PATH)\n",
        "os.makedirs(DEST_PATH)\n",
        "if \"gs://\" in SOURCE_PATH:                ##download tfevent files into local system for processing\n",
        "  cmd = \"gsutil -m rsync -r \"+SOURCE_PATH+\" \"+DEST_PATH\n",
        "  !{cmd}\n",
        "  cmd = \"gsutil -m rm -r \"+SOURCE_PATH\n",
        "  !{cmd}\n",
        "else:\n",
        "  shutil.copytree(SOURCE_PATH,DEST_PATH)\n",
        "  shutil.rmtree(SOURCE_PATH)\n",
        "  os.makedirs(SOURCE_PATH)\n",
        "\n",
        "graph_datas = {}\n",
        "for run in os.listdir(DEST_PATH):          ##assumes each folder comtains multiple subfolders, with each folder denoting \n",
        "  runp = DEST_PATH+\"/\"+run                 ##a single run. Generates a different set of data to be graphed for each run.\n",
        "  try:\n",
        "    run_data = json.load(open(DEST_PATH+\"/\"+run+\"/compiled_data.json\"))\n",
        "  except:\n",
        "    run_data = {}\n",
        "  for path,dirs,files in os.walk(runp):\n",
        "    for file in files:\n",
        "      subrun = path.replace(runp+\"/\",\"\")\n",
        "      if not subrun in run_data.keys():\n",
        "        run_data[subrun] = {}\n",
        "      filep = path+\"/\"+file\n",
        "      if \"tfevents\" not in filep:\n",
        "        continue\n",
        "      metrics = tabulate_event(filep)\n",
        "      for k,v in metrics.items():\n",
        "        for event_data in v:\n",
        "          try:\n",
        "            if not event_data in run_data[subrun][k]:\n",
        "              run_data[subrun][k].append(event_data)\n",
        "          except:\n",
        "            run_data[subrun][k] = [event_data]\n",
        "  graph_datas[run] = run_data\n",
        "  json.dump(run_data,tf.gfile.Open(SOURCE_PATH+\"/\"+run+\"/compiled_data.json\",\"w+\")) ##upload a json to take the place of many tfevent files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fmmecLvTp7S"
      },
      "source": [
        "###Plotting smoothed average curves using matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sw4p4eBN3YTW"
      },
      "outputs": [],
      "source": [
        "#@markdown Range of the local average for viewing training graphs (amount of steps to average into one datatpoint) (to disable local averaging, set it to 0):\n",
        "avg_range = 100 #@param {type:\"integer\"}\n",
        "#@markdown Whether or not to save graphs into files:\n",
        "save_figs = True #@param {type:\"boolean\"}\n",
        "#@markdown * If saving figs, destination path for saving them (can be either a drive path or local path):\n",
        "outfolder = \"evaluation_graphs\" #@param {type:\"string\"}\n",
        "\n",
        "if \"/content/drive\" in outfolder: \n",
        "  from google.colab import drive\n",
        "  print(\"Mount drive:\")\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH=\"/content/drive/My Drive\"\n",
        "\n",
        "for run,run_data in graph_datas.items():\n",
        "    print(\"\\n\\nGraphs for run:\",run,\"\\n\\n\")\n",
        "    graphs = {}\n",
        "    for subrun,subrun_data in run_data.items():\n",
        "      for metric,data in subrun_data.items():\n",
        "        try:\n",
        "          graphs[metric][subrun] = data\n",
        "        except:\n",
        "          graphs[metric] = {subrun:data}\n",
        "    for metric,metric_datas in graphs.items():\n",
        "      plt.figure(figsize=(10,5))\n",
        "      plt.title(metric+\" graph\")\n",
        "      plt.xlabel(\"steps\")\n",
        "      plt.ylabel(metric)\n",
        "      for subrun,data in metric_datas.items():\n",
        "        steps = []\n",
        "        values = []\n",
        "        nan = 0\n",
        "        for datapt in data:\n",
        "          step = int(float(datapt[0]))\n",
        "          value = float(datapt[1])\n",
        "          if not math.isnan(value):\n",
        "              values.append(value)\n",
        "              steps.append(step)\n",
        "          else:\n",
        "              nan+=1\n",
        "        print(\"found and deleted\",nan,\"nan values in subrun:\",subrun)\n",
        "        steps_values_sorted = sorted(zip(steps, values), key=lambda pair: pair[0])\n",
        "        steps = [x for x,_ in steps_values_sorted]\n",
        "        values = [x for _,x in steps_values_sorted]\n",
        "\n",
        "        avged_values = []\n",
        "        for step,value in steps_values_sorted:\n",
        "          min_value_in_range = step-avg_range\n",
        "          max_value_in_range = step+avg_range\n",
        "          values_within_avg_range = [value for step,value in steps_values_sorted if min_value_in_range<=step<=max_value_in_range]\n",
        "          avged_values.append(sum(values_within_avg_range)/len(values_within_avg_range))\n",
        "        plt.plot(steps,avged_values,label=subrun)\n",
        "                \n",
        "      plt.legend()\n",
        "      if save_figs:\n",
        "        figout_folder = outfolder+\"/\"+run\n",
        "        if not os.path.exists(figout_folder):\n",
        "          os.makedirs(figout_folder)\n",
        "        plt.savefig(figout_folder+\"/\"+metric.replace(\"/\",\"_\")+\".png\")\n",
        "      plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbXhWZ7tTm76"
      },
      "source": [
        "###Tensorboard viewing (If you wish to use tensorboard instead)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvVNtrI1cH-H"
      },
      "outputs": [],
      "source": [
        "LOGS_DIR = \"/content/drive/My Drive\" #@param (type:\"string\")\n",
        "LOGS_DIR = \"\\\"\"+LOGS_DIR+\"\\\"\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir $LOGS_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op5ZqhKaR5ir"
      },
      "source": [
        "##Predictions Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5KuZA6dR857"
      },
      "source": [
        "###General config/authenticate GCS and mount drive if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsjY5uzqm8uk",
        "outputId": "85f6ebe3-0557-446f-d150-b08fe9d68feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authorize for GCS:\n",
            "Authorize done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "#@markdown For GCS pathes, what the name of the bucket is:\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown Folder source where predictions have been stored (should point to the PREDICTIONS_DIR variable in the evaluation/prediction script) (can be either a GCS path or a drive path, depending on where the predictions were written):\n",
        "SOURCE_PATH = \"\" #@param {type:\"string\"}\n",
        "#@markdown * If using a GCS path, whether to use link authorization for GCS (link authorization allows connection to another account other than the one running the script, while normal authorization disables connecting to different accounts):\n",
        "GCS_LINK_AUTHORIZATION = False #@param {type:\"boolean\"}\n",
        "\n",
        "DEST_PATH = SOURCE_PATH.replace(BUCKET_PATH+\"/\",\"\")\n",
        "\n",
        "if \"gs://\" in SOURCE_PATH:\n",
        "  from google.colab import auth\n",
        "  print(\"Authorize for GCS:\")\n",
        "  if not GCS_LINK_AUTHORIZATION: \n",
        "    auth.authenticate_user()\n",
        "  else: \n",
        "    !gcloud auth login --no-launch-browser\n",
        "  print(\"Authorize done\")\n",
        "elif \"/content/drive\" in SOURCE_PATH: \n",
        "  from google.colab import drive\n",
        "  print(\"Mount drive:\")\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH=\"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kofpP15fnxvU"
      },
      "source": [
        "###Transfer predictions(optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjGDjYMdn2Yp"
      },
      "source": [
        "If desired, prediction results can be copied to a new path, and processed from there (useful for downloading files into drive from GCS)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMhTyizHn-0f"
      },
      "outputs": [],
      "source": [
        "#@markdown Where to write the predictions into (can be a drive path, in which case drive will be mounted if not mounted already)\n",
        "DESTINATION_PATH = \"/content/drive/My Drive/MutFormer_updated_finetuning_predictions\" #@param{type:\"string\"}\n",
        "\n",
        "if \"/content/drive\" in DEST_PATH: \n",
        "  from google.colab import drive\n",
        "  print(\"Mount drive:\")\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH=\"/content/drive/My Drive\"\n",
        "\n",
        "def tabulate_event(fpath):\n",
        "  stuff = {}\n",
        "  \n",
        "  ea = EventAccumulator(fpath).Reload()\n",
        "  tags = ea.Tags()['scalars']\n",
        "\n",
        "  for tag in tags:\n",
        "    for event in ea.Scalars(tag):\n",
        "      try:\n",
        "          stuff[tag].append((event.step,event.value))\n",
        "      except:\n",
        "          stuff[tag] = [(event.step,event.value)]\n",
        "  return stuff\n",
        "\n",
        "if not os.path.exists(DESTINATION_PATH):\n",
        "  os.makedirs(DESTINATION_PATH)\n",
        "\n",
        "if \"gs://\" in SOURCE_PATH:                ##download tfevent files into local system for processing\n",
        "  cmd = \"gsutil -m rsync -r \\\"\"+SOURCE_PATH+\"\\\" \\\"\"+DESTINATION_PATH + \"\\\"\"\n",
        "  !{cmd}\n",
        "elif \"/content/drive\" in SOURCE_PATH: \n",
        "  shutil.copytree(SOURCE_PATH,DESTINATION_PATH)\n",
        "\n",
        "SOURCE_PATH = DESTINATION_PATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgcdwJsyLe-o"
      },
      "source": [
        "###Download files into local system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gWxOLmK3MYq"
      },
      "source": [
        "In order to work with the prediction files, they are first downloaded into the local runtime file system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU8ZMwiMLh7V"
      },
      "outputs": [],
      "source": [
        "if os.path.exists(DEST_PATH): ##before downloading, clear the local destination\n",
        "  shutil.rmtree(DEST_PATH)\n",
        "outer_folder = \"/\".join(DEST_PATH.split(\"/\")[:-1])\n",
        "if outer_folder and not os.path.exists(outer_folder):\n",
        "  os.makedirs(outer_folder)\n",
        "if \"gs://\" in SOURCE_PATH:                ##download tfevent files into local system for processing\n",
        "  cmd = \"gsutil -m rsync -r \"+SOURCE_PATH+\" \"+DEST_PATH\n",
        "  !{cmd}\n",
        "else: \n",
        "  shutil.copytree(SOURCE_PATH,DEST_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS4CZqFotPnI"
      },
      "source": [
        "###Convert tfevents into txts (If used EVALUATE_WHILE_PREDICT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PaxobJKrtW_i"
      },
      "source": [
        "If during prediction, the EVALUATE_WHILE_PREDICT option was used, predictions will be written in the form of tfevent files. This script will convert these tfevent files into txts (There is no need to run this code segment if EVALUATE_WHILE_PREDICT was not used during prediction)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eDrQ8MRpkP2"
      },
      "outputs": [],
      "source": [
        "def tabulate_event(fpath):\n",
        "  stuff = {}\n",
        "  \n",
        "  ea = EventAccumulator(fpath).Reload()\n",
        "  tags = ea.Tags()['scalars']\n",
        "\n",
        "  for tag in tags:\n",
        "    for n,event in enumerate(ea.Scalars(tag)):\n",
        "      try:\n",
        "          stuff[tag].append((event.step,n,event.value))\n",
        "      except:\n",
        "          stuff[tag] = [(event.step,n,event.value)]\n",
        "  return stuff\n",
        "\n",
        "\n",
        "for run in os.listdir(DEST_PATH):          ##assumes each folder comtains multiple subfolders, with each folder denoting \n",
        "  runp = DEST_PATH+\"/\"+run                 ##a single run. Generates a different set of data to be graphed for each run.\n",
        "  run_data = {}\n",
        "  for path,dirs,files in os.walk(runp):\n",
        "    for file in files:\n",
        "      subrun = path.replace(runp+\"/\",\"\")\n",
        "      if not subrun in run_data.keys():\n",
        "        run_data[subrun] = {}\n",
        "      filep = path+\"/\"+file\n",
        "      if \"tfevents\" not in filep:\n",
        "        continue\n",
        "      preds_data = tabulate_event(filep)\n",
        "      for tag,v in preds_data.items():\n",
        "        tag=re.sub(\"\\_\\d+$\",\"\",tag)\n",
        "        for event_data in v:\n",
        "          try:\n",
        "            if not event_data in run_data[subrun][tag]:\n",
        "              run_data[subrun][tag].append(event_data)\n",
        "          except:\n",
        "            run_data[subrun][tag] = [event_data]\n",
        "  for subrun,subrun_data in run_data.items():\n",
        "    predp = DEST_PATH+\"/\"+run+\"/\"+subrun+\"/predictions.txt\"\n",
        "    lines = []\n",
        "    for tag,data in subrun_data.items():\n",
        "      sorted_data = [x for x in sorted(sorted(data,key=lambda x:x[0]),key=lambda x:x[1])]\n",
        "      for d,dp in enumerate(sorted_data):\n",
        "        try:\n",
        "          lines[d]+=\"\\t\"+tag+\":\"+str(dp[2])\n",
        "        except:\n",
        "          lines.append(tag+\":\"+str(dp[2]))\n",
        "    if len(lines)>0:\n",
        "      open(predp,\"w+\").write(\"\\n\".join(lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLZ-4p3-3lwU"
      },
      "source": [
        "###Plot ROC Curves using txts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OLsc3mH3pKN"
      },
      "outputs": [],
      "source": [
        "#@markdown Whether or not to save ROC curves into files:\n",
        "save_figs = True #@param {type:\"boolean\"}\n",
        "#@markdown * If saving figs, destination path for saving them (can be either a drive path or local path):\n",
        "outfolder = \"outfigs\" #@param {type:\"string\"}\n",
        "\n",
        "def str2list(string):\n",
        "    string = string.strip(\"[]\").replace(\",\",\" \")\n",
        "    return string.split()\n",
        "\n",
        "if \"/content/drive\" in outfolder: \n",
        "  from google.colab import drive\n",
        "  print(\"Mount drive:\")\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  DRIVE_PATH=\"/content/drive/My Drive\"\n",
        "\n",
        "AUCs = {}\n",
        "\n",
        "for run in os.listdir(DEST_PATH):          ##assumes each folder comtains multiple subfolders, with each folder denoting \n",
        "  runp = DEST_PATH+\"/\"+run                 ##a single run. Generates a different set of data to be graphed for each run.\n",
        "  plt.figure(figsize=(20,10))\n",
        "  for path,dirs,files in os.walk(runp):\n",
        "    for file in files:\n",
        "      subrun = path.replace(runp+\"/\",\"\")\n",
        "      filep = path+\"/\"+file\n",
        "      if \"predictions\" not in file:\n",
        "        continue\n",
        "      labels = []\n",
        "\n",
        "      pred_probs = []\n",
        "      tp=0\n",
        "      tn=0\n",
        "      fp=0\n",
        "      fn=0\n",
        "      print(\"Stats for (run/subrun):\",run+\"/\"+subrun,\"\\n\")\n",
        "\n",
        "      for n,line in enumerate(open(filep).read().split(\"\\n\")[:-1]):\n",
        "        line_dict = {}\n",
        "        try:\n",
        "          for item in line.split(\"\\t\"):\n",
        "              line_dict[item.split(\":\")[0]] = item.split(\":\")[1]\n",
        "          label = float(line_dict[\"labels\"])\n",
        "          pred = float(str2list(line_dict[\"probabilities\"])[1])\n",
        "          if label == 1 and pred>0.5:\n",
        "            tp+=1\n",
        "          elif label == 0 and pred<0.5:\n",
        "            tn+=1\n",
        "          elif label == 0 and pred>0.5:\n",
        "            fp+=1\n",
        "          elif label == 1 and pred<0.5:\n",
        "            fn+=1\n",
        "          else:\n",
        "            continue ##probably invalid input, so slip\n",
        "          pred_probs.append(pred)\n",
        "          labels.append(label)\n",
        "        except Exception as e:\n",
        "          print(\"failed at line\",n, \"error:\",e)\n",
        "          print(\"full failed line:\",line,\"\\n\")\n",
        "\n",
        "\n",
        "      print(\"tp:\",tp,\n",
        "            \"tn:\",tn,\n",
        "            \"fp:\",fp,\n",
        "            \"fn:\",fn)\n",
        "      \n",
        "      try:\n",
        "        acc = (tp+tn)/(tp+tn+fp+fn)\n",
        "        recall = tp/(tp+fn)\n",
        "        precision = tp/(tp+fp)\n",
        "        f1 = 2*precision*recall/(precision+recall)\n",
        "        print(\"acc:\",acc)\n",
        "        print(\"recall_total:\",recall)\n",
        "        print(\"precision_total:\",precision)\n",
        "        print(\"f1_total:\",f1,\"\\n\")\n",
        "      except:\n",
        "        pass\n",
        "      \n",
        "      ##calculate roc auc\n",
        "      pred_auc = roc_auc_score(labels, pred_probs)\n",
        "\n",
        "\n",
        "      labels = labels[:min(len(labels),len(pred_probs))]        ##trims both lists to the same length\n",
        "      pred_probs = pred_probs[:min(len(labels),len(pred_probs))]\n",
        "      print(\"Graphing ROC curve for\",len(labels),\"predictions\")\n",
        "      pred_fpr, pred_tpr, _ = roc_curve(labels, pred_probs)\n",
        "      plt.plot(pred_fpr, pred_tpr, linestyle=\"-\", label=subrun+': Area under curve: '+str(round(pred_auc,3)))\n",
        "      plt.xlabel('False Positive Rate')\n",
        "      plt.ylabel('True Positive Rate')\n",
        "      AUCs[run+\"/\"+subrun] = round(pred_auc,3)\n",
        "\n",
        "  plt.legend()\n",
        "  plt.title(\"ROC for run: \"+run)\n",
        "  if save_figs:\n",
        "    if not os.path.exists(outfolder):\n",
        "      os.makedirs(outfolder)\n",
        "    plt.savefig(outfolder+\"/\"+run+\"_ROC.png\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "AUCs = {k:v for k,v in sorted([(k,v) for k,v in AUCs.items()],key=lambda x:x[1])}\n",
        "print(\"Printing all AUCs...\")\n",
        "for k,v in AUCs.items():\n",
        "  print(\"run/subrun:\",k,\"\\tAUC:\",v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5ToZevqh0Xi"
      },
      "source": [
        "##Check trainable variable values in model checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgxCdjmYh2tl"
      },
      "source": [
        "Used to check the values of the trainable variables in a checkpoint. Currently, reads in the latest checkpoint from the specified \"ckpt_folder,\" then saves the values of all the trainable variables into a text file on the local system titled \"ckpt_details.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Momry-huh_dA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python import pywrap_tensorflow\n",
        "import os\n",
        "\n",
        "#@markdown checkpoint folder path:\n",
        "ckpt_folder = \"bert_model_mrpc_ex_data_all_flbs5_6_2022/fl_0_bs_32\" #@param {type:\"string\"}\n",
        "\n",
        "ckpt_folderp = f\"gs://{BUCKET_NAME}/{ckpt_folder}\"\n",
        "latest_ckpt = \"gs://theodore_jiang/bert_model_mrpc_ex_data_all_flbs5_6_2022/fl_0_bs_32/model.ckpt-12000\"#tf.train.latest_checkpoint(ckpt_folderp)\n",
        "\n",
        "reader = pywrap_tensorflow.NewCheckpointReader(latest_ckpt)\n",
        "var_to_shape_map = reader.get_variable_to_shape_map()\n",
        "\n",
        "print(f\"\\n\\nReading from checkpoint: {latest_ckpt}\\n\\n\")\n",
        "\n",
        "with open(\"ckpt_details.txt\",\"w+\") as out:\n",
        "  for n,key in enumerate(var_to_shape_map):\n",
        "    print(f\"tensor number {n}: tensor_name: \", key)\n",
        "    out.write(f\"\\n{key}:\\n\")\n",
        "    if \"combine_dense\" in key:\n",
        "      try:\n",
        "        out.write(\"\\n\".join([str(row) for row in reader.get_tensor(key)]))\n",
        "      except Exception:\n",
        "        print(reader.get_tensor(key))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7",
      "name": "py37"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}