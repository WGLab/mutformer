{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-60rHUe5DT2r"
      },
      "source": [
        "#Finetuning Evaluation and Prediction Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhEhbQ08DXyG"
      },
      "source": [
        "This notebook evlauates and performs predictions on test data using finetuned models.\n",
        "\n",
        "Note: If using a TPU from Google Cloud (not the Colab TPU), make sure to run this notebook on a VM with access to all GCP APIs, and make sure TPUs are enabled for the GCP project"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downgrade Python and Tensorflow \n",
        "\n",
        "(the default python version in Colab does not support Tensorflow 1.15)\n",
        "\n",
        "* **Note** that because the Python used in this notebook is not the default path, syntax highlighting most likely will not function."
      ],
      "metadata": {
        "id": "iNnG-Bg31aXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. First, download and install Python version 3.7:"
      ],
      "metadata": {
        "id": "YA4HWP0ORAtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py37_22.11.1-1-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py37\" --user"
      ],
      "metadata": {
        "id": "J2DWe6M6RCex",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "554a0dab-0f24-45d5-b9f5-6f7a529e1ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-04 21:20:52--  https://repo.anaconda.com/miniconda/Miniconda3-py37_22.11.1-1-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86308321 (82M) [application/x-sh]\n",
            "Saving to: ‘mini.sh’\n",
            "\n",
            "mini.sh             100%[===================>]  82.31M   127MB/s    in 0.6s    \n",
            "\n",
            "2023-04-04 21:20:53 (127 MB/s) - ‘mini.sh’ saved [86308321/86308321]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "                                                                                     \n",
            "Installing base environment...\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: ...working... failed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - jupyter\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    anyio-3.5.0                |   py37h06a4308_0         165 KB\n",
            "    argon2-cffi-21.3.0         |     pyhd3eb1b0_0          15 KB\n",
            "    argon2-cffi-bindings-21.2.0|   py37h7f8727e_0          33 KB\n",
            "    attrs-22.1.0               |   py37h06a4308_0          84 KB\n",
            "    babel-2.11.0               |   py37h06a4308_0         6.8 MB\n",
            "    backcall-0.2.0             |     pyhd3eb1b0_0          13 KB\n",
            "    beautifulsoup4-4.11.1      |   py37h06a4308_0         185 KB\n",
            "    bleach-4.1.0               |     pyhd3eb1b0_0         123 KB\n",
            "    ca-certificates-2023.01.10 |       h06a4308_0         120 KB\n",
            "    conda-23.1.0               |   py37h06a4308_0         937 KB\n",
            "    dbus-1.13.18               |       hb2f20db_0         504 KB\n",
            "    debugpy-1.5.1              |   py37h295c915_0         1.7 MB\n",
            "    decorator-5.1.1            |     pyhd3eb1b0_0          12 KB\n",
            "    defusedxml-0.7.1           |     pyhd3eb1b0_0          23 KB\n",
            "    entrypoints-0.4            |   py37h06a4308_0          16 KB\n",
            "    expat-2.4.9                |       h6a678d5_0         156 KB\n",
            "    fontconfig-2.14.1          |       h52c9d5c_1         281 KB\n",
            "    freetype-2.12.1            |       h4a9f257_0         626 KB\n",
            "    glib-2.69.1                |       he621ea3_2         1.9 MB\n",
            "    gst-plugins-base-1.14.1    |       h6a678d5_1         2.2 MB\n",
            "    gstreamer-1.14.1           |       h5eee18b_1         1.7 MB\n",
            "    icu-58.2                   |       he6710b0_3        10.5 MB\n",
            "    importlib_resources-5.2.0  |     pyhd3eb1b0_1          21 KB\n",
            "    ipykernel-6.15.2           |   py37h06a4308_0         189 KB\n",
            "    ipython-7.31.1             |   py37h06a4308_1        1002 KB\n",
            "    ipython_genutils-0.2.0     |     pyhd3eb1b0_1          27 KB\n",
            "    ipywidgets-7.6.5           |     pyhd3eb1b0_1         105 KB\n",
            "    jedi-0.18.1                |   py37h06a4308_1         980 KB\n",
            "    jinja2-3.1.2               |   py37h06a4308_0         209 KB\n",
            "    jpeg-9e                    |       h5eee18b_1         262 KB\n",
            "    json5-0.9.6                |     pyhd3eb1b0_0          21 KB\n",
            "    jsonschema-4.17.3          |   py37h06a4308_0         138 KB\n",
            "    jupyter-1.0.0              |   py37h06a4308_8           7 KB\n",
            "    jupyter_client-7.4.9       |   py37h06a4308_0         204 KB\n",
            "    jupyter_console-6.4.4      |   py37h06a4308_0          42 KB\n",
            "    jupyter_core-4.11.2        |   py37h06a4308_0          80 KB\n",
            "    jupyter_server-1.23.4      |   py37h06a4308_0         382 KB\n",
            "    jupyterlab-3.5.3           |   py37h06a4308_0         4.4 MB\n",
            "    jupyterlab_pygments-0.1.2  |             py_0           8 KB\n",
            "    jupyterlab_server-2.19.0   |   py37h06a4308_0          80 KB\n",
            "    jupyterlab_widgets-1.0.0   |     pyhd3eb1b0_1         109 KB\n",
            "    libpng-1.6.39              |       h5eee18b_0         304 KB\n",
            "    libsodium-1.0.18           |       h7b6447c_0         244 KB\n",
            "    libuuid-1.41.5             |       h5eee18b_0          27 KB\n",
            "    libxcb-1.15                |       h7f8727e_0         505 KB\n",
            "    libxml2-2.9.14             |       h74e7548_0         718 KB\n",
            "    markupsafe-2.1.1           |   py37h7f8727e_0          21 KB\n",
            "    matplotlib-inline-0.1.6    |   py37h06a4308_0          16 KB\n",
            "    mistune-0.8.4              |py37h14c3975_1001          54 KB\n",
            "    nbclassic-0.5.2            |   py37h06a4308_0         6.1 MB\n",
            "    nbclient-0.5.13            |   py37h06a4308_0          91 KB\n",
            "    nbconvert-6.4.4            |   py37h06a4308_0         493 KB\n",
            "    nbformat-5.7.0             |   py37h06a4308_0         133 KB\n",
            "    nest-asyncio-1.5.6         |   py37h06a4308_0          14 KB\n",
            "    notebook-6.5.2             |   py37h06a4308_0         508 KB\n",
            "    notebook-shim-0.2.2        |   py37h06a4308_0          22 KB\n",
            "    openssl-1.1.1t             |       h7f8727e_0         3.7 MB\n",
            "    packaging-22.0             |   py37h06a4308_0          68 KB\n",
            "    pandocfilters-1.5.0        |     pyhd3eb1b0_0          11 KB\n",
            "    parso-0.8.3                |     pyhd3eb1b0_0          70 KB\n",
            "    pcre-8.45                  |       h295c915_0         207 KB\n",
            "    pexpect-4.8.0              |     pyhd3eb1b0_3          53 KB\n",
            "    pickleshare-0.7.5          |  pyhd3eb1b0_1003          13 KB\n",
            "    pkgutil-resolve-name-1.3.10|   py37h06a4308_0           9 KB\n",
            "    prometheus_client-0.14.1   |   py37h06a4308_0          90 KB\n",
            "    prompt-toolkit-3.0.36      |   py37h06a4308_0         571 KB\n",
            "    prompt_toolkit-3.0.36      |       hd3eb1b0_0           5 KB\n",
            "    psutil-5.9.0               |   py37h5eee18b_0         328 KB\n",
            "    ptyprocess-0.7.0           |     pyhd3eb1b0_2          17 KB\n",
            "    pygments-2.11.2            |     pyhd3eb1b0_0         759 KB\n",
            "    pyqt-5.9.2                 |   py37h05f1152_2         4.5 MB\n",
            "    pyrsistent-0.18.0          |   py37heee7806_0          95 KB\n",
            "    python-dateutil-2.8.2      |     pyhd3eb1b0_0         233 KB\n",
            "    python-fastjsonschema-2.16.2|   py37h06a4308_0         230 KB\n",
            "    pytz-2022.7                |   py37h06a4308_0         207 KB\n",
            "    pyzmq-23.2.0               |   py37h6a678d5_0         438 KB\n",
            "    qt-5.9.7                   |       h5867ecd_1        68.5 MB\n",
            "    qtconsole-5.4.0            |   py37h06a4308_0         189 KB\n",
            "    qtpy-2.2.0                 |   py37h06a4308_0          84 KB\n",
            "    send2trash-1.8.0           |     pyhd3eb1b0_1          19 KB\n",
            "    sip-4.19.8                 |   py37hf484d3e_0         274 KB\n",
            "    sniffio-1.2.0              |   py37h06a4308_1          15 KB\n",
            "    soupsieve-2.3.2.post1      |   py37h06a4308_0          65 KB\n",
            "    terminado-0.17.1           |   py37h06a4308_0          31 KB\n",
            "    testpath-0.6.0             |   py37h06a4308_0          85 KB\n",
            "    tomli-2.0.1                |   py37h06a4308_0          24 KB\n",
            "    tornado-6.2                |   py37h5eee18b_0         584 KB\n",
            "    traitlets-5.7.1            |   py37h06a4308_0         199 KB\n",
            "    typing-extensions-4.4.0    |   py37h06a4308_0           8 KB\n",
            "    wcwidth-0.2.5              |     pyhd3eb1b0_0          26 KB\n",
            "    webencodings-0.5.1         |           py37_1          19 KB\n",
            "    websocket-client-0.58.0    |   py37h06a4308_4          66 KB\n",
            "    widgetsnbextension-3.5.2   |   py37h06a4308_0         645 KB\n",
            "    zeromq-4.3.4               |       h2531618_0         331 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       127.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  anyio              pkgs/main/linux-64::anyio-3.5.0-py37h06a4308_0 \n",
            "  argon2-cffi        pkgs/main/noarch::argon2-cffi-21.3.0-pyhd3eb1b0_0 \n",
            "  argon2-cffi-bindi~ pkgs/main/linux-64::argon2-cffi-bindings-21.2.0-py37h7f8727e_0 \n",
            "  attrs              pkgs/main/linux-64::attrs-22.1.0-py37h06a4308_0 \n",
            "  babel              pkgs/main/linux-64::babel-2.11.0-py37h06a4308_0 \n",
            "  backcall           pkgs/main/noarch::backcall-0.2.0-pyhd3eb1b0_0 \n",
            "  beautifulsoup4     pkgs/main/linux-64::beautifulsoup4-4.11.1-py37h06a4308_0 \n",
            "  bleach             pkgs/main/noarch::bleach-4.1.0-pyhd3eb1b0_0 \n",
            "  dbus               pkgs/main/linux-64::dbus-1.13.18-hb2f20db_0 \n",
            "  debugpy            pkgs/main/linux-64::debugpy-1.5.1-py37h295c915_0 \n",
            "  decorator          pkgs/main/noarch::decorator-5.1.1-pyhd3eb1b0_0 \n",
            "  defusedxml         pkgs/main/noarch::defusedxml-0.7.1-pyhd3eb1b0_0 \n",
            "  entrypoints        pkgs/main/linux-64::entrypoints-0.4-py37h06a4308_0 \n",
            "  expat              pkgs/main/linux-64::expat-2.4.9-h6a678d5_0 \n",
            "  fontconfig         pkgs/main/linux-64::fontconfig-2.14.1-h52c9d5c_1 \n",
            "  freetype           pkgs/main/linux-64::freetype-2.12.1-h4a9f257_0 \n",
            "  glib               pkgs/main/linux-64::glib-2.69.1-he621ea3_2 \n",
            "  gst-plugins-base   pkgs/main/linux-64::gst-plugins-base-1.14.1-h6a678d5_1 \n",
            "  gstreamer          pkgs/main/linux-64::gstreamer-1.14.1-h5eee18b_1 \n",
            "  icu                pkgs/main/linux-64::icu-58.2-he6710b0_3 \n",
            "  importlib_resourc~ pkgs/main/noarch::importlib_resources-5.2.0-pyhd3eb1b0_1 \n",
            "  ipykernel          pkgs/main/linux-64::ipykernel-6.15.2-py37h06a4308_0 \n",
            "  ipython            pkgs/main/linux-64::ipython-7.31.1-py37h06a4308_1 \n",
            "  ipython_genutils   pkgs/main/noarch::ipython_genutils-0.2.0-pyhd3eb1b0_1 \n",
            "  ipywidgets         pkgs/main/noarch::ipywidgets-7.6.5-pyhd3eb1b0_1 \n",
            "  jedi               pkgs/main/linux-64::jedi-0.18.1-py37h06a4308_1 \n",
            "  jinja2             pkgs/main/linux-64::jinja2-3.1.2-py37h06a4308_0 \n",
            "  jpeg               pkgs/main/linux-64::jpeg-9e-h5eee18b_1 \n",
            "  json5              pkgs/main/noarch::json5-0.9.6-pyhd3eb1b0_0 \n",
            "  jsonschema         pkgs/main/linux-64::jsonschema-4.17.3-py37h06a4308_0 \n",
            "  jupyter            pkgs/main/linux-64::jupyter-1.0.0-py37h06a4308_8 \n",
            "  jupyter_client     pkgs/main/linux-64::jupyter_client-7.4.9-py37h06a4308_0 \n",
            "  jupyter_console    pkgs/main/linux-64::jupyter_console-6.4.4-py37h06a4308_0 \n",
            "  jupyter_core       pkgs/main/linux-64::jupyter_core-4.11.2-py37h06a4308_0 \n",
            "  jupyter_server     pkgs/main/linux-64::jupyter_server-1.23.4-py37h06a4308_0 \n",
            "  jupyterlab         pkgs/main/linux-64::jupyterlab-3.5.3-py37h06a4308_0 \n",
            "  jupyterlab_pygmen~ pkgs/main/noarch::jupyterlab_pygments-0.1.2-py_0 \n",
            "  jupyterlab_server  pkgs/main/linux-64::jupyterlab_server-2.19.0-py37h06a4308_0 \n",
            "  jupyterlab_widgets pkgs/main/noarch::jupyterlab_widgets-1.0.0-pyhd3eb1b0_1 \n",
            "  libpng             pkgs/main/linux-64::libpng-1.6.39-h5eee18b_0 \n",
            "  libsodium          pkgs/main/linux-64::libsodium-1.0.18-h7b6447c_0 \n",
            "  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0 \n",
            "  libxcb             pkgs/main/linux-64::libxcb-1.15-h7f8727e_0 \n",
            "  libxml2            pkgs/main/linux-64::libxml2-2.9.14-h74e7548_0 \n",
            "  markupsafe         pkgs/main/linux-64::markupsafe-2.1.1-py37h7f8727e_0 \n",
            "  matplotlib-inline  pkgs/main/linux-64::matplotlib-inline-0.1.6-py37h06a4308_0 \n",
            "  mistune            pkgs/main/linux-64::mistune-0.8.4-py37h14c3975_1001 \n",
            "  nbclassic          pkgs/main/linux-64::nbclassic-0.5.2-py37h06a4308_0 \n",
            "  nbclient           pkgs/main/linux-64::nbclient-0.5.13-py37h06a4308_0 \n",
            "  nbconvert          pkgs/main/linux-64::nbconvert-6.4.4-py37h06a4308_0 \n",
            "  nbformat           pkgs/main/linux-64::nbformat-5.7.0-py37h06a4308_0 \n",
            "  nest-asyncio       pkgs/main/linux-64::nest-asyncio-1.5.6-py37h06a4308_0 \n",
            "  notebook           pkgs/main/linux-64::notebook-6.5.2-py37h06a4308_0 \n",
            "  notebook-shim      pkgs/main/linux-64::notebook-shim-0.2.2-py37h06a4308_0 \n",
            "  packaging          pkgs/main/linux-64::packaging-22.0-py37h06a4308_0 \n",
            "  pandocfilters      pkgs/main/noarch::pandocfilters-1.5.0-pyhd3eb1b0_0 \n",
            "  parso              pkgs/main/noarch::parso-0.8.3-pyhd3eb1b0_0 \n",
            "  pcre               pkgs/main/linux-64::pcre-8.45-h295c915_0 \n",
            "  pexpect            pkgs/main/noarch::pexpect-4.8.0-pyhd3eb1b0_3 \n",
            "  pickleshare        pkgs/main/noarch::pickleshare-0.7.5-pyhd3eb1b0_1003 \n",
            "  pkgutil-resolve-n~ pkgs/main/linux-64::pkgutil-resolve-name-1.3.10-py37h06a4308_0 \n",
            "  prometheus_client  pkgs/main/linux-64::prometheus_client-0.14.1-py37h06a4308_0 \n",
            "  prompt-toolkit     pkgs/main/linux-64::prompt-toolkit-3.0.36-py37h06a4308_0 \n",
            "  prompt_toolkit     pkgs/main/noarch::prompt_toolkit-3.0.36-hd3eb1b0_0 \n",
            "  psutil             pkgs/main/linux-64::psutil-5.9.0-py37h5eee18b_0 \n",
            "  ptyprocess         pkgs/main/noarch::ptyprocess-0.7.0-pyhd3eb1b0_2 \n",
            "  pygments           pkgs/main/noarch::pygments-2.11.2-pyhd3eb1b0_0 \n",
            "  pyqt               pkgs/main/linux-64::pyqt-5.9.2-py37h05f1152_2 \n",
            "  pyrsistent         pkgs/main/linux-64::pyrsistent-0.18.0-py37heee7806_0 \n",
            "  python-dateutil    pkgs/main/noarch::python-dateutil-2.8.2-pyhd3eb1b0_0 \n",
            "  python-fastjsonsc~ pkgs/main/linux-64::python-fastjsonschema-2.16.2-py37h06a4308_0 \n",
            "  pytz               pkgs/main/linux-64::pytz-2022.7-py37h06a4308_0 \n",
            "  pyzmq              pkgs/main/linux-64::pyzmq-23.2.0-py37h6a678d5_0 \n",
            "  qt                 pkgs/main/linux-64::qt-5.9.7-h5867ecd_1 \n",
            "  qtconsole          pkgs/main/linux-64::qtconsole-5.4.0-py37h06a4308_0 \n",
            "  qtpy               pkgs/main/linux-64::qtpy-2.2.0-py37h06a4308_0 \n",
            "  send2trash         pkgs/main/noarch::send2trash-1.8.0-pyhd3eb1b0_1 \n",
            "  sip                pkgs/main/linux-64::sip-4.19.8-py37hf484d3e_0 \n",
            "  sniffio            pkgs/main/linux-64::sniffio-1.2.0-py37h06a4308_1 \n",
            "  soupsieve          pkgs/main/linux-64::soupsieve-2.3.2.post1-py37h06a4308_0 \n",
            "  terminado          pkgs/main/linux-64::terminado-0.17.1-py37h06a4308_0 \n",
            "  testpath           pkgs/main/linux-64::testpath-0.6.0-py37h06a4308_0 \n",
            "  tomli              pkgs/main/linux-64::tomli-2.0.1-py37h06a4308_0 \n",
            "  tornado            pkgs/main/linux-64::tornado-6.2-py37h5eee18b_0 \n",
            "  traitlets          pkgs/main/linux-64::traitlets-5.7.1-py37h06a4308_0 \n",
            "  typing-extensions  pkgs/main/linux-64::typing-extensions-4.4.0-py37h06a4308_0 \n",
            "  wcwidth            pkgs/main/noarch::wcwidth-0.2.5-pyhd3eb1b0_0 \n",
            "  webencodings       pkgs/main/linux-64::webencodings-0.5.1-py37_1 \n",
            "  websocket-client   pkgs/main/linux-64::websocket-client-0.58.0-py37h06a4308_4 \n",
            "  widgetsnbextension pkgs/main/linux-64::widgetsnbextension-3.5.2-py37h06a4308_0 \n",
            "  zeromq             pkgs/main/linux-64::zeromq-4.3.4-h2531618_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                     2022.10.11-h06a4308_0 --> 2023.01.10-h06a4308_0 \n",
            "  conda                              22.11.1-py37h06a4308_4 --> 23.1.0-py37h06a4308_0 \n",
            "  openssl                                 1.1.1s-h7f8727e_0 --> 1.1.1t-h7f8727e_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - google-colab\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    aiohttp-3.8.1              |   py37h540881e_1         561 KB  conda-forge\n",
            "    aiosignal-1.3.1            |     pyhd8ed1ab_0          12 KB  conda-forge\n",
            "    async-timeout-4.0.2        |     pyhd8ed1ab_0           9 KB  conda-forge\n",
            "    asynctest-0.13.0           |             py_0          24 KB  conda-forge\n",
            "    ca-certificates-2022.12.7  |       ha878542_0         143 KB  conda-forge\n",
            "    cachetools-5.3.0           |     pyhd8ed1ab_0          14 KB  conda-forge\n",
            "    certifi-2022.12.7          |     pyhd8ed1ab_0         147 KB  conda-forge\n",
            "    frozenlist-1.3.3           |   py37h5eee18b_0          44 KB\n",
            "    google-auth-2.17.1         |     pyh1a96a4e_0          97 KB  conda-forge\n",
            "    google-colab-1.0.0         |     pyh44b312d_0          77 KB  conda-forge\n",
            "    libblas-3.9.0              |15_linux64_openblas          12 KB  conda-forge\n",
            "    libcblas-3.9.0             |15_linux64_openblas          12 KB  conda-forge\n",
            "    libgfortran-ng-12.2.0      |      h69a702a_19          22 KB  conda-forge\n",
            "    libgfortran5-12.2.0        |      h337968e_19         1.8 MB  conda-forge\n",
            "    liblapack-3.9.0            |15_linux64_openblas          12 KB  conda-forge\n",
            "    libopenblas-0.3.20         |pthreads_h78a6416_0        10.1 MB  conda-forge\n",
            "    multidict-6.0.2            |   py37h5eee18b_0          47 KB\n",
            "    numpy-1.21.6               |   py37h976b520_0         6.1 MB  conda-forge\n",
            "    pandas-1.2.3               |   py37hdc94413_0        11.8 MB  conda-forge\n",
            "    portpicker-1.5.2           |     pyhd8ed1ab_0          17 KB  conda-forge\n",
            "    pyasn1-0.4.8               |             py_0          53 KB  conda-forge\n",
            "    pyasn1-modules-0.2.7       |             py_0          60 KB  conda-forge\n",
            "    python_abi-3.7             |          2_cp37m           4 KB  conda-forge\n",
            "    pyu2f-0.1.5                |     pyhd8ed1ab_0          31 KB  conda-forge\n",
            "    rsa-4.9                    |     pyhd8ed1ab_0          29 KB  conda-forge\n",
            "    setuptools-59.8.0          |   py37h89c1867_1         1.0 MB  conda-forge\n",
            "    yarl-1.7.2                 |   py37h540881e_2         132 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        32.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  aiohttp            conda-forge/linux-64::aiohttp-3.8.1-py37h540881e_1 \n",
            "  aiosignal          conda-forge/noarch::aiosignal-1.3.1-pyhd8ed1ab_0 \n",
            "  async-timeout      conda-forge/noarch::async-timeout-4.0.2-pyhd8ed1ab_0 \n",
            "  asynctest          conda-forge/noarch::asynctest-0.13.0-py_0 \n",
            "  cachetools         conda-forge/noarch::cachetools-5.3.0-pyhd8ed1ab_0 \n",
            "  frozenlist         pkgs/main/linux-64::frozenlist-1.3.3-py37h5eee18b_0 \n",
            "  google-auth        conda-forge/noarch::google-auth-2.17.1-pyh1a96a4e_0 \n",
            "  google-colab       conda-forge/noarch::google-colab-1.0.0-pyh44b312d_0 \n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-15_linux64_openblas \n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-15_linux64_openblas \n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-12.2.0-h69a702a_19 \n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-12.2.0-h337968e_19 \n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-15_linux64_openblas \n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.20-pthreads_h78a6416_0 \n",
            "  multidict          pkgs/main/linux-64::multidict-6.0.2-py37h5eee18b_0 \n",
            "  numpy              conda-forge/linux-64::numpy-1.21.6-py37h976b520_0 \n",
            "  pandas             conda-forge/linux-64::pandas-1.2.3-py37hdc94413_0 \n",
            "  portpicker         conda-forge/noarch::portpicker-1.5.2-pyhd8ed1ab_0 \n",
            "  pyasn1             conda-forge/noarch::pyasn1-0.4.8-py_0 \n",
            "  pyasn1-modules     conda-forge/noarch::pyasn1-modules-0.2.7-py_0 \n",
            "  python_abi         conda-forge/linux-64::python_abi-3.7-2_cp37m \n",
            "  pyu2f              conda-forge/noarch::pyu2f-0.1.5-pyhd8ed1ab_0 \n",
            "  rsa                conda-forge/noarch::rsa-4.9-pyhd8ed1ab_0 \n",
            "  yarl               conda-forge/linux-64::yarl-1.7.2-py37h540881e_2 \n",
            "\n",
            "The following packages will be SUPERSEDED by a higher-priority channel:\n",
            "\n",
            "  ca-certificates    pkgs/main::ca-certificates-2023.01.10~ --> conda-forge::ca-certificates-2022.12.7-ha878542_0 \n",
            "  certifi            pkgs/main/linux-64::certifi-2022.12.7~ --> conda-forge/noarch::certifi-2022.12.7-pyhd8ed1ab_0 \n",
            "  setuptools         pkgs/main::setuptools-65.5.0-py37h06a~ --> conda-forge::setuptools-59.8.0-py37h89c1867_1 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Installed kernelspec py37 in /root/.local/share/jupyter/kernels/py37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Then, reload the webpage (not restart runtime) to allow Colab to recognize the newly installed python\n",
        "####3. Finally, run the following commands to install tensorflow 1.15:"
      ],
      "metadata": {
        "id": "f9Et3qU_RDik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow==1.15\n",
        "!python3 -m pip install numpy==1.19.5\n",
        "!python3 -m pip install protobuf==3.20.1"
      ],
      "metadata": {
        "id": "uUJFq4K_1aGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c156af-c34c-44de-9798-e83dee574872"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting keras-preprocessing>=1.0.5\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.4/503.4 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.16.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.37.1)\n",
            "Collecting absl-py>=0.7.0\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-pasta>=0.1.6\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
            "Collecting protobuf>=3.6.1\n",
            "  Downloading protobuf-4.22.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.21.6)\n",
            "Collecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.53.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py\n",
            "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (59.8.0)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=c82f7e34a1979d5172b0dfa8dd9f936035f6de74789e5d7a2edd742bd19ed28e\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/87/6f/3f34218ef184368cec9ee65bdfd65baf117811f0a0ce1263ff\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, wrapt, werkzeug, termcolor, protobuf, opt-einsum, keras-preprocessing, h5py, grpcio, google-pasta, gast, astor, absl-py, markdown, keras-applications, tensorboard, tensorflow\n",
            "Successfully installed absl-py-1.4.0 astor-0.8.1 gast-0.2.2 google-pasta-0.2.0 grpcio-1.53.0 h5py-3.8.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.4.3 opt-einsum-3.3.0 protobuf-4.22.1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 termcolor-2.2.0 werkzeug-2.2.3 wrapt-1.15.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "Successfully installed numpy-1.19.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20.1\n",
            "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.22.1\n",
            "    Uninstalling protobuf-4.22.1:\n",
            "      Successfully uninstalled protobuf-4.22.1\n",
            "Successfully installed protobuf-3.20.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "outputs": [],
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown In the case that an inference database is large and a long duration of continuous runtime is required, a GCP TPU/runtime to run this notebook may be desirable. If that's the case, specify here:\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "#@markdown How many TPU scores the TPU has: if using colab, NUM_TPU_CORES is 8.\n",
        "NUM_TPU_CORES = 8 #@param {type:\"number\"}\n",
        "#@markdown Which mode to use (a different mode means a different finetuning task): options are:\n",
        "#@markdown * \"MRPC\" - paired sequence method\n",
        "#@markdown * \"MRPC_w_ex_data\" - paired sequence method with external data\n",
        "#@markdown * \"RE\" - single sequence method\n",
        "#@markdown * \"NER\" - single sequence per residue prediction \n",
        "#@markdown \n",
        "#@markdown You can add more modes by creating a new processor and/or a new model_fn inside of the \"mutformer_model_code\" folder downloaded from github, then changing the corresponding code snippets in the code segment named \"Authorize for GCS, Imports, and General Setup\" (also edit the dropdown below).\n",
        "MODE = \"MRPC_w_ex_data\" #@param   [\"MRPC_w_ex_data\", \"MRPC\", \"RE\", \"NER\"]   {type:\"string\"} \n",
        "                        ####      ^^^^^ dropdown list for all modes ^^^^^\n",
        "#@markdown Name of the GCS bucket to use (Make sure to set this to the name of your own GCS  bucket):\n",
        "BUCKET_NAME = \"\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown Where the processed data was stored in GCS:\n",
        "PROCESSED_DATA_DIR = \"all_snp_prediction_data_loaded\" #@param {type:\"string\"}\n",
        "#@markdown What folder to write predictions into (location of this folder will either be GCS or google drive) (the PREDICTIONS_FOLDER variable can be the same across all finetuning notebooks):\n",
        "PREDICTIONS_FOLDER = \"full_database_prediction\" #@param {type:\"string\"}\n",
        "#@markdown What folder to write evaluation results into (location of this folder will either be GCS or google drive) EVALUATIONS_FOLDER variable can be the same across all finetuning notebooks):\n",
        "EVALUATIONS_FOLDER = \"\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLgXP-sV4zAd"
      },
      "source": [
        "#If running on a GCP runtime, follow these instructions to set it up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eT2L30n5Eiu"
      },
      "source": [
        "###1) Create a VM from the GCP website\n",
        "###2) Open a command prompt on your computer and perform the following steps\"\n",
        "To ssh into the VM, run:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Note: Make sure the port above matches the port below (in this case it's 8888)\n",
        "\\\n",
        "\\\n",
        "In the new command prompt that popped out, either run each of the commands below individually, or copy and paste the one liner below:\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "One command:\n",
        "```\n",
        "sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "###3) In this notebook, click the \"connect to local runtime\" option under the connect button, and copy and paste the link outputted by command prompt with \"locahost: ...\"\n",
        "###4) Finally, run this code segment, which creates a TPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOygMeIt5JqM"
      },
      "outputs": [],
      "source": [
        "GCE_PROJECT_NAME = \"\" #@param {type:\"string\"}\n",
        "TPU_ZONE = \"us-central1-f\" #@param {type:\"string\"}\n",
        "TPU_NAME = \"mutformer-tpu\" #@param {type:\"string\"}\n",
        "\n",
        "!gcloud alpha compute tpus create $TPU_NAME --accelerator-type=tpu-v2 --version=1.15.5 --zone=$TPU_ZONE ##create new TPU\n",
        "\n",
        "!gsutil iam ch serviceAccount:`gcloud alpha compute tpus describe $TPU_NAME | grep serviceAccount | cut -d' ' -f2`:admin $BUCKET_PATH && echo 'Successfully set permissions!' ##give TPU access to GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k0FGNhvN_Qa"
      },
      "source": [
        "#Clone the MutFormer repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DDPM9ml0N-za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32702883-fb54-485d-d66f-a50bedeab0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mutformer'...\n",
            "remote: Enumerating objects: 1574, done.\u001b[K\n",
            "remote: Counting objects: 100% (454/454), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 1574 (delta 313), reused 364 (delta 256), pack-reused 1120\u001b[K\n",
            "Receiving objects: 100% (1574/1574), 5.93 MiB | 21.22 MiB/s, done.\n",
            "Resolving deltas: 100% (1102/1102), done.\n"
          ]
        }
      ],
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git\n",
        "#@markdown Where to clone the repo into:\n",
        "REPO_DESTINATION_PATH = \"mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Authorize for GCS, Imports, and General Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9S4CiOh3RzFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b17030f-160b-4cf1-ca5e-320a9c7a8bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authorize for GCS:\n",
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=iWIhEfnRcyvXqK8cwcvOSWYMl3zQ5c&prompt=consent&access_type=offline&code_challenge=uRQFdPMf1oE27z27HWUjdNyO4VX72sfa_3m-cmIu2dk&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AVHEtk78S6d9vEeh7WQj_A_JrJtb2lwm8qSa2fh45lcYhumGfGYJou67A0DNM2at4S6z-g\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /tmp/ipykernel_12380/3146016911.py:18: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2023-04-04 21:25:49.877882: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2023-04-04 21:25:49.887689: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2023-04-04 21:25:49.887724: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (44f1cc70897b): /proc/driver/nvidia/version does not exist\n",
            "2023-04-04 21:25:49.889100: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2023-04-04 21:25:49.897478: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
            "2023-04-04 21:25:49.899306: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3fe93f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2023-04-04 21:25:49.899336: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/mutformer/optimization.py:105: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "2023-04-04 21:25:51,370 - tensorflow - INFO - Using TPU runtime\n",
            "INFO:tensorflow:Using TPU runtime\n",
            "2023-04-04 21:25:51,373 - tensorflow - INFO - TPU address is grpc://10.17.185.194:8470\n",
            "INFO:tensorflow:TPU address is grpc://10.17.185.194:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authorize done\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import importlib\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "\n",
        "if not GCP_RUNTIME:\n",
        "  print(\"Authorize for GCS:\")\n",
        "  def authenticate_user(): ##authentication function that uses link authentication instead of popup\n",
        "    if os.path.exists(\"/content/.config/application_default_credentials.json\"): \n",
        "      return\n",
        "    !gcloud auth application-default login  --no-launch-browser\n",
        "    with tf.Session() as sess:\n",
        "      with open(\"/content/.config/application_default_credentials.json\", 'r') as f:\n",
        "            auth_info = json.load(f)\n",
        "      tf.contrib.cloud.configure_gcs(sess, credentials=auth_info)\n",
        "  authenticate_user()\n",
        "  print(\"Authorize done\")\n",
        "  \n",
        "if REPO_DESTINATION_PATH == \"mutformer\":\n",
        "  if os.path.exists(\"mutformer_code\"):\n",
        "    shutil.rmtree(\"mutformer_code\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH,\"mutformer_code\")\n",
        "  REPO_DESTINATION_PATH = \"mutformer_code\"\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic  #### <<<<< if you added more modes, change these imports to import the correct processors, \n",
        "from mutformer.modeling import BertModel,BertModelModified                                        #### <<<<< correct training scripts (i.e. run_classifier and run_ner_for_pathogenic), and\n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor,MrpcWithExDataProcessor            #### <<<<< correct model classes\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor  \n",
        "\n",
        "##reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_classifier,\n",
        "                  run_ner_for_pathogenic]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "#@markdown Whether or not to write logs to a file\n",
        "DO_FILE_LOGGING = True #@param {type:\"boolean\"}\n",
        "if DO_FILE_LOGGING:\n",
        "  #@markdown * If using file logging, what path to write logs to\n",
        "  FILE_LOGGING_PATH = 'file_logging/spam.log' #@param {type:\"string\"}\n",
        "  if not os.path.exists(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1])):\n",
        "    os.makedirs(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1]))\n",
        "  fh = logging.FileHandler(FILE_LOGGING_PATH)\n",
        "  fh.setLevel(logging.INFO)\n",
        "  fh.setFormatter(formatter)\n",
        "  log.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    ##upload credentials to TPU.\n",
        "    with open(f\"/content/.config/application_default_credentials.json\", 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\":      ####       vvvvv if you added more modes, change this part to set the processors and training scripts correctly vvvvv\n",
        "  processor = MrpcProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = False\n",
        "elif MODE==\"MRPC_w_ex_data\":\n",
        "  processor = MrpcWithExDataProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = True\n",
        "elif MODE==\"RE\":\n",
        "  processor = REProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = False\n",
        "elif MODE==\"NER\":\n",
        "  processor = NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "  USING_EX_DATA = False\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\",\\\"MRPC_w_ex_data\\\" \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()\n",
        "                      ####       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0TXw9GtCKz"
      },
      "source": [
        "#Specify location preferences for google drive vs GCS/Mount Drive if needed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eYsYBUCJMTdz"
      },
      "outputs": [],
      "source": [
        "#@markdown ###### Note: For all of these, if using GCP_RUNTIME, all of these parameters must use GCS, because a GCP TPU can't access google drive\n",
        "#@markdown \\\n",
        "#@markdown If original data was stored in drive and data was not generated into more than one shard, full drive path to the original data (for detecting the # of steps per epoch) (this variable should match up with the \"INPUT_DATA_DIR\" variable in the data generation script) (this is used to limit interaction with GCS; it can also be left blank and steps will be automatically detected from tfrecords stored in GCS):\n",
        "#@markdown * If GCP_RUNTIME, drive paths will not work, so steps detection will automatically default to tfrecords\n",
        "ORIG_DATA_FOLDER = \"\" #@param {type: \"string\"}\n",
        "DRIVE_PATH = \"/content/drive/My Drive\"\n",
        "#@markdown Whether to use GCS for writing predictions, if not, defaults to drive\n",
        "GCS_PREDICTIONS = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether to use GCS for writing eval results, if not, defaults to drive\n",
        "GCS_EVAL = True #@param {type:\"boolean\"}\n",
        "\n",
        "PREDS_PATH = BUCKET_PATH if GCS_PREDICTIONS else DRIVE_PATH\n",
        "EVALS_PATH = BUCKET_PATH if GCS_EVAL else DRIVE_PATH\n",
        "\n",
        "if GCP_RUNTIME:\n",
        "  FILES_PATH = BUCKET_PATH\n",
        "\n",
        "if (\"/content/drive\" in ORIG_DATA_FOLDER and not GCP_RUNTIME) or not GCS_PREDICTIONS or not GCS_EVAL:\n",
        "  def mount_drive(): ##mount drive function which uses link mounting instead of popup mounting\n",
        "    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
        "      os.makedirs(\"/content/drive/MyDrive\")\n",
        "      !sudo add-apt-repository -y ppa:alessandro-strada/ppa &> /dev/null ##install google-drive-ocamlfuse\n",
        "      !sudo apt-get update -qq &> /dev/null\n",
        "      !sudo apt -y install -qq google-drive-ocamlfuse &> /dev/null\n",
        "    if len(os.listdir(\"/content/drive/MyDrive\")) >0:\n",
        "      print(\"Drive already mounted.\")\n",
        "      return\n",
        "\n",
        "    if not os.path.exists(\"/content/driveauthlink.txt\") or not open(\"/content/driveauthlink.txt\").read(): ##if the auth link has not been generated, generate it\n",
        "      !google-drive-ocamlfuse &> /content/driveauthlink.txt\n",
        "    !sudo apt-get install -qq w3m &> /dev/null\n",
        "    !xdg-settings set default-web-browser w3m.desktop &> /dev/null\n",
        "    import re\n",
        "    link = re.findall(\"https://.+\",[x for x in open(\"/content/driveauthlink.txt\").read().split(\"\\n\") if x][-1])[0].split(\"\\\"\")[0]\n",
        "    print(f\"Click this link to authenticate for mounting drive: {link}\") ##print auth link\n",
        "    print(\"Waiting for valid athentication...\")\n",
        "    error = None\n",
        "    while True: ##while the google-drive-ocamlfuse mounting doesn't work (user hasn't athenticated yet), keep trying    \n",
        "      if os.path.exists(\"/content/drivemounterror.txt\"):\n",
        "        os.remove(\"/content/drivemounterror.txt\")\n",
        "      !google-drive-ocamlfuse /content/drive/MyDrive 2> \"/content/drivemounterror.txt\" 1> /dev/null\n",
        "      if error and open(\"/content/drivemounterror.txt\").read()!=error:\n",
        "        raise Exception(f\"Drive mount failed. Error: \\n\\n {open('/content/drivemounterror.txt').read()}\")\n",
        "      error = open(\"/content/drivemounterror.txt\").read()\n",
        "      no_error = not len(error) >0\n",
        "      if no_error:\n",
        "        if len(os.listdir(\"/content/drive/MyDrive\")) >0:\n",
        "          print(\"Drive mounted successfully!\")\n",
        "        else:\n",
        "          raise Exception(f\"Drive mount failed. Error: Unknown (likely Keyboard Interrupt)\")\n",
        "        break\n",
        "  mount_drive()\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run Eval/prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MRlQ61PZ4fl"
      },
      "source": [
        "This following section will perform evaluation and prediction on either the eval dataset or the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvuxLWh5_Vm8"
      },
      "source": [
        "###General Setup and definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pVe9VfxJ_Y38"
      },
      "outputs": [],
      "source": [
        "#@markdown When performing prediction, whether or not to ensure all datapoints are predicted via a trailing test dataset: (if so, make sure this option was also specified as True during data generation)\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown Maximum batch size the runtime can handle during prediction without OOM for all models being evaluated/tested: note that this value should match up with the variable \"MAX_BATCH_SIZE\" in the data generation script.\n",
        "MAX_BATCH_SIZE =   512#@param {type:\"integer\"}\n",
        "\n",
        "def latest_checkpoint(dir):\n",
        "  cmd = \"gsutil ls \"+dir\n",
        "  files = !{cmd}\n",
        "  for file in files:\n",
        "    if \"model.ckpt\" in file:\n",
        "      return file.replace(\".\"+file.split(\".\")[-1],\"\")\n",
        "\n",
        "def write_metrics(metrics,dir):\n",
        "  tf.logging.info(\"writing metrics to \"+dir)\n",
        "  if os.path.exists(dir):\n",
        "    shutil.rmtree(dir)\n",
        "  os.makedirs(dir)\n",
        "  gs = metrics[\"global_step\"]\n",
        "  tf.logging.info(\"global step \"+str(gs))\n",
        "\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "  tf.reset_default_graph()\n",
        "  for key,value in metrics.items():\n",
        "    tf.logging.info(str(key)+\":\"+str(value))\n",
        "    x_scalar = tf.constant(value)\n",
        "    first_summary = tf.summary.scalar(name=key, tensor=x_scalar)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        writer = tf.summary.FileWriter(dir)\n",
        "        sess.run(init)\n",
        "        summary = sess.run(first_summary)\n",
        "        writer.add_summary(summary, gs)\n",
        "        writer.flush()\n",
        "        tf.logging.info(\"Done with writing the scalar summary\")\n",
        "    time.sleep(1)\n",
        "\n",
        "  if GCS_EVAL:\n",
        "    cmd = \"gsutil -m cp -r \\\"\"+dir+\"/.\\\" \\\"\"+EVALS_PATH+\"/\"+dir+\"\\\"\"\n",
        "    !{cmd}  \n",
        "  else:\n",
        "    if not os.path.exists(EVALS_PATH+\"/\"+dir):\n",
        "      os.makedirs(EVALS_PATH+\"/\"+dir)\n",
        "    shutil.copytree(dir,EVALS_PATH+\"/\"+dir)\n",
        "  \n",
        "\n",
        "def write_predictions(PREDICTIONS_DIR,\n",
        "                      result,\n",
        "                      result_trailing,\n",
        "                      shard_id=\"\"):\n",
        "  if not os.path.exists(PREDS_PATH+\"/\"+PREDICTIONS_DIR):\n",
        "    os.makedirs(PREDS_PATH+\"/\"+PREDICTIONS_DIR)\n",
        "  with tf.gfile.Open(PREDS_PATH+\"/\"+PREDICTIONS_DIR+\"/predictions\"+shard_id+\".txt\", \"w\") as writer:\n",
        "    tf.logging.info(\"***** Predict results *****\")\n",
        "    if result:\n",
        "      for (i, prediction) in enumerate(result):\n",
        "        output_line = \"\\t\".join([str(k)+\":\"+str(v) for k,v in prediction.items()]) + \"\\n\"\n",
        "        writer.write(output_line)\n",
        "    if result_trailing:\n",
        "      for (i, prediction) in enumerate(result_trailing):\n",
        "        output_line = \"\\t\".join([str(k)+\":\"+str(v) for k,v in prediction.items()]) + \"\\n\"\n",
        "        writer.write(output_line)\n",
        "\n",
        "\n",
        "def evaluation_loop(RUN_EVAL,\n",
        "                    RUN_PREDICTION,\n",
        "                    EVALUATE_WHILE_PREDICT,\n",
        "                    test_or_dev,\n",
        "                    MODEL,\n",
        "                    total_metrics,\n",
        "                    MAX_SEQ_LENGTH,\n",
        "                    current_ORIG_DATA_FOLDER,\n",
        "                    BERT_GCS_DIR,\n",
        "                    USE_LATEST,\n",
        "                    CHECKPOINT_STEP,\n",
        "                    DATA_GCS_DIR,\n",
        "                    USING_SHARDS,\n",
        "                    START_SHARD,\n",
        "                    END_SHARD,\n",
        "                    USING_EX_DATA,\n",
        "                    PRED_NUM,\n",
        "                    EVAL_WHILE_PREDICT_PREDICTIONS_DIR,\n",
        "                    PREDICTIONS_DIR,\n",
        "                    EVALUATIONS_DIR,\n",
        "                    CONFIG_FILE):\n",
        "\n",
        "  try: ##wrap everything in a giant try except so that any \n",
        "       ##glitches won't completely stop evaluation in the middle\n",
        "    current_ckpt = \"\"\n",
        "\n",
        "    tf.logging.info(\"Using data from: \"+DATA_GCS_DIR)\n",
        "    tf.logging.info(\"Loading model from: \"+BERT_GCS_DIR)\n",
        "\n",
        "    \n",
        "    def steps_getter(input_files):\n",
        "      tot_sequences = []\n",
        "      for input_file in input_files:\n",
        "        tf.logging.info(\"reading:\"+input_file+\" for steps\")\n",
        "\n",
        "        d = tf.data.TFRecordDataset(input_file)\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "          tot_sequences.append(sess.run(d.reduce(0, lambda x,_: x+1)))\n",
        "\n",
        "      return tot_sequences\n",
        "\n",
        "    test_datasets = [re.findall(\"test_(\\w+).tf_record\",file)[0] \\\n",
        "                for file in tf.io.gfile.listdir(DATA_GCS_DIR) \\\n",
        "                if re.findall(\"test_(\\w+).tf_record\",file) and \"trailing\" not in file]\n",
        "    if not test_datasets or test_or_dev!=\"test\":\n",
        "      test_datasets = [None]\n",
        "    for dataset in test_datasets:\n",
        "      evaluating_file = f\"{test_or_dev}_{dataset}.tf_record\" if dataset else f\"{test_or_dev}.tf_record\"\n",
        "      eval_file = os.path.join(DATA_GCS_DIR, evaluating_file)\n",
        "      PREDICTIONS_DIR_for_dataset = f\"{PREDICTIONS_DIR}/{dataset}\"  if dataset else PREDICTIONS_DIR\n",
        "      EVALUATIONS_DIR_for_dataset = f\"{EVALUATIONS_DIR}/{dataset}\" if dataset else EVALUATIONS_DIR\n",
        "      if USING_SHARDS:\n",
        "        shards_folder = DATA_GCS_DIR\n",
        "        input_file = os.path.join(DATA_GCS_DIR, evaluating_file)\n",
        "        file_name = input_file.split(\"/\")[-1]\n",
        "        all_shards = [[int(re.match(f\"{file_name}_(\\d+)\", file).groups()[0]), shards_folder + \"/\" + file] for file in tf.io.gfile.listdir(shards_folder) if\n",
        "                  re.match(f\"{file_name}_\\d+\", file)]\n",
        "        all_shards = sorted(all_shards,key=lambda x:x[0])\n",
        "        shards_and_inds = [[shard_ind,shard] for shard_ind,shard in all_shards if START_SHARD<=shard_ind and ((shard_ind<END_SHARD) if END_SHARD!=-1 else True)]\n",
        "\n",
        "        shards = [shard for shard_ind,shard in shards_and_inds]\n",
        "        shard_inds = [shard_ind for shard_ind,shard in shards_and_inds]\n",
        "      else:\n",
        "        all_shards = [[0,eval_file]]\n",
        "        shards = [eval_file]\n",
        "\n",
        "      if USING_SHARDS:\n",
        "        tf.logging.info(\"\\nUSING SHARDs:\")\n",
        "        for n,shard in enumerate(shards):\n",
        "          if n==END_SHARD: break\n",
        "          tf.logging.info(shard)\n",
        "        tf.logging.info(\"\\n\")\n",
        "\n",
        "      if RUN_EVAL:\n",
        "        try:\n",
        "          if len(shards)>1:\n",
        "            raise Exception(\"more than one shard needs detection of steps from tfrecords. Reverting to tfrecord steps detection...\")\n",
        "          if dataset==\"dev\":\n",
        "            data_path = \"/content/drive/My Drive/\"+current_ORIG_DATA_FOLDER+\"/dev.tsv\"\n",
        "          else:\n",
        "            data_path = \"/content/drive/My Drive/\"+current_ORIG_DATA_FOLDER+(f\"/test_{dataset}.tsv\" if dataset else \"test.tsv\")\n",
        "          lines = open(data_path).read().split(\"\\n\")\n",
        "          EVAL_STEPSs = [int(len(lines)/EVAL_BATCH_SIZE)]\n",
        "        except Exception:\n",
        "          SEQUENCES_PER_EPOCHs = steps_getter(shards)\n",
        "          EVAL_STEPSs = [int(SEQUENCES_PER_EPOCH/EVAL_BATCH_SIZE) for SEQUENCES_PER_EPOCH in SEQUENCES_PER_EPOCHs]\n",
        "\n",
        "      \n",
        "      if EVALUATE_WHILE_PREDICT:\n",
        "        cmd = \"gsutil -m rm -r \"+EVAL_WHILE_PREDICT_PREDICTIONS_DIR\n",
        "        !{cmd}\n",
        "      def rewrite_ckpt_file2_restore_ckpt():\n",
        "        if USE_LATEST:\n",
        "          try:\n",
        "            latest_ckpt = tf.train.latest_checkpoint(BERT_GCS_DIR).split(\"/\")[-1]\n",
        "            max_step = max([int(ckpt.split(\".\")[-2].split(\"-\")[-1]) for ckpt in tf.io.gfile.listdir(BERT_GCS_DIR)])\n",
        "            RESTORE_CHECKPOINT = [\".\".join(ckpt.split(\".\")[:-1]) \n",
        "                                  for ckpt in tf.io.gfile.listdir(BERT_GCS_DIR) \n",
        "                                  if len(ckpt.split(\".\"))==3 and str(max_step) == ckpt.split(\".\")[-2].split(\"-\")[-1]][0]\n",
        "            old_file_lines = tf.gfile.Open(BERT_GCS_DIR+\"/checkpoint\").read().split(\"\\n\")\n",
        "            new_file_lines = old_file_lines.copy()\n",
        "            new_file_lines[0] = new_file_lines[0].replace(latest_ckpt,RESTORE_CHECKPOINT)\n",
        "            RESTORE_CHECKPOINT = BERT_GCS_DIR+\"/\"+RESTORE_CHECKPOINT\n",
        "\n",
        "            tf.gfile.Open(BERT_GCS_DIR+\"/checkpoint\",\"w+\").write(\"\\n\".join(new_file_lines))\n",
        "            \n",
        "          except Exception:\n",
        "            try:\n",
        "              RESTORE_CHECKPOINT = latest_checkpoint(BERT_GCS_DIR)\n",
        "            except Exception:\n",
        "              raise Exception(\"No checkpoints were found in the given location\")\n",
        "        else:\n",
        "          try:\n",
        "            latest_ckpt = tf.train.latest_checkpoint(BERT_GCS_DIR).split(\"/\")[-1]\n",
        "            RESTORE_CHECKPOINT = [\".\".join(ckpt.split(\".\")[:-1]) \n",
        "                                  for ckpt in tf.io.gfile.listdir(BERT_GCS_DIR) \n",
        "                                  if len(ckpt.split(\".\"))==3 and str(CHECKPOINT_STEP) == ckpt.split(\".\")[-2].split(\"-\")[-1]][0]\n",
        "            old_file_lines = tf.gfile.Open(BERT_GCS_DIR+\"/checkpoint\").read().split(\"\\n\")\n",
        "            new_file_lines = old_file_lines.copy()\n",
        "            new_file_lines[0] = new_file_lines[0].replace(latest_ckpt,RESTORE_CHECKPOINT)\n",
        "            RESTORE_CHECKPOINT = BERT_GCS_DIR+\"/\"+RESTORE_CHECKPOINT\n",
        "\n",
        "            tf.gfile.Open(BERT_GCS_DIR+\"/checkpoint\",\"w+\").write(\"\\n\".join(new_file_lines))\n",
        "          except Exception as e:\n",
        "            tf.logging.info(\"\\n\\nCould not find the checkpoint specified. Error:\"+str(e)+\". Skipping...\\n\\n\")\n",
        "            return False,total_metrics,current_ckpt\n",
        "        return RESTORE_CHECKPOINT\n",
        "\n",
        "      RESTORE_CHECKPOINT = rewrite_ckpt_file2_restore_ckpt()\n",
        "      current_ckpt=RESTORE_CHECKPOINT\n",
        "      tf.logging.info(\"USING CHECKPOINT:\"+RESTORE_CHECKPOINT)\n",
        "        \n",
        "      config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "\n",
        "      model_fn = script.model_fn_builder(\n",
        "          bert_config=config,\n",
        "          num_labels=len(label_list),\n",
        "          init_checkpoint=None,\n",
        "          restore_checkpoint=RESTORE_CHECKPOINT,\n",
        "          init_learning_rate=0,\n",
        "          decay_per_step=0,\n",
        "          num_warmup_steps=10,\n",
        "          use_tpu=True,\n",
        "          use_one_hot_embeddings=True,\n",
        "          bert=MODEL,\n",
        "          test_results_dir=EVAL_WHILE_PREDICT_PREDICTIONS_DIR,\n",
        "          yield_predictions=EVALUATE_WHILE_PREDICT,\n",
        "          using_ex_data=USING_EX_DATA)\n",
        "\n",
        "      \n",
        "      tf.logging.info(\"USING FILE:\"+eval_file)\n",
        "\n",
        "      def load_stuff(batch_size,file):\n",
        "          tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "          run_config = tf.contrib.tpu.RunConfig(\n",
        "            cluster=tpu_cluster_resolver,\n",
        "            model_dir=BERT_GCS_DIR,\n",
        "            tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "                num_shards=min(NUM_TPU_CORES,batch_size),\n",
        "                per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "          estimator = tf.contrib.tpu.TPUEstimator(\n",
        "              use_tpu=True,\n",
        "              model_fn=model_fn,\n",
        "              config=run_config,\n",
        "              train_batch_size=1,\n",
        "              eval_batch_size=batch_size,\n",
        "              predict_batch_size=batch_size)\n",
        "          input_fn = script.file_based_input_fn_builder(\n",
        "              input_file=file,\n",
        "              seq_length=MAX_SEQ_LENGTH,\n",
        "              is_training=False,\n",
        "              drop_remainder=True,\n",
        "              pred_num=PRED_NUM if USING_EX_DATA else None)\n",
        "          return estimator, input_fn\n",
        "\n",
        "      all_eval_metrics = []\n",
        "      tf.logging.info(\"***** Running evaluation/prediction *****\")\n",
        "      tf.logging.info(\" Eval Batch size = \"+str(EVAL_BATCH_SIZE))\n",
        "      tf.logging.info(\" Predict Batch size = \"+str(MAX_BATCH_SIZE))\n",
        "      for n,shard in enumerate(shards):\n",
        "        tf.logging.info(f\"\\n\\nUSING SHARD: {shard}...\\n\\n\")\n",
        "        if RUN_EVAL:\n",
        "          estimator,input_fn = load_stuff(EVAL_BATCH_SIZE,shard)\n",
        "          if EVAL_STEPSs[n] > 0:\n",
        "            RESTORE_CHECKPOINT = rewrite_ckpt_file2_restore_ckpt()\n",
        "            eval_metrics = estimator.evaluate(input_fn=input_fn, steps=EVAL_STEPSs[n])\n",
        "            all_eval_metrics.append([eval_metrics,EVAL_STEPSs[n]*EVAL_BATCH_SIZE])\n",
        "          if PRECISE_TESTING and shard == all_shards[-1][1] and test_or_dev==\"test\":\n",
        "            trailing_test_file = os.path.join(DATA_GCS_DIR, (f\"test_trailing_{dataset}.tf_record\" if dataset else \"test_trailing.tf_record\"))\n",
        "            if tf.gfile.Open(trailing_test_file).size() > 0:\n",
        "              steps = EVAL_STEPSs[n]\n",
        "              estimator_trailing,eval_input_fn_trailing = load_stuff(1,trailing_test_file)\n",
        "              RESTORE_CHECKPOINT = rewrite_ckpt_file2_restore_ckpt()\n",
        "              eval_metrics=estimator_trailing.evaluate(input_fn=eval_input_fn_trailing, steps=steps)\n",
        "              all_eval_metrics.append([eval_metrics,steps])\n",
        "        if RUN_PREDICTION:\n",
        "          result, result_trailing = [None, None]\n",
        "          if tf.gfile.Open(shard).size() > 0: ##sometimes, if test dataset is really small, the test dataset doesn't have data; the data is all in the trailing dataset\n",
        "            estimator,input_fn = load_stuff(MAX_BATCH_SIZE,shard)\n",
        "            RESTORE_CHECKPOINT = rewrite_ckpt_file2_restore_ckpt()\n",
        "            result=estimator.predict(input_fn=input_fn)\n",
        "          if PRECISE_TESTING and shard == all_shards[-1][1] and test_or_dev==\"test\":\n",
        "            trailing_test_file = f\"{DATA_GCS_DIR}/{'test_trailing_'+dataset+'.tf_record' if dataset else 'test_trailing.tf_record'}\" \n",
        "            if tf.gfile.Open(trailing_test_file).size() > 0:\n",
        "              tf.logging.info(f\"\\n\\nUSING TRAILING DATASET: {trailing_test_file}...\\n\\n\")\n",
        "              estimator_trailing,test_input_fn_trailing = load_stuff(1,trailing_test_file)\n",
        "              RESTORE_CHECKPOINT = rewrite_ckpt_file2_restore_ckpt()\n",
        "              result_trailing=estimator_trailing.predict(input_fn=test_input_fn_trailing)\n",
        "          write_predictions(PREDICTIONS_DIR_for_dataset,\n",
        "                            result,\n",
        "                            result_trailing,\n",
        "                            shard_id=str(shard_inds[n]) if USING_SHARDS else \"\")\n",
        "\n",
        "      if RUN_EVAL:\n",
        "        combined_metrics = {}\n",
        "        weight_divisor = sum([v for k,v in all_eval_metrics])\n",
        "        for metric_set,weight in all_eval_metrics:\n",
        "          for k,v in metric_set.items():\n",
        "            try:\n",
        "              combined_metrics[k]+=v*weight/weight_divisor\n",
        "            except:\n",
        "              combined_metrics[k]=v*weight/weight_divisor\n",
        "        \n",
        "        ##write out evaluation metrics data\n",
        "        write_metrics(combined_metrics,EVALUATIONS_DIR_for_dataset)\n",
        "        tf.logging.info(f\"\\n\\n\\n\\n\\n\\nEVAL METRICS ({EVALUATIONS_DIR_for_dataset}):\")\n",
        "        print(all_eval_metrics)\n",
        "        for k,v in combined_metrics.items():\n",
        "          tf.logging.info(k+\":\"+str(v))\n",
        "        tf.logging.info(\"\\n\\n\\n\\n\\n\\n\\n\")\n",
        "\n",
        "        if not REPEAT_LOOP:\n",
        "            total_metrics[EVALUATIONS_DIR_for_dataset] = combined_metrics\n",
        "    return True,total_metrics,current_ckpt\n",
        "  except Exception as e:\n",
        "      tf.logging.info(\"\\n\\nFAILED-error:\"+str(e)+\". Skipping...\\n\\n\")\n",
        "      return False,total_metrics,current_ckpt\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Eval/prediction loops"
      ],
      "metadata": {
        "id": "t9leUaCyufey"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM7HXcKbZvn-"
      },
      "source": [
        "Following are two code segments for runnign the finetuning evaluation/prediction loops:\n",
        "1. Model/sequence length: perform evaluation/prediction for the train loop with the same name from the \"mutformer_finetuning_benchmark\" file\n",
        "1. Freezing/batch size: perform evaluation/prediction for the train loop with the same name from the \"mutformer_finetuning_benchmark\" file\n",
        "\n",
        "Choose a desired code segment to run, enter the desired options for evaluating/predicting, and run that code segment\n",
        "\n",
        "Note: One may write more evaluation/prediction loops for more tests based on a similar format to these two example evaluation/prediction loops below, i.e. batch size/sequence length.\n",
        "\\\n",
        "\\\n",
        "Note: All evaluation results will be written into the previously specified logging directory either under google drive or GCS, depending on the values of GCS_COMS, GCS_PREDICTIONS, and GCS_EVAL specified before. To view the results, use the colab notebook titled \"mutformer processing and viewing finetuning..._results,\" which can also be used to view prediction results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgI2CyoFPjyY"
      },
      "source": [
        "###Model/Sequence Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "outputs": [],
      "source": [
        "#@markdown ### IO config\n",
        "#@markdown Folder for where to load the finetuned model from\n",
        "FINETUNED_MODEL_DIR = \"\" #@param {type:\"string\"}\n",
        "#@markdown Which folder inside of PREDICTIONS_DIR and EVALUATIONS_DIR to write predictions and evaluations, respectively, into:\n",
        "RUN_NAME = \"\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ### Evaluation/prediction procedure config\n",
        "#@markdown The evaluation loop will loop through a list of models and a list of sequence lengths, attempting to evaluate a finetuned model for each combination of pretrained model and sequence length (failed combinations will be skipped).\n",
        "#@markdown * List of pretrained models that were used for finetuning (should indicate the names of the model folders inside INIT_MODEL_DIR from the finetuning training script):\n",
        "MODELS = [\"MutFormer_em_adap8L\"] #@param\n",
        "#@markdown * List of model architectures for each model in the \"MODELS\" list defined in the entry above: each position in this list must correctly indicate the model architecture of its corresponding model folder in the list \"MODELS\" (BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture).\n",
        "MODEL_ARCHITECTURES = [\"MutFormer_embedded_convs\"] #@param\n",
        "#@markdown * List of sequence lengthed models to test\n",
        "MAX_SEQ_LENGTHS = [1024] #@param\n",
        "#@markdown Whether to evaluate on the test set or the dev set (\"test\" or \"dev\")\n",
        "dataset = \"test\" #@param{type:\"string\"}\n",
        "#@markdown Whether or not to run evaluation\n",
        "RUN_EVAL = False #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to run prediction (in a seperate loop from evaluation; EVALUATE_WHILE_PREDICT will override this value to False)\n",
        "RUN_PREDICTION = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to repeat this operation in a loop (if performing parallel evaluation operation, set to True, False otherwise)\n",
        "#@markdown * If using REPEAT_LOOP, to prevent the script from evaluating every single model trained on every single combination of batch size and sequence length every loop, the script will only evaluate models that are being currently trained (the script will only evaluate on the model folders that have seen a new latest checkpoint since the script started running).\n",
        "REPEAT_LOOP = False #@param {type:\"boolean\"}\n",
        "#@markdown When using REPEAT_LOOP, how long to wait in between each loop before checking again for updated train progress:\n",
        "CHECK_MODEL_EVERY_N_SECS =  150#@param {type:\"integer\"}\n",
        "#@markdown If evaluating, whether or not to evaluate and predict results in the same loop; useful when amount of test data is very small and the time it takes to restart a loop is significant (if yes, prediction results will be written in the form of tfevent files into GCS that need to be viewed using the notebook titled \"mutformer processing and viewing finetuning results\")\n",
        "#@markdown \n",
        "#@markdown Note: If using EVALUATE_WHILE_PREDICT, prediction results must be read using the previously mentioned colab notebook, otherwise, predictions will be written directly as txts and will be directly accessible from google drive under the folder specified above\n",
        "EVALUATE_WHILE_PREDICT =  False #@param {type:\"boolean\"}\n",
        "#@markdown What batch size to use during evaluation (larger batch size will increase evaluation speed but may skip more datapoints)\n",
        "EVAL_BATCH_SIZE = 1 #@param {type:\"integer\"}\n",
        "#@markdown Whether or not testing/evaluating data was generated in shards\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown * If using shards, set this value to indicate which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown * If using shards, set this value to indicate which shard index to evaluate until (not inclusive) (defualt -1 for last shard)\n",
        "END_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Whether to use the latest checkpoint in the folder (set to false if an intermediate checkpoint should be used)\n",
        "USE_LATEST = False #@param {type:\"boolean\"}\n",
        "#@markdown * If not using latest checkpoint, which step's checkpoint to use\n",
        "CHECKPOINT_STEP =  None#@param {type:\"integer\"}\n",
        "\n",
        "total_metrics = {}  ## a dictionary for all metrics to  \n",
        "                    ## print at the end during testing, \n",
        "                    ## not necessary during evaluation   \n",
        "if dataset==\"test\":\n",
        "  evaluating_file = \"test.tf_record\"\n",
        "elif dataset==\"dev\":\n",
        "  evaluating_file = \"eval.tf_record\"\n",
        "else:\n",
        "  raise Exception(\"only datasets supported are dev and test\")\n",
        "\n",
        "DATA_INFOS = [[\"N/A\" for MODEL_NAME in MODELS]            ##create an empty 2D list to store all\n",
        "              for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS]      ##the data info dictionaries\n",
        "\n",
        "current_ckpts = [[\"N/A\" for MODEL_NAME in MODELS]\n",
        "                 for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS]\n",
        "for M,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "  for m,MODEL_NAME in enumerate(MODELS):\n",
        "        BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "        try:\n",
        "          current_ckpts[M][m] = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "        except:\n",
        "          try:\n",
        "            current_ckpts[M][m] = latest_checkpoint(BERT_GCS_DIR)\n",
        "          except:\n",
        "            raise Exception(f\"could not find any checkpoints in the model dir specified:{BERT_GCS_DIR}\")\n",
        "\n",
        "def get_new_ckpts(current_ckpts):\n",
        "  new_ckpts = []\n",
        "  for M,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "    for m,MODEL_NAME in enumerate(MODELS):\n",
        "          BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "          try:\n",
        "            current_ckpt = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "            if current_ckpts[M][m]!=current_ckpt:\n",
        "              new_ckpts.append([M,m])\n",
        "          except:\n",
        "            try:\n",
        "              current_ckpt = latest_checkpoint(BERT_GCS_DIR)\n",
        "              if current_ckpts[M][m]!=current_ckpt:\n",
        "                new_ckpts.append([M,m])\n",
        "            except:\n",
        "              raise Exception(f\"could not find any checkpoints in the model dir specified:{BERT_GCS_DIR}\")\n",
        "  return new_ckpts\n",
        "\n",
        "while True:\n",
        "  sleeping = True   ##to prevent excessive interaction with GCS, \n",
        "                    ##if an eval/pred loop fails, the script \n",
        "                    ##will wait for a while before trying again\n",
        "\n",
        "  if REPEAT_LOOP:                             ##if using REPEAT_LOOP, only evaluate on new checkpoints\n",
        "    new_ckpts = get_new_ckpts(current_ckpts)\n",
        "    if len(new_ckpts) == 0:\n",
        "      print(\"No new checkpoints have been written since script start/last evaluation. Trying again in another\",CHECK_MODEL_EVERY_N_SECS,\"seconds.\")\n",
        "\n",
        "  for M,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "    for m,MODEL_NAME in enumerate(MODELS):\n",
        "\n",
        "      if REPEAT_LOOP:\n",
        "        if [M,m] not in new_ckpts:\n",
        "          continue\n",
        "\n",
        "      print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "            \"\\nINPUT MAX SEQ LENGTH:\",MAX_SEQ_LENGTH)\n",
        "      \n",
        "      MODEL = getattr(modeling, MODEL_ARCHITECTURES[m])\n",
        "      current_ORIG_DATA_FOLDER= ORIG_DATA_FOLDER+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "      BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "      DATA_GCS_DIR = BUCKET_PATH+\"/\"+PROCESSED_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "          \n",
        "      EVAL_WHILE_PREDICT_PREDICTIONS_DIR = BUCKET_PATH+\"/\"+PREDICTIONS_FOLDER+\"/\"+RUN_NAME+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "      EVALUATIONS_DIR = EVALUATIONS_FOLDER+\"/\"+RUN_NAME+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "      PREDICTIONS_DIR = PREDICTIONS_FOLDER+\"/\"+RUN_NAME+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "      CONFIG_FILE = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)+\"/config.json\"\n",
        "      \n",
        "      if DATA_INFOS[M][m] == \"N/A\":\n",
        "        DATA_INFOS[M][m] = json.load(tf.gfile.Open(DATA_GCS_DIR+\"/info.json\"))\n",
        "      \n",
        "      EX_DATA_NUM = DATA_INFOS[M][m][\"ex_data_num\"] if USING_EX_DATA else 0\n",
        "\n",
        "\n",
        "      ##run the evaluation/prediction loop\n",
        "      sucess,total_metrics,current_ckpt = \\\n",
        "          evaluation_loop(RUN_EVAL,\n",
        "                          RUN_PREDICTION,\n",
        "                          EVALUATE_WHILE_PREDICT,\n",
        "                          dataset,\n",
        "                          MODEL,\n",
        "                          total_metrics,\n",
        "                          MAX_SEQ_LENGTH,\n",
        "                          current_ORIG_DATA_FOLDER,\n",
        "                          BERT_GCS_DIR,\n",
        "                          USE_LATEST,\n",
        "                          CHECKPOINT_STEP,\n",
        "                          DATA_GCS_DIR,\n",
        "                          USING_SHARDS,\n",
        "                          START_SHARD,\n",
        "                          END_SHARD,\n",
        "                          USING_EX_DATA,\n",
        "                          EX_DATA_NUM,\n",
        "                          EVAL_WHILE_PREDICT_PREDICTIONS_DIR,\n",
        "                          PREDICTIONS_DIR,\n",
        "                          EVALUATIONS_DIR,\n",
        "                          CONFIG_FILE)\n",
        "              \n",
        "      current_ckpts[M][m] = current_ckpt\n",
        "      if sucess:\n",
        "        sleeping = False\n",
        "    break\n",
        "  time.sleep(CHECK_MODEL_EVERY_N_SECS if sleeping else 0)\n",
        "  if not REPEAT_LOOP:\n",
        "    break\n",
        "if not REPEAT_LOOP:\n",
        "  for evals_dir,metrics in total_metrics.items():\n",
        "    print(\"Printing metrics for:\",evals_dir,\"\\n\")\n",
        "    for key,metric in metrics.items():\n",
        "      print(key+\":\",metric)\n",
        "    print(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFvyPQ8NMBxC"
      },
      "source": [
        "###Freezing/Batch Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmtho7loME5y"
      },
      "outputs": [],
      "source": [
        "#@markdown ### IO config\n",
        "#@markdown Folder for where to load the finetuned model from\n",
        "FINETUNED_MODEL_DIR = \"\" #@param {type:\"string\"}\n",
        "#@markdown Which folder inside of PREDICTIONS_DIR and EVALUATIONS_DIR to write predictions and evaluations, respectively, into:\n",
        "RUN_NAME = \"\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ### Evaluation/prediction procedure config\n",
        "#@markdown The evaluation loop will loop through a list of models and a list of sequence lengths, attempting to evaluate a finetuned model for each combination of pretrained model and sequence length (failed combinations will be skipped).\n",
        "#@markdown * List of pretrained models that were used for finetuning (should indicate the names of the model folders inside INIT_MODEL_DIR from the finetuning training script):\n",
        "FREEZINGS = [0] #@param\n",
        "#@markdown Batch size to use\n",
        "BATCH_SIZES =  [32] #@param\n",
        "#@markdown The training loop will loop through a list of pretrained models and a list of sequence lengths, training a model for each combination of pretrained model and sequence length\n",
        "#@markdown * Model Name to use (should indicate the name of a model folder inside the specified INIT_MODEL_DIR\n",
        "MODEL_NAME =  \"MutFormer_em_adap8L\"#@param\n",
        "#@markdown * Model architecture to use. Must correctly correspond to the model indicated by the model folder specified by the above \"MODEL_NAME\" parameter (BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture without integrated convs, MutFormer_embedded_convs indicates MutFormer with integrated convolutions).\n",
        "MODEL_ARCHITECTURE = \"MutFormer_embedded_convs\" #@param\n",
        "#@markdown * List of sequence lengths to test\n",
        "MAX_SEQ_LENGTH = 1024 #@param\n",
        "#@markdown What dataset to evaluate/predict (either \"dev\" or \"test\"):\n",
        "dataset = \"train\" #@param{type:\"string\"}\n",
        "#@markdown Whether or not to run evaluation\n",
        "RUN_EVAL = False #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to run prediction (in a seperate loop from evaluation; EVALUATE_WHILE_PREDICT will override this value to False)\n",
        "RUN_PREDICTION = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to repeat this operation in a loop (if performing parallel evaluation operation, set to True, False otherwise)\n",
        "#@markdown * If using REPEAT_LOOP, to prevent the script from evaluating every single model trained on every single combination of batch size and sequence length every loop, the script will only evaluate models that are being currently trained (the script will only evaluate on the model folders that have seen a new latest checkpoint since the script started running).\n",
        "REPEAT_LOOP = False #@param {type:\"boolean\"}\n",
        "#@markdown When using REPEAT_LOOP, how long to wait in between each loop before checking again for updated train progress:\n",
        "CHECK_MODEL_EVERY_N_SECS =  150#@param {type:\"integer\"}\n",
        "#@markdown If evaluating, whether or not to evaluate and predict results in the same loop; useful when amount of test data is very small and the time it takes to restart a loop is significant (if yes, prediction results will be written in the form of tfevent files into GCS that need to be viewed using the notebook titled \"mutformer processing and viewing finetuning results\")\n",
        "#@markdown \n",
        "#@markdown Note: If using EVALUATE_WHILE_PREDICT, prediction results must be read using the previously mentioned colab notebook, otherwise, predictions will be written directly as txts and will be directly accessible from google drive under the folder specified above\n",
        "EVALUATE_WHILE_PREDICT =  False #@param {type:\"boolean\"}\n",
        "#@markdown What batch size to use during evaluation (larger batch size will increase evaluation speed but may skip more datapoints)\n",
        "EVAL_BATCH_SIZE =  2#@param {type:\"integer\"}\n",
        "#@markdown Whether or not testing/evaluating data was generated in shards\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown * If using shards, set this value to indicate which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown * If using shards, set this value to indicate which shard index to evaluate until (not inclusive) (defualt -1 for last shard)\n",
        "END_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Whether to use the latest checkpoint in the folder (set to false if an intermediate checkpoint should be used)\n",
        "USE_LATEST = False #@param {type:\"boolean\"}\n",
        "#@markdown * If not using latest checkpoint, which step's checkpoint to use\n",
        "CHECKPOINT_STEP =  None#@param {type:\"integer\"}\n",
        "\n",
        "total_metrics = {}  ## a dictionary for all metrics to  \n",
        "                    ## print at the end during testing, \n",
        "                    ## not necessary during evaluation   \n",
        "\n",
        "  \n",
        "\n",
        "DATA_INFOS = [[\"N/A\" for BATCH_SIZE in BATCH_SIZES]            ##create an empty 2D list to store all\n",
        "              for FREEZING in FREEZINGS]      ##the data info dictionaries\n",
        "\n",
        "current_ckpts = [[\"N/A\" for BATCH_SIZE in BATCH_SIZES] for FREEZING in FREEZINGS]\n",
        "for M,FREEZING in enumerate(FREEZINGS):\n",
        "    for m,BATCH_SIZE in enumerate(BATCH_SIZES):\n",
        "        BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+f\"/fl_{FREEZING}_bs_{BATCH_SIZE}\"\n",
        "        try:\n",
        "          current_ckpts[M][m] = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "        except:\n",
        "          try:\n",
        "            current_ckpts[M][m] = latest_checkpoint(BERT_GCS_DIR)\n",
        "          except:\n",
        "            pass\n",
        "\n",
        "def get_new_ckpts(current_ckpts):\n",
        "  new_ckpts = []\n",
        "  for M,FREEZING in enumerate(FREEZINGS):\n",
        "    for m,BATCH_SIZE in enumerate(BATCH_SIZES):\n",
        "          BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+f\"/fl_{FREEZING}_bs_{BATCH_SIZE}\"\n",
        "          try:\n",
        "            current_ckpt = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "            if current_ckpts[M][m]!=current_ckpt:\n",
        "              new_ckpts.append([M,m])\n",
        "          except:\n",
        "            try:\n",
        "              current_ckpt = latest_checkpoint(BERT_GCS_DIR)\n",
        "              if current_ckpts[M][m]!=current_ckpt:\n",
        "                new_ckpts.append([M,m])\n",
        "            except:\n",
        "              pass\n",
        "  return new_ckpts\n",
        "\n",
        "while True:\n",
        "  sleeping = True   ##to prevent excessive interaction with GCS, \n",
        "                    ##if an eval/pred loop fails, the script \n",
        "                    ##will wait for a while before trying again\n",
        "\n",
        "  if REPEAT_LOOP:                             ##if using REPEAT_LOOP, only evaluate on new checkpoints\n",
        "    new_ckpts = get_new_ckpts(current_ckpts)\n",
        "    if len(new_ckpts) == 0:\n",
        "      print(\"No new checkpoints have been written since script start/last evaluation. Trying again in another\",CHECK_MODEL_EVERY_N_SECS,\"seconds.\")\n",
        "\n",
        "  for M,FREEZING in enumerate(FREEZINGS):\n",
        "    for m,BATCH_SIZE in enumerate(BATCH_SIZES):\n",
        "\n",
        "      if REPEAT_LOOP:\n",
        "        if [M,m] not in new_ckpts:\n",
        "          continue\n",
        "\n",
        "      print(\"\\n\\n\\nFreezing layers:\",FREEZING,\n",
        "            \"\\nBATCH SIZE:\",BATCH_SIZE)\n",
        "      \n",
        "      MODEL = getattr(modeling, MODEL_ARCHITECTURE)\n",
        "      current_ORIG_DATA_FOLDER= ORIG_DATA_FOLDER+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "      BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+f\"/fl_{FREEZING}_bs_{BATCH_SIZE}\"\n",
        "      DATA_GCS_DIR = BUCKET_PATH+\"/\"+PROCESSED_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "          \n",
        "      EVAL_WHILE_PREDICT_PREDICTIONS_DIR = BUCKET_PATH+\"/\"+PREDICTIONS_FOLDER+\"/\"+RUN_NAME+f\"/fl_{FREEZING}_bs_{BATCH_SIZE}\"\n",
        "      EVALUATIONS_DIR = EVALUATIONS_FOLDER+\"/\"+RUN_NAME+f\"/fl_{FREEZING}_bs_{BATCH_SIZE}\"\n",
        "      PREDICTIONS_DIR = PREDICTIONS_FOLDER+\"/\"+RUN_NAME+f\"/fl_{FREEZING}_bs_{BATCH_SIZE}\"\n",
        "      CONFIG_FILE = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+f\"/fl_{FREEZING}_bs_{BATCH_SIZE}\"+\"/config.json\"\n",
        "      \n",
        "      if DATA_INFOS[M][m] == \"N/A\":\n",
        "        DATA_INFOS[M][m] = json.load(tf.gfile.Open(DATA_GCS_DIR+\"/info.json\"))\n",
        "      \n",
        "      EX_DATA_NUM = DATA_INFOS[M][m][\"ex_data_num\"] if USING_EX_DATA else 0\n",
        "\n",
        "\n",
        "      ##run the evaluation/prediction loop\n",
        "      sucess,total_metrics,current_ckpt = \\\n",
        "          evaluation_loop(RUN_EVAL,\n",
        "                          RUN_PREDICTION,\n",
        "                          EVALUATE_WHILE_PREDICT,\n",
        "                          dataset,\n",
        "                          MODEL,\n",
        "                          total_metrics,\n",
        "                          MAX_SEQ_LENGTH,\n",
        "                          current_ORIG_DATA_FOLDER,\n",
        "                          BERT_GCS_DIR,\n",
        "                          USE_LATEST,\n",
        "                          CHECKPOINT_STEP,\n",
        "                          DATA_GCS_DIR,\n",
        "                          USING_SHARDS,\n",
        "                          START_SHARD,\n",
        "                          END_SHARD,\n",
        "                          USING_EX_DATA,\n",
        "                          EX_DATA_NUM,\n",
        "                          EVAL_WHILE_PREDICT_PREDICTIONS_DIR,\n",
        "                          PREDICTIONS_DIR,\n",
        "                          EVALUATIONS_DIR,\n",
        "                          CONFIG_FILE)\n",
        "              \n",
        "      \n",
        "      current_ckpts[M][m] = current_ckpt\n",
        "      if sucess:\n",
        "        sleeping = False\n",
        "  time.sleep(CHECK_MODEL_EVERY_N_SECS if sleeping else 0)\n",
        "  if not REPEAT_LOOP:\n",
        "    break\n",
        "if not REPEAT_LOOP:\n",
        "  for evals_dir,metrics in total_metrics.items():\n",
        "    print(\"Printing metrics for:\",evals_dir,\"\\n\")\n",
        "    for key,metric in metrics.items():\n",
        "      print(key+\":\",metric)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "  "
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "CLgXP-sV4zAd"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "name": "py37",
      "display_name": "Python 3.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}