{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-60rHUe5DT2r"
      },
      "source": [
        "#Finetuning Evaluation and Prediction Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhEhbQ08DXyG"
      },
      "source": [
        "This notebook evlauates and performs predictions on test data using finetuned models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "outputs": [],
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown In the case that an inference database is large and a long duration of continuous runtime is required, a GCP TPU/runtime to run this notebook may be desirable. If that's the case, specify here:\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "#@markdown How many TPU scores the TPU has: if using colab, NUM_TPU_CORES is 8.\n",
        "NUM_TPU_CORES = 8 #@param {type:\"number\"}\n",
        "#@markdown Which mode to use (a different mode means a different finetuning task): options are:\n",
        "#@markdown * \"MRPC\" - paired sequence method\n",
        "#@markdown * \"MRPC_w_ex_data\" - paired sequence method with external data\n",
        "#@markdown * \"RE\" - single sequence method\n",
        "#@markdown * \"NER\" - single sequence per residue prediction \n",
        "#@markdown \n",
        "#@markdown You can add more modes by creating a new processor and/or a new model_fn inside of the \"mutformer_model_code\" folder downloaded from github, then changing the corresponding code snippets in the code segment named \"Authorize for GCS, Imports, and General Setup\" (also edit the dropdown below).\n",
        "MODE = \"RE\" #@param   [\"MRPC_w_ex_data\", \"MRPC\", \"RE\", \"NER\"]   {type:\"string\"} \n",
        "                        ####      ^^^^^ dropdown list for all modes ^^^^^\n",
        "#@markdown Name of the GCS bucket to use:\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown Where the processed data was stored in GCS:\n",
        "PROCESSED_DATA_DIR = \"RE_finetune_update_loaded\" #@param {type:\"string\"}\n",
        "#@markdown What folder to write predictions into (location of this folder will either be GCS or google drive) (the PREDICTIONS_FOLDER variable can be the same across all finetuning notebooks):\n",
        "PREDICTIONS_FOLDER = \"MutFormer_updated_finetuning_predictions\" #@param {type:\"string\"}\n",
        "#@markdown What folder to write evaluation results into (location of this folder will either be GCS or google drive) EVALUATIONS_FOLDER variable can be the same across all finetuning notebooks):\n",
        "EVALUATIONS_FOLDER = \"MutFormer_updated_finetuning_eval_results\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLgXP-sV4zAd"
      },
      "source": [
        "#If running on a GCP runtime, follow these instructions to set it up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eT2L30n5Eiu"
      },
      "source": [
        "###1) Create a VM from the GCP website\n",
        "###2) Open a command prompt on your computer and perform the following steps\"\n",
        "To ssh into the VM, run:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Note: Make sure the port above matches the port below (in this case it's 8888)\n",
        "\\\n",
        "\\\n",
        "In the new command prompt that popped out, either run each of the commands below individually, or copy and paste the one liner below:\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "One command:\n",
        "```\n",
        "sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "###3) In this notebook, click the \"connect to local runtime\" option under the connect button, and copy and paste the link outputted by command prompt with \"locahost: ...\"\n",
        "###4) Finally, run this code segment, which creates a TPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOygMeIt5JqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52305eb0-3cba-4118-a1e3-5bab91ad6897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.alpha.compute.tpus.create) Error parsing [tpu].\n",
            "The [tpu] resource is not properly specified.\n",
            "Failed to find attribute [project]. The attribute can be set in the following ways: \n",
            "- provide the argument `--project` on the command line\n",
            "- set the property `core/project`\n",
            "\u001b[1;31mERROR:\u001b[0m (gcloud.alpha.compute.tpus.describe) Error parsing [tpu].\n",
            "The [tpu] resource is not properly specified.\n",
            "Failed to find attribute [project]. The attribute can be set in the following ways: \n",
            "- provide the argument `--project` on the command line\n",
            "- set the property `core/project`\n",
            "ServiceException: 401 Anonymous caller does not have storage.buckets.getIamPolicy access to the Google Cloud Storage bucket.\n"
          ]
        }
      ],
      "source": [
        "GCE_PROJECT_NAME = \"genome-project-319100\" #@param {type:\"string\"}\n",
        "TPU_ZONE = \"us-central1-f\" #@param {type:\"string\"}\n",
        "TPU_NAME = \"mutformer-tpu\" #@param {type:\"string\"}\n",
        "\n",
        "!gcloud alpha compute tpus create $TPU_NAME --accelerator-type=tpu-v2 --version=1.15.5 --zone=$TPU_ZONE ##create new TPU\n",
        "\n",
        "!gsutil iam ch serviceAccount:`gcloud alpha compute tpus describe $TPU_NAME | grep serviceAccount | cut -d' ' -f2`:admin gs://theodore_jiang && echo 'Successfully set permissions!' ##give TPU access to GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k0FGNhvN_Qa"
      },
      "source": [
        "#Clone the MutFormer repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDPM9ml0N-za",
        "outputId": "1fb1dc9a-71f7-4806-db78-a4ed4ef124c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mutformer'...\n",
            "remote: Enumerating objects: 614, done.\u001b[K\n",
            "remote: Counting objects: 100% (415/415), done.\u001b[K\n",
            "remote: Compressing objects: 100% (335/335), done.\u001b[K\n",
            "remote: Total 614 (delta 299), reused 111 (delta 78), pack-reused 199\u001b[K\n",
            "Receiving objects: 100% (614/614), 2.14 MiB | 12.64 MiB/s, done.\n",
            "Resolving deltas: 100% (410/410), done.\n"
          ]
        }
      ],
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git\n",
        "#@markdown Where to clone the repo into:\n",
        "REPO_DESTINATION_PATH = \"mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Authorize for GCS, Imports, and General Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S4CiOh3RzFW",
        "outputId": "45cd4baf-5c1c-4aeb-c182-3be19bbb5e3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Authorize for GCS:\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Authorize done\n",
            "WARNING:tensorflow:From /content/mutformer/optimization.py:82: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-08 01:16:27,854 - tensorflow - INFO - Using TPU runtime\n",
            "2022-01-08 01:16:27,857 - tensorflow - INFO - TPU address is grpc://10.94.154.10:8470\n"
          ]
        }
      ],
      "source": [
        "if not GCP_RUNTIME:\n",
        "  %tensorflow_version 1.x\n",
        "  from google.colab import auth\n",
        "  print(\"Authorize for GCS:\")\n",
        "  auth.authenticate_user()\n",
        "  print(\"Authorize done\")\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow.compat.v1 as tf\n",
        "import time\n",
        "import importlib\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "if REPO_DESTINATION_PATH == \"mutformer\":\n",
        "  if os.path.exists(\"mutformer_code\"):\n",
        "    shutil.rmtree(\"mutformer_code\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH,\"mutformer_code\")\n",
        "  REPO_DESTINATION_PATH = \"mutformer_code\"\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic  #### <<<<< if you added more modes, change these imports to import the correct processors, \n",
        "from mutformer.modeling import BertModel,BertModelModified                                        #### <<<<< correct training scripts (i.e. run_classifier and run_ner_for_pathogenic), and\n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor,MrpcWithExDataProcessor            #### <<<<< correct model classes\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor  \n",
        "\n",
        "##reload modules so that you don't need to restart the runtime to reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_classifier,\n",
        "                  run_ner_for_pathogenic]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "#@markdown ###### Whether or not to write logs to a file\n",
        "DO_FILE_LOGGING = False #@param {type:\"boolean\"}\n",
        "if DO_FILE_LOGGING:\n",
        "  #@markdown ###### If using file logging, what path to write logs to\n",
        "  FILE_LOGGING_PATH = 'file_logging/spam.log' #@param {type:\"string\"}\n",
        "  if not os.path.exists(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1])):\n",
        "    os.makedirs(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1]))\n",
        "  fh = logging.FileHandler(FILE_LOGGING_PATH)\n",
        "  fh.setLevel(logging.INFO)\n",
        "  fh.setFormatter(formatter)\n",
        "  log.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "if GCP_RUNTIME:\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_NAME, zone=TPU_ZONE, project=GCE_PROJECT_NAME)\n",
        "  TPU_ADDRESS = tpu_cluster_resolver.get_master()\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "      log.info('TPU address is ' + TPU_ADDRESS)\n",
        "      tf.contrib.cloud.configure_gcs(session)\n",
        "else:\n",
        "  if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    log.info(\"Using TPU runtime\")\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "    with tf.Session(TPU_ADDRESS) as session:\n",
        "      log.info('TPU address is ' + TPU_ADDRESS)\n",
        "      # Upload credentials to TPU.\n",
        "      with open('/content/adc.json', 'r') as f:\n",
        "        auth_info = json.load(f)\n",
        "      tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "      \n",
        "  else:\n",
        "    raise Exception('Not connected to TPU runtime, TPU required to run mutformer')\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\":      ####       vvvvv if you added more modes, change this part to set the processors and training scripts correctly vvvvv\n",
        "  processor = MrpcProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = False\n",
        "elif MODE==\"MRPC_w_ex_data\":\n",
        "  processor = MrpcWithExDataProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = True\n",
        "elif MODE==\"RE\":\n",
        "  processor = REProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = False\n",
        "elif MODE==\"NER\":\n",
        "  processor = NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "  USING_EX_DATA = False\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\",\\\"MRPC_w_ex_data\\\" \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()\n",
        "                      ####       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0TXw9GtCKz"
      },
      "source": [
        "#Specify location preferences for google drive vs GCS/Mount Drive if needed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYsYBUCJMTdz"
      },
      "outputs": [],
      "source": [
        "#@markdown ###### Note: For all of these, if using GCP_RUNTIME, all of these parameters must use GCS, because a GCP TPU can't access google drive\n",
        "#@markdown \\\n",
        "#@markdown If original data was stored in drive (this variable should match up with the \"INPUT_DATA_DIR\" variable in the data generation script), full drive path to the original data (for detecting the # of steps per epoch) (this variable is used to limit interaction with GCS; it can also be left blank and steps will be automatically detected from tfrecords stored in GCS):\n",
        "#@markdown * If GCP_RUNTIME, drive paths will not work, so steps detection will automatically default to tfrecords\n",
        "ORIG_DATA_FOLDER = \"\" #@param {type: \"string\"}\n",
        "DRIVE_PATH = \"/content/drive/My Drive\"\n",
        "#@markdown Whether to use GCS for writing predictions, if not, defaults to drive\n",
        "GCS_PREDICTIONS = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether to use GCS for writing eval results, if not, defaults to drive\n",
        "GCS_EVAL = True #@param {type:\"boolean\"}\n",
        "\n",
        "PREDS_PATH = BUCKET_PATH if GCS_PREDICTIONS else DRIVE_PATH\n",
        "EVALS_PATH = BUCKET_PATH if GCS_EVAL else DRIVE_PATH\n",
        "\n",
        "if GCP_RUNTIME:\n",
        "  FILES_PATH = BUCKET_PATH\n",
        "\n",
        "if (\"/content/drive\" in ORIG_DATA_FOLDER and not GCP_RUNTIME) or not GCS_PREDICTIONS or not GCS_EVAL:\n",
        "  from google.colab import drive,auth\n",
        "  !fusermount -u /content/drive\n",
        "  drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run Eval/prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MRlQ61PZ4fl"
      },
      "source": [
        "This following section will perform evaluation and prediction on either the eval dataset or the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvuxLWh5_Vm8"
      },
      "source": [
        "###General Setup and definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVe9VfxJ_Y38"
      },
      "outputs": [],
      "source": [
        "#@markdown When performing prediction, whether or not to ensure all datapoints are predicted via a trailing test dataset: (if so, make sure this option was also specified as True during data generation)\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown Maximum batch size the runtime can handle during prediction without OOM for all models being evaluated/tested: note that this value should match up with the variable \"MAX_BATCH_SIZE\" in the data generation script.\n",
        "MAX_BATCH_SIZE =  512 #@param {type:\"integer\"}\n",
        "\n",
        "def latest_checkpoint(dir):\n",
        "  cmd = \"gsutil ls \"+dir\n",
        "  files = !{cmd}\n",
        "  for file in files:\n",
        "    if \"model.ckpt\" in file:\n",
        "      return file.replace(\".\"+file.split(\".\")[-1],\"\")\n",
        "\n",
        "def write_metrics(metrics,dir):\n",
        "  tf.logging.info(\"writing metrics to \"+dir)\n",
        "  if os.path.exists(dir):\n",
        "    shutil.rmtree(dir)\n",
        "  os.makedirs(dir)\n",
        "  gs = metrics[\"global_step\"]\n",
        "  tf.logging.info(\"global step \"+str(gs))\n",
        "\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "  tf.reset_default_graph()\n",
        "  for key,value in metrics.items():\n",
        "    tf.logging.info(str(key)+\":\"+str(value))\n",
        "    x_scalar = tf.constant(value)\n",
        "    first_summary = tf.summary.scalar(name=key, tensor=x_scalar)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        writer = tf.summary.FileWriter(dir)\n",
        "        sess.run(init)\n",
        "        summary = sess.run(first_summary)\n",
        "        writer.add_summary(summary, gs)\n",
        "        writer.flush()\n",
        "        tf.logging.info(\"Done with writing the scalar summary\")\n",
        "    time.sleep(1)\n",
        "\n",
        "  if GCS_EVAL:\n",
        "    cmd = \"gsutil -m cp -r \\\"\"+dir+\"/.\\\" \\\"\"+EVALS_PATH+\"/\"+dir+\"\\\"\"\n",
        "    !{cmd}  \n",
        "  else:\n",
        "    if not os.path.exists(EVALS_PATH+\"/\"+dir):\n",
        "      os.makedirs(EVALS_PATH+\"/\"+dir)\n",
        "    shutil.copytree(dir,EVALS_PATH+\"/\"+dir)\n",
        "  \n",
        "\n",
        "def write_predictions(PREDICTIONS_DIR,\n",
        "                      result,\n",
        "                      result_trailing,\n",
        "                      shard_id=\"\"):\n",
        "  if not os.path.exists(PREDS_PATH+\"/\"+PREDICTIONS_DIR):\n",
        "    os.makedirs(PREDS_PATH+\"/\"+PREDICTIONS_DIR)\n",
        "  with tf.gfile.Open(PREDS_PATH+\"/\"+PREDICTIONS_DIR+\"/predictions\"+shard_id+\".txt\", \"w\") as writer:\n",
        "    tf.logging.info(\"***** Predict results *****\")\n",
        "    for (i, prediction) in enumerate(result):\n",
        "      output_line = \"\\t\".join([str(k)+\":\"+str(v) for k,v in prediction.items()]) + \"\\n\"\n",
        "      writer.write(output_line)\n",
        "    if result_trailing:\n",
        "      for (i, prediction) in enumerate(result_trailing):\n",
        "        output_line = \"\\t\".join([str(k)+\":\"+str(v) for k,v in prediction.items()]) + \"\\n\"\n",
        "        writer.write(output_line)\n",
        "\n",
        "\n",
        "def evaluation_loop(RUN_EVAL,\n",
        "                    RUN_PREDICTION,\n",
        "                    EVALUATE_WHILE_PREDICT,\n",
        "                    dataset,\n",
        "                    MODEL,\n",
        "                    total_metrics,\n",
        "                    MAX_SEQ_LENGTH,\n",
        "                    current_ORIG_DATA_FOLDER,\n",
        "                    BERT_GCS_DIR,\n",
        "                    USE_LATEST,\n",
        "                    CHECKPOINT_STEP,\n",
        "                    DATA_GCS_DIR,\n",
        "                    USING_SHARDS,\n",
        "                    START_SHARD,\n",
        "                    USING_EX_DATA,\n",
        "                    PRED_NUM,\n",
        "                    EVAL_WHILE_PREDICT_PREDICTIONS_DIR,\n",
        "                    PREDICTIONS_DIR,\n",
        "                    EVALUATIONS_DIR,\n",
        "                    CONFIG_FILE):\n",
        "\n",
        "  try: ##wrap everything in a giant try except so that any \n",
        "       ##glitches won't completely stop evaluation in the middle\n",
        "    current_ckpt = \"\"\n",
        "\n",
        "    tf.logging.info(\"Using data from: \"+DATA_GCS_DIR)\n",
        "    tf.logging.info(\"Loading model from: \"+BERT_GCS_DIR)\n",
        "\n",
        "    eval_file = os.path.join(DATA_GCS_DIR, evaluating_file)\n",
        "\n",
        "    def steps_getter(input_files):\n",
        "      tot_sequences = 0\n",
        "      for input_file in input_files:\n",
        "        tf.logging.info(\"reading:\"+input_file+\" for steps\")\n",
        "\n",
        "        d = tf.data.TFRecordDataset(input_file)\n",
        "\n",
        "        with tf.Session() as sess:\n",
        "          tot_sequences+=sess.run(d.reduce(0, lambda x,_: x+1))\n",
        "\n",
        "      return tot_sequences\n",
        "\n",
        "    if USING_SHARDS:\n",
        "      shards_folder = DATA_GCS_DIR\n",
        "      input_file = os.path.join(DATA_GCS_DIR, evaluating_file)\n",
        "      import re\n",
        "      file_name = input_file.split(\"/\")[-1]\n",
        "      shards = [shards_folder + \"/\" + file for file in tf.io.gfile.listdir(shards_folder) if\n",
        "                re.match(file_name + \"_\\d+\", file)]\n",
        "      shards = sorted(shards,key=lambda shard:int(shard.split(\"_\")[-1]))[START_SHARD:]\n",
        "    else:\n",
        "      shards = [eval_file]\n",
        "\n",
        "    if USING_SHARDS:\n",
        "      tf.logging.info(\"\\nUSING SHARDs:\")\n",
        "      for shard in shards:\n",
        "        tf.logging.info(shard)\n",
        "      tf.logging.info(\"\\n\")\n",
        "\n",
        "    if RUN_EVAL:   \n",
        "      try:\n",
        "        if dataset==\"dev\":\n",
        "          data_path_eval = \"/content/drive/My Drive/\"+current_ORIG_DATA_FOLDER+\"/dev.tsv\"\n",
        "        else:\n",
        "          data_path_eval = \"/content/drive/My Drive/\"+current_ORIG_DATA_FOLDER+\"/test.tsv\"\n",
        "        lines = open(data_path_eval).read().split(\"\\n\")\n",
        "        EVAL_STEPS = int(len(lines)/EVAL_BATCH_SIZE)\n",
        "      except Exception:\n",
        "        SEQUENCES_PER_EPOCH = steps_getter(shards)\n",
        "        EVAL_STEPS = int(SEQUENCES_PER_EPOCH/EVAL_BATCH_SIZE)\n",
        "      else: ##dataset==\"test\"\n",
        "        SEQUENCES_PER_EPOCH = steps_getter(shards)\n",
        "        EVAL_STEPS = int(SEQUENCES_PER_EPOCH/EVAL_BATCH_SIZE)\n",
        "\n",
        "      tf.logging.info(\"eval steps:\"+str(EVAL_STEPS))\n",
        "\n",
        "    \n",
        "    if EVALUATE_WHILE_PREDICT:\n",
        "      cmd = \"gsutil -m rm -r \"+EVAL_WHILE_PREDICT_PREDICTIONS_DIR\n",
        "      !{cmd}\n",
        "    if USE_LATEST:\n",
        "      try:\n",
        "        RESTORE_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "      except Exception:\n",
        "        RESTORE_CHECKPOINT = latest_checkpoint(BERT_GCS_DIR)\n",
        "    else:\n",
        "      try:\n",
        "        latest_ckpt = tf.train.latest_checkpoint(BERT_GCS_DIR).split(\"/\")[-1]\n",
        "        RESTORE_CHECKPOINT = [\".\".join(ckpt.split(\".\")[:-1]) \n",
        "                              for ckpt in tf.io.gfile.listdir(BERT_GCS_DIR) \n",
        "                              if len(ckpt.split(\".\"))==3 and str(CHECKPOINT_STEP) == ckpt.split(\".\")[-2].split(\"-\")[-1]][0]\n",
        "        old_file_lines = tf.gfile.Open(BERT_GCS_DIR+\"/checkpoint\").read().split(\"\\n\")\n",
        "        new_file_lines = old_file_lines.copy()\n",
        "        new_file_lines[0] = new_file_lines[0].replace(latest_ckpt,RESTORE_CHECKPOINT)\n",
        "        RESTORE_CHECKPOINT = BERT_GCS_DIR+\"/\"+RESTORE_CHECKPOINT\n",
        "\n",
        "        tf.gfile.Open(BERT_GCS_DIR+\"/checkpoint\",\"w+\").write(\"\\n\".join(new_file_lines))\n",
        "      except Exception as e:\n",
        "        tf.logging.info(\"\\n\\nCould not find the checkpoint specified. Error:\"+str(e)+\". Skipping...\\n\\n\")\n",
        "        return False,total_metrics,current_ckpt\n",
        "    try:\n",
        "      current_ckpt=RESTORE_CHECKPOINT\n",
        "      tf.logging.info(\"USING CHECKPOINT:\"+RESTORE_CHECKPOINT)\n",
        "    except Exception:\n",
        "      raise Exception(\"No checkpoints were found in the given location\")\n",
        "    config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "\n",
        "    model_fn = script.model_fn_builder(\n",
        "        bert_config=config,\n",
        "        num_labels=len(label_list),\n",
        "        init_checkpoint=None,\n",
        "        restore_checkpoint=RESTORE_CHECKPOINT,\n",
        "        init_learning_rate=0,\n",
        "        decay_per_step=0,\n",
        "        num_warmup_steps=10,\n",
        "        use_tpu=True,\n",
        "        use_one_hot_embeddings=True,\n",
        "        bert=MODEL,\n",
        "        test_results_dir=EVAL_WHILE_PREDICT_PREDICTIONS_DIR,\n",
        "        yield_predictions=EVALUATE_WHILE_PREDICT,\n",
        "        using_ex_data=USING_EX_DATA)\n",
        "\n",
        "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "    run_config = tf.contrib.tpu.RunConfig(\n",
        "        cluster=tpu_cluster_resolver,\n",
        "        model_dir=BERT_GCS_DIR,\n",
        "        tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "            num_shards=NUM_TPU_CORES,\n",
        "            per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "    estimator = tf.contrib.tpu.TPUEstimator(\n",
        "        use_tpu=True,\n",
        "        model_fn=model_fn,\n",
        "        config=run_config,\n",
        "        train_batch_size=1,\n",
        "        eval_batch_size=EVAL_BATCH_SIZE,\n",
        "        predict_batch_size=MAX_BATCH_SIZE)\n",
        "      \n",
        "    tf.logging.info(\"USING FILE:\"+eval_file)\n",
        "    for n,shard in enumerate(shards):\n",
        "      input_fn = script.file_based_input_fn_builder(\n",
        "            input_file=shard,\n",
        "            seq_length=MAX_SEQ_LENGTH,\n",
        "            is_training=False,\n",
        "            drop_remainder=True,\n",
        "            pred_num=PRED_NUM if USING_EX_DATA else None)\n",
        "\n",
        "\n",
        "      tf.logging.info(\"***** Running evaluation/prediction *****\")\n",
        "      tf.logging.info(\" Eval Batch size = \"+str(EVAL_BATCH_SIZE))\n",
        "      tf.logging.info(\" Predict Batch size = \"+str(MAX_BATCH_SIZE))\n",
        "      \n",
        "    \n",
        "      if RUN_EVAL:\n",
        "        eval_metrics = estimator.evaluate(input_fn=input_fn, steps=EVAL_STEPS)\n",
        "        tf.logging.info(\"\\n\\n\\n\\n\\n\\nEVAL METRICS:\")\n",
        "        for k,v in eval_metrics.items():\n",
        "          tf.logging.info(k+\":\"+str(v))\n",
        "        tf.logging.info(\"\\n\\n\\n\\n\\n\\n\\n\")\n",
        "        write_metrics(eval_metrics,EVALUATIONS_DIR)\n",
        "        if not REPEAT_LOOP:\n",
        "          total_metrics[EVALUATIONS_DIR] = eval_metrics\n",
        "      if RUN_PREDICTION:\n",
        "        result=estimator.predict(input_fn=input_fn)\n",
        "        if PRECISE_TESTING and n==len(shards)-1:\n",
        "          run_config_trailing = tf.contrib.tpu.RunConfig(\n",
        "            cluster=tpu_cluster_resolver,\n",
        "            model_dir=BERT_GCS_DIR,\n",
        "            tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "                num_shards=1,\n",
        "                per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "          estimator_trailing = tf.contrib.tpu.TPUEstimator(\n",
        "              use_tpu=True,\n",
        "              model_fn=model_fn,\n",
        "              config=run_config_trailing,\n",
        "              train_batch_size=1,\n",
        "              predict_batch_size=1)\n",
        "          test_file_trailing = os.path.join(DATA_GCS_DIR, \"test_trailing.tf_record\")\n",
        "          test_input_fn_trailing = script.file_based_input_fn_builder(\n",
        "              input_file=test_file_trailing,\n",
        "              seq_length=MAX_SEQ_LENGTH,\n",
        "              is_training=False,\n",
        "              drop_remainder=True,\n",
        "              pred_num=PRED_NUM if USING_EX_DATA else None)\n",
        "          result_trailing=estimator_trailing.predict(input_fn=test_input_fn_trailing)\n",
        "        else:\n",
        "          result_trailing = None\n",
        "        write_predictions(PREDICTIONS_DIR,\n",
        "                          result,\n",
        "                          result_trailing,\n",
        "                          shard_id=str(START_SHARD+n)if USING_SHARDS else \"\")\n",
        "    if not USE_LATEST:\n",
        "      tf.gfile.Open(BERT_GCS_DIR+\"/checkpoint\",\"w+\").write(\"\\n\".join(old_file_lines))\n",
        "    return True,total_metrics,current_ckpt\n",
        "  except Exception as e:\n",
        "    tf.logging.info(\"\\n\\nFAILED-error:\"+str(e)+\". Skipping...\\n\\n\")\n",
        "    return False,total_metrics,current_ckpt\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM7HXcKbZvn-"
      },
      "source": [
        "Following are three different code segments to run:\n",
        "1. For if you benchmarked model/sequence length during finetuning and wish to evaluate each model \\\n",
        "2. For if you benchmarked sequence length/batch size during finetuning and wish to evaluate each model \\\n",
        "3. For only evaluating/predicting using a single model\n",
        "\n",
        "Choose a desired code segment to run, select the desired options for evaluating/predicting, and run that code segment\n",
        "\\\n",
        "\\\n",
        "Note: All evaluation results will be written into the previously specified logging directory either under google drive or GCS, depending on the values of GCS_COMS, GCS_PREDICTIONS, and GCS_EVAL specified before. To view the results, use the colab notebook titled \"mutformer processing and viewing finetuning results,\" which can also be used to view prediction results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgI2CyoFPjyY"
      },
      "source": [
        "###Model/Sequence Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "outputs": [],
      "source": [
        "#@markdown ### IO config\n",
        "#@markdown Folder for where to load the finetuned model from\n",
        "FINETUNED_MODEL_DIR = \"bert_model_re_mn_sl_try8\" #@param {type:\"string\"}\n",
        "#@markdown Which folder inside of PREDICTIONS_DIR and EVALUATIONS_DIR to write predictions and evaluations, respectively, into:\n",
        "RUN_NAME = \"RE_updated_mn_sl_try8\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ### Evaluation/prediction procedure config\n",
        "#@markdown The evaluation loop will loop through a list of models and a list of sequence lengths, attempting to evaluate a finetuned model for each combination of pretrained model and sequence length (failed combinations will be skipped).\n",
        "#@markdown * List of pretrained models that were used for finetuning (should indicate the names of the model folders inside INIT_MODEL_DIR from the finetuning training script):\n",
        "MODELS = [\"MutBERT10L\",\"MutBERT8L\",\"MutFormer8L\"] #@param\n",
        "#@markdown * List of model architectures for each model in the \"MODELS\" list defined in the entry above: each position in this list must correctly indicate the model architecture of its corresponding model folder in the list \"MODELS\" (BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture).\n",
        "MODEL_ARCHITECTURES = [BertModel,BertModel,BertModelModified] #@param\n",
        "#@markdown * List of sequence lengthed models to test\n",
        "MAX_SEQ_LENGTHS = [1024,512,256,128,64] #@param\n",
        "#@markdown Whether to evaluate on the test set or the dev set (\"test\" or \"dev\")\n",
        "dataset = \"test\" #@param{type:\"string\"}\n",
        "#@markdown Whether or not to run evaluation\n",
        "RUN_EVAL = False #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to run prediction (in a seperate loop from evaluation; EVALUATE_WHILE_PREDICT will override this value to False)\n",
        "RUN_PREDICTION = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to repeat this operation in a loop (if performing parallel evaluation operation, set to True, False otherwise)\n",
        "#@markdown * If using REPEAT_LOOP, to prevent the script from evaluating every single model trained on every single combination of batch size and sequence length every loop, the script will only evaluate models that are being currently trained (the script will only evaluate on the model folders that have seen a new latest checkpoint since the script started running).\n",
        "REPEAT_LOOP = False #@param {type:\"boolean\"}\n",
        "#@markdown When using REPEAT_LOOP, how long to wait in between each loop before checking again for updated train progress:\n",
        "CHECK_MODEL_EVERY_N_SECS =  150#@param {type:\"integer\"}\n",
        "#@markdown If evaluating, whether or not to evaluate and predict results in the same loop; useful when amount of test data is very small and the time it takes to restart a loop is significant (if yes, prediction results will be written in the form of tfevent files into GCS that need to be viewed using the notebook titled \"mutformer processing and viewing finetuning results\")\n",
        "#@markdown \n",
        "#@markdown Note: If using EVALUATE_WHILE_PREDICT, prediction results must be read using the previously mentioned colab notebook, otherwise, predictions will be written directly as txts and will be directly accessible from google drive under the folder specified above\n",
        "EVALUATE_WHILE_PREDICT =  False #@param {type:\"boolean\"}\n",
        "#@markdown What batch size to use during evaluation (larger batch size will increase evaluation speed but may skip more datapoints)\n",
        "EVAL_BATCH_SIZE = 64 #@param {type:\"integer\"}\n",
        "#@markdown Whether or not testing/evaluating data was generated in shards\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown * If using shards, which shard index to start at (defualt 0 for first shard) (script will not delete older predictions, only continue generating predictions starting with this position):\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Whether to use the latest checkpoint in the folder (set to false if an intermediate checkpoint should be used)\n",
        "USE_LATEST = True #@param {type:\"boolean\"}\n",
        "#@markdown * If not using latest checkpoint, which step's checkpoint to use\n",
        "CHECKPOINT_STEP = 4000 #@param {type:\"integer\"}\n",
        "\n",
        "total_metrics = {}  ## a dictionary for all metrics to  \n",
        "                    ## print at the end during testing, \n",
        "                    ## not necessary during evaluation   \n",
        "if dataset==\"test\":\n",
        "  evaluating_file = \"test.tf_record\"\n",
        "elif dataset==\"dev\":\n",
        "  evaluating_file = \"eval.tf_record\"\n",
        "else:\n",
        "  raise Exception(\"only datasets supported are dev and test\")\n",
        "\n",
        "DATA_INFOS = [[\"N/A\" for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS]   ##create an empty 2D list to store all\n",
        "              for MODEL_NAME in MODELS]                  ##the data info dictionaries\n",
        "\n",
        "current_ckpts = [[\"N/A\" for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS] for MODEL_NAME in MODELS]\n",
        "for M,MODEL_NAME in enumerate(MODELS):\n",
        "  for m,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "        BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "        try:\n",
        "          current_ckpts[M][m] = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "        except:\n",
        "          try:\n",
        "            current_ckpts[M][m] = latest_checkpoint(BERT_GCS_DIR)\n",
        "          except:\n",
        "            raise Exception(\"could not find any checkpoints in the model dir specified\")\n",
        "\n",
        "def get_new_ckpts(current_ckpts):\n",
        "  new_ckpts = []\n",
        "  for M,MODEL_NAME in enumerate(MODELS):\n",
        "    for m,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "          BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "          try:\n",
        "            current_ckpt = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "            if current_ckpts[M][m]!=current_ckpt:\n",
        "              new_ckpts.append([M,m])\n",
        "          except:\n",
        "            try:\n",
        "              current_ckpt = latest_checkpoint(BERT_GCS_DIR)\n",
        "              if current_ckpts[M][m]!=current_ckpt:\n",
        "                new_ckpts.append([M,m])\n",
        "            except:\n",
        "              raise Exception(\"could not find any checkpoints in the model dir specified\")\n",
        "  return new_ckpts\n",
        "\n",
        "while True:\n",
        "  sleeping = True   ##to prevent excessive interaction with GCS, \n",
        "                    ##if an eval/pred loop fails, the script \n",
        "                    ##will wait for a while before trying again\n",
        "\n",
        "  if REPEAT_LOOP:                             ##if using REPEAT_LOOP, only evaluate on new checkpoints\n",
        "    new_ckpts = get_new_ckpts(current_ckpts)\n",
        "    if len(new_ckpts) == 0:\n",
        "      tf.logging.info(\"No new checkpoints have been written since script start/last evaluation. Trying again in another \"+str(CHECK_MODEL_EVERY_N_SECS)+\" seconds.\")\n",
        "\n",
        "  for M,MODEL_NAME in enumerate(MODELS):\n",
        "    for m,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "\n",
        "      if REPEAT_LOOP:\n",
        "        if [M,m] not in new_ckpts:\n",
        "          continue\n",
        "\n",
        "      tf.logging.info(\"\\n\\n\\nMODEL NAME:\"+MODEL_NAME+\n",
        "            \"\\nINPUT MAX SEQ LENGTH:\"+str(MAX_SEQ_LENGTH))\n",
        "      \n",
        "      MODEL = MODEL_ARCHITECTURES[M]\n",
        "      current_ORIG_DATA_FOLDER= ORIG_DATA_FOLDER+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "      BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "      DATA_GCS_DIR = BUCKET_PATH+\"/\"+PROCESSED_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "          \n",
        "      EVAL_WHILE_PREDICT_PREDICTIONS_DIR = BUCKET_PATH+\"/\"+PREDICTIONS_FOLDER+\"/\"+RUN_NAME+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "      EVALUATIONS_DIR = EVALUATIONS_FOLDER+\"/\"+RUN_NAME+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "      PREDICTIONS_DIR = PREDICTIONS_FOLDER+\"/\"+RUN_NAME+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "      CONFIG_FILE = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)+\"/config.json\"\n",
        "      \n",
        "      if DATA_INFOS[M][m] == \"N/A\":\n",
        "        DATA_INFOS[M][m] = json.load(tf.gfile.Open(DATA_GCS_DIR+\"/info.json\"))\n",
        "      \n",
        "      EX_DATA_NUM = DATA_INFOS[M][m][\"ex_data_num\"] if USING_EX_DATA else 0\n",
        "\n",
        "\n",
        "      ##run the evaluation/prediction loop\n",
        "      sucess,total_metrics,current_ckpt = \\\n",
        "              evaluation_loop(RUN_EVAL,\n",
        "                              RUN_PREDICTION,\n",
        "                              EVALUATE_WHILE_PREDICT,\n",
        "                              dataset,\n",
        "                              MODEL,\n",
        "                              total_metrics,\n",
        "                              MAX_SEQ_LENGTH,\n",
        "                              current_ORIG_DATA_FOLDER,\n",
        "                              BERT_GCS_DIR,\n",
        "                              USE_LATEST,\n",
        "                              CHECKPOINT_STEP,\n",
        "                              DATA_GCS_DIR,\n",
        "                              USING_SHARDS,\n",
        "                              START_SHARD,\n",
        "                              USING_EX_DATA,\n",
        "                              EX_DATA_NUM,\n",
        "                              EVAL_WHILE_PREDICT_PREDICTIONS_DIR,\n",
        "                              PREDICTIONS_DIR,\n",
        "                              EVALUATIONS_DIR,\n",
        "                              CONFIG_FILE)\n",
        "\n",
        "      current_ckpts[M][m] = current_ckpt\n",
        "      if sucess:\n",
        "        sleeping = False\n",
        "  time.sleep(CHECK_MODEL_EVERY_N_SECS if sleeping else 0)\n",
        "  if not REPEAT_LOOP:\n",
        "    break\n",
        "if not REPEAT_LOOP and RUN_EVAL:\n",
        "  tf.logging.info(\"Printing all metrics...\\n\\n\")\n",
        "  for evals_dir,metrics in total_metrics.items():\n",
        "    tf.logging.info(\"Printing metrics for:\"+evals_dir+\"\\n\")\n",
        "    for key,metric in metrics.items():\n",
        "      tf.logging.info(key+\":\"+str(metric))\n",
        "    tf.logging.info(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw3EPxIe98Lg"
      },
      "source": [
        "###Batch Size/Sequence Length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN8Fpeb-96nA"
      },
      "outputs": [],
      "source": [
        "#@markdown ### IO config\n",
        "#@markdown Folder for where to load the finetuned model from\n",
        "FINETUNED_MODEL_DIR = \"bert_model_mrpc_adding_preds_only_others_xxx\" #@param {type:\"string\"}\n",
        "#@markdown Name of the folder to the finetuned model to load from inside FINETUNED_MODEL_DIR\n",
        "MODEL_NAME=\"bert_model_modified_large\" #@param {type:\"string\"}\n",
        "#@markdown Model architecture to use BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture\n",
        "MODEL_ARCHITECTURE = BertModelModified #@param\n",
        "#@markdown What folder to write predictions into (location of this folder will either be GCS or google drive)\n",
        "PREDICTIONS_FOLDER = \"added_preds_only_others\" #@param {type:\"string\"}\n",
        "#@markdown What folder to write evaluation results into (location of this folder will either be GCS or google drive)\n",
        "EVALUATIONS_FOLDER = \"added_preds_only_others\" #@param {type:\"string\"}\n",
        "#@markdown Which folder inside of PREDICTIONS_DIR and EVALUATIONS_DIR to write predictions and evaluations, respectively, into:\n",
        "RUN_NAME = \"MRPC_adding_preds_w_mutformer12L\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ### Evaluation/prediction procedure config\n",
        "#@markdown The evaluation loop will loop through a list of batch sizes and a list of sequence lengths, attempting to evaluate a finetuned model for each combination of batch size and sequence length (failed combinations will be skipped).\n",
        "#@markdown * List of batch sized models to test\n",
        "BATCH_SIZES = [32,16,64] #@param\n",
        "#@markdown * List of sequence lengthed models to test\n",
        "MAX_SEQ_LENGTHS = [256,512,1024] #@param\n",
        "#@markdown Whether to evaluate on the test set or the dev set (\"test\" or \"dev\")\n",
        "dataset = \"test\" #@param{type:\"string\"}\n",
        "#@markdown Whether or not to run evaluation\n",
        "RUN_EVAL = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to run prediction (in a seperate loop from evaluation; EVALUATE_WHILE_PREDICT will override this value to False)\n",
        "RUN_PREDICTION = False #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to repeat this operation in a loop (if performing parallel evaluation operation, set to True, False otherwise)\n",
        "#@markdown * If using REPEAT_LOOP, to prevent the script from evaluating every single model trained on every single combination of batch size and sequence length every loop, the script will only evaluate models that are being currently trained (the script will only evaluate on the model folders that have seen a new latest checkpoint since the script started running).\n",
        "REPEAT_LOOP = True #@param {type:\"boolean\"}\n",
        "#@markdown When using REPEAT_LOOP, how long to wait in between each loop before checking again for updated train progress:\n",
        "CHECK_MODEL_EVERY_N_SECS =  300#@param {type:\"integer\"}\n",
        "#@markdown If evaluating, whether or not to evaluate and predict results in the same loop; useful when amount of test data is very small and the time it takes to restart a loop is significant (if yes, prediction results will be written in the form of tfevent files into GCS that need to be viewed using the notebook titled \"mutformer processing and viewing finetuning results\")\n",
        "#@markdown \n",
        "#@markdown Note: If using EVALUATE_WHILE_PREDICT, prediction results must be read using the previously mentioned colab notebook, otherwise, predictions will be written directly as txts and will be directly accessible from google drive under the folder specified above\n",
        "EVALUATE_WHILE_PREDICT =  False #@param {type:\"boolean\"}\n",
        "#@markdown What batch size to use during evaluation (larger batch size will increase evaluation speed but may skip more datapoints)\n",
        "EVAL_BATCH_SIZE = 64 #@param {type:\"integer\"}\n",
        "#@markdown Whether or not testing/evaluating data was generated in shards\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown * If using shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Whether to use the latest checkpoint in the folder (set to false if an intermediate checkpoint should be used)\n",
        "USE_LATEST = False #@param {type:\"boolean\"}\n",
        "#@markdown * If not using latest checkpoint, which step's checkpoint to use\n",
        "CHECKPOINT_STEP = 7000 #@param {type:\"integer\"}\n",
        "\n",
        "total_metrics = {}  ## a dictionary for all metrics to  \n",
        "                    ## print at the end during testing, \n",
        "                    ## not necessary during evaluation   \n",
        "if dataset==\"test\":                  \n",
        "  evaluating_file = \"test.tf_record\" \n",
        "elif dataset==\"dev\":                \n",
        "  evaluating_file = \"eval.tf_record\" \n",
        "else:\n",
        "  raise Exception(\"only datasets supported are dev and test\")\n",
        "\n",
        "\n",
        "DATA_INFOS = [[\"N/A\" for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS]   ##create an empty 2D list to store all\n",
        "              for BATCH_SIZE in BATCH_SIZES]                  ##the data info dictionaries\n",
        "\n",
        "current_ckpts = [[\"N/A\" for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS] for BATCH_SIZE in BATCH_SIZES]\n",
        "for B,BATCH_SIZE in enumerate(BATCH_SIZES):\n",
        "    for M,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "        BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "        try:\n",
        "          current_ckpts[M][m] = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "        except:\n",
        "          try:\n",
        "            current_ckpts[M][m] = latest_checkpoint(BERT_GCS_DIR)\n",
        "          except:\n",
        "            raise Exception(\"could not find any checkpoints in the model dir specified\")\n",
        "\n",
        "def get_new_ckpts(current_ckpts):\n",
        "  new_ckpts = []\n",
        "  for M,MODEL_NAME in enumerate(MODELS):\n",
        "    for m,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "          BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "          try:\n",
        "            current_ckpt = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "            if current_ckpts[M][m]!=current_ckpt:\n",
        "              new_ckpts.append([M,m])\n",
        "          except:\n",
        "            try:\n",
        "              current_ckpt = latest_checkpoint(BERT_GCS_DIR)\n",
        "              if current_ckpts[M][m]!=current_ckpt:\n",
        "                new_ckpts.append([M,m])\n",
        "            except:\n",
        "              raise Exception(\"could not find any checkpoints in the model dir specified\")\n",
        "  return new_ckpts\n",
        "\n",
        "while True:\n",
        "  sleeping = True ##to prevent excessive interaction with GCS, \n",
        "                   ##if an eval/pred loop fails, the script \n",
        "                   ##will wait for a while before trying again\n",
        "\n",
        "  if REPEAT_LOOP:                             ##if using REPEAT_LOOP, only evaluate on new checkpoints\n",
        "    new_ckpts = get_new_ckpts(current_ckpts)\n",
        "    if len(new_ckpts) == 0:\n",
        "      tf.logging.info(\"No new checkpoints have been written since script start/last evaluation. Trying again in another \"+str(CHECK_MODEL_EVERY_N_SECS)+\" seconds.\")\n",
        "\n",
        "  for B,BATCH_SIZE in enumerate(BATCH_SIZES):\n",
        "    for M,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "\n",
        "      if REPEAT_LOOP:\n",
        "        if [M,m] not in new_ckpts:\n",
        "          continue\n",
        "\n",
        "      tf.logging.info(\"\\n\\n\\nBATCH SIZE:\"+str(BATCH_SIZE)+\n",
        "          \"\\nINPUT MAX SEQ LENGTH:\"+str(MAX_SEQ_LENGTH))\n",
        "\n",
        "      MODEL = MODEL_ARCHITECTURE\n",
        "      current_ORIG_DATA_FOLDER= ORIG_DATA_FOLDER+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "      BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/bs_\"+str(BATCH_SIZE)+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "      DATA_GCS_DIR = BUCKET_PATH+\"/\"+PROCESSED_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "          \n",
        "      EVAL_WHILE_PREDICT_PREDICTIONS_DIR = BUCKET_PATH+\"/\"+PREDICTIONS_FOLDER+\"/\"+RUN_NAME+\"/bs_\"+str(BATCH_SIZE)+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "      EVALUATIONS_DIR = EVALUATIONS_FOLDER+\"/\"+RUN_NAME+\"/bs_\"+str(BATCH_SIZE)+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "      PREDICTIONS_DIR = PREDICTIONS_FOLDER+\"/\"+RUN_NAME+\"/bs_\"+str(BATCH_SIZE)+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "      CONFIG_FILE = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/bs_\"+str(BATCH_SIZE)+\"_sl_\"+str(MAX_SEQ_LENGTH)+\"/config.json\"\n",
        "      \n",
        "      if DATA_INFOS[B][M] == \"N/A\":\n",
        "        DATA_INFOS[B][M] = json.load(tf.gfile.Open(DATA_GCS_DIR+\"/info.json\"))\n",
        "      \n",
        "      EX_DATA_NUM = DATA_INFOS[M][m][\"ex_data_num\"] if USING_EX_DATA else 0\n",
        "\n",
        "      ##run the evaluation/prediction loop\n",
        "      sucess,total_metrics,current_ckpt = \\\n",
        "              evaluation_loop(RUN_EVAL,\n",
        "                              RUN_PREDICTION,\n",
        "                              EVALUATE_WHILE_PREDICT,\n",
        "                              dataset,\n",
        "                              MODEL,\n",
        "                              total_metrics,\n",
        "                              MAX_SEQ_LENGTH,\n",
        "                              current_ORIG_DATA_FOLDER,\n",
        "                              BERT_GCS_DIR,\n",
        "                              USE_LATEST,\n",
        "                              CHECKPOINT_STEP,\n",
        "                              DATA_GCS_DIR,\n",
        "                              USING_SHARDS,\n",
        "                              START_SHARD,\n",
        "                              USING_EX_DATA,\n",
        "                              EX_DATA_NUM,\n",
        "                              EVAL_WHILE_PREDICT_PREDICTIONS_DIR,\n",
        "                              PREDICTIONS_DIR,\n",
        "                              EVALUATIONS_DIR,\n",
        "                              CONFIG_FILE)\n",
        "        \n",
        "      current_ckpts[B][M] = current_ckpt\n",
        "      if sucess:\n",
        "        sleeping = False\n",
        "  time.sleep(CHECK_MODEL_EVERY_N_SECS if sleeping else 0)\n",
        "  if not REPEAT_LOOP:\n",
        "    break\n",
        "if not REPEAT_LOOP and RUN_EVAL:\n",
        "  tf.logging.info(\"Printing all metrics...\\n\\n\")\n",
        "  for evals_dir,metrics in total_metrics.items():\n",
        "    tf.logging.info(\"Printing metrics for:\"+evals_dir+\"\\n\")\n",
        "    for key,metric in metrics.items():\n",
        "      tf.logging.info(key+\":\"+str(metric))\n",
        "    tf.logging.info(\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3RKscL93Fw2"
      },
      "source": [
        "###Just one model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pfcXh3N93JdG"
      },
      "outputs": [],
      "source": [
        "#@markdown ### IO config\n",
        "#@markdown Folder for where to load the finetuned model from\n",
        "FINETUNED_MODEL_DIR = \"bert_model_mrpc_all_preds_12L_try7\" #@param {type:\"string\"}\n",
        "#@markdown Model architecture to use BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture\n",
        "MODEL_ARCHITECTURE = BertModelModified #@param\n",
        "#@markdown Which folder inside of PREDICTIONS_DIR and EVALUATIONS_DIR to write predictions and evaluations, respectively, into:\n",
        "RUN_NAME = \"MRPC_all_preds_12L_try7\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ### Evaluation/prediction procedure config\n",
        "#@markdown Whether to evaluate on the test set or the dev set (\"test\" or \"dev\")\n",
        "dataset = \"test\" #@param{type:\"string\"}\n",
        "#@markdown Whether or not to run evaluation\n",
        "RUN_EVAL = False #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to run prediction (in a seperate loop from evaluation; EVALUATE_WHILE_PREDICT will override this value to False)\n",
        "RUN_PREDICTION = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not to repeat this operation in a loop (if performing parallel evaluation operation, set to True, False otherwise)\n",
        "REPEAT_LOOP = False #@param {type:\"boolean\"}\n",
        "#@markdown When using REPEAT_LOOP, how long to wait in between each loop before checking again for updated train progress:\n",
        "CHECK_MODEL_EVERY_N_SECS =  20#@param {type:\"integer\"}\n",
        "#@markdown If evaluating, whether or not to evaluate and predict results in the same loop; useful when amount of test data is very small and the time it takes to restart a loop is significant (if yes, prediction results will be written in the form of tfevent files into GCS that need to be viewed using the notebook titled \"mutformer processing and viewing finetuning results\")\n",
        "#@markdown \n",
        "#@markdown Note: If using EVALUATE_WHILE_PREDICT, prediction results must be read using the previously mentioned colab notebook, otherwise, predictions will be written directly as txts and will be directly accessible from google drive under the folder specified above\n",
        "EVALUATE_WHILE_PREDICT =  False #@param {type:\"boolean\"}\n",
        "#@markdown What batch size to use during evaluation (larger batch size will increase evaluation speed but may skip more datapoints)\n",
        "EVAL_BATCH_SIZE = 64 #@param {type:\"integer\"}\n",
        "#@markdown What sequence length to use\n",
        "MAX_SEQ_LENGTH =  512#@param {type:\"integer\"}\n",
        "#@markdown Whether or not testing/evaluating data was generated in shards\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown * If using shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Whether to use the latest checkpoint in the folder (set to false if an intermediate checkpoint should be used)\n",
        "USE_LATEST = True #@param {type:\"boolean\"}\n",
        "#@markdown * If not using latest checkpoint, which step's checkpoint to use\n",
        "CHECKPOINT_STEP = 10000 #@param {type:\"integer\"}\n",
        "\n",
        "total_metrics = {}  ## a dictionary for all metrics to  \n",
        "                    ## print at the end during testing, \n",
        "                    ## not necessary during evaluation   \n",
        "if dataset==\"test\":\n",
        "  evaluating_file = \"test.tf_record\"\n",
        "elif dataset==\"dev\":\n",
        "  evaluating_file = \"eval.tf_record\"\n",
        "else:\n",
        "  raise Exception(\"only datasets supported are dev and test\")\n",
        "\n",
        "BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR\n",
        "try:\n",
        "  current_ckpt = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "except:\n",
        "  try:\n",
        "    current_ckpt = latest_checkpoint(BERT_GCS_DIR)\n",
        "  except:\n",
        "    raise Exception(\"could not find any checkpoints in the model dir specified\")\n",
        "\n",
        "def get_new_ckpt(current_ckpt):\n",
        "  BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR\n",
        "  try:\n",
        "    new_ckpt = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "  except:\n",
        "    try:\n",
        "      new_ckpt = latest_checkpoint(BERT_GCS_DIR)\n",
        "    except:\n",
        "      raise Exception(\"could not find any checkpoints in the model dir specified\")\n",
        "  return not (new_ckpt==current_ckpt)\n",
        "\n",
        "\n",
        "while True:\n",
        "  if REPEAT_LOOP:\n",
        "    if not get_new_ckpt(current_ckpt):\n",
        "      tf.logging.info(\"No new checkpoints have been written since script start/last evaluation. Trying again in another \"+str(CHECK_MODEL_EVERY_N_SECS)+\" seconds.\")\n",
        "      time.sleep(CHECK_MODEL_EVERY_N_SECS)\n",
        "      continue  \n",
        "  MODEL = MODEL_ARCHITECTURE\n",
        "  current_ORIG_DATA_FOLDER = ORIG_DATA_FOLDER+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "  BERT_GCS_DIR = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR\n",
        "  DATA_GCS_DIR = BUCKET_PATH+\"/\"+PROCESSED_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "  EVAL_WHILE_PREDICT_PREDICTIONS_DIR = BUCKET_PATH+\"/\"+PREDICTIONS_FOLDER+\"/\"+RUN_NAME\n",
        "  EVALUATIONS_DIR = EVALUATIONS_FOLDER+\"/\"+RUN_NAME\n",
        "  PREDICTIONS_DIR = PREDICTIONS_FOLDER+\"/\"+RUN_NAME\n",
        "  CONFIG_FILE = BUCKET_PATH+\"/\"+FINETUNED_MODEL_DIR+\"/config.json\"\n",
        "\n",
        "  DATA_INFO = json.load(tf.gfile.Open(DATA_GCS_DIR+\"/info.json\"))   ##get the data info dictionary\n",
        "  EX_DATA_NUM = DATA_INFO[\"ex_data_num\"] if USING_EX_DATA else 0\n",
        "\n",
        "  ##run the evaluation/prediction loop\n",
        "  sucess,total_metrics,current_ckpt = \\\n",
        "          evaluation_loop(RUN_EVAL,\n",
        "                          RUN_PREDICTION,\n",
        "                          EVALUATE_WHILE_PREDICT,\n",
        "                          dataset,\n",
        "                          MODEL,\n",
        "                          total_metrics,\n",
        "                          MAX_SEQ_LENGTH,\n",
        "                          current_ORIG_DATA_FOLDER,\n",
        "                          BERT_GCS_DIR,\n",
        "                          USE_LATEST,\n",
        "                          CHECKPOINT_STEP,\n",
        "                          DATA_GCS_DIR,\n",
        "                          USING_SHARDS,\n",
        "                          START_SHARD,\n",
        "                          USING_EX_DATA,\n",
        "                          EX_DATA_NUM,\n",
        "                          EVAL_WHILE_PREDICT_PREDICTIONS_DIR,\n",
        "                          PREDICTIONS_DIR,\n",
        "                          EVALUATIONS_DIR,\n",
        "                          CONFIG_FILE)\n",
        "\n",
        "  if not sucess and REPEAT_LOOP:                        ##to prevent excessive interaction with GCS,  \n",
        "    time.sleep(CHECK_MODEL_EVERY_N_SECS)                ##if an eval/pred loop fails, the script \n",
        "    continue                                            ##will wait for a while before trying again\n",
        "\n",
        "  if not REPEAT_LOOP:\n",
        "    break\n",
        "if not REPEAT_LOOP and RUN_EVAL:\n",
        "  tf.logging.info(\"Printing all metrics...\\n\\n\")\n",
        "  for evals_dir,metrics in total_metrics.items():\n",
        "    tf.logging.info(\"Printing metrics for:\"+evals_dir+\"\\n\")\n",
        "    for key,metric in metrics.items():\n",
        "      tf.logging.info(key+\":\"+str(metric))\n",
        "    tf.logging.info(\"\\n\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "CLgXP-sV4zAd",
        "cw3EPxIe98Lg"
      ],
      "machine_shape": "hm",
      "name": "mutformer_finetuning_benchmark_eval_predict.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}