{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "mutformer_finetuning_benchmark_eval_predict.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "CLgXP-sV4zAd",
        "SgI2CyoFPjyY",
        "cw3EPxIe98Lg"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Finetuning Evaluation and Prediction Script"
      ],
      "metadata": {
        "id": "-60rHUe5DT2r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook evlauates and performs predictions on test data using finetuned models"
      ],
      "metadata": {
        "id": "dhEhbQ08DXyG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown In the case that an inference database is large and a long duration of continuous runtime is required, a GCP TPU/runtime to run this notebook may be desirable. If that's the case, specify here:\n",
        "USE_GCP_TPU = False #@param {type:\"boolean\"}\n",
        "#@markdown Which task to perform: options are \"MRPC\" for paired sequence method, \"RE\" for single sequence method, or \"NER\" for single sequance per residue prediction (if you add more modes make sure to change the corresponding code segments)\n",
        "MODE = \"MRPC_w_ex_data\" #@param {type:\"string\"}\n",
        "MAX_SEQ_LENGTH =  1024#@param {type:\"integer\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "#@markdown ###### For if multiple models are being evaluated: xxx is the placeholder for the individual model identifier (if only one is being evaluated replace xx with the actual name of the model)\n",
        "#@markdown \\\n",
        "#@markdown folder prefix for where to load the finetuned model from\n",
        "MODEL_DIR_format = \"bert_model_mrpc_adding_preds_only_others_xxx\" #@param {type:\"string\"}\n",
        "#@markdown folder where the config.json file was stored in\n",
        "INIT_MODEL_DIR_format = \"bert_model_xxx\" #@param {type:\"string\"}\n",
        "#@markdown folder prefix for where to get the finetuning data\n",
        "DATA_DIR_format = \"MRPC_adding_preds_only_others_xxx\" #@param {type:\"string\"}\n",
        "#@markdown specify a header for all output locations (set to \"\" to disable)\n",
        "RUN_NAME_format = \"MRPC_adding_preds_only_others_xxx\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Eval procedure config\n",
        "EVAL_BATCH_SIZE =  64 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8 #@param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLgXP-sV4zAd"
      },
      "source": [
        "#If running on a GCP runtime, follow these instructions to set it up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eT2L30n5Eiu"
      },
      "source": [
        "###1) Create a VM from the GCP website\n",
        "###2) Open a command prompt on your computer and perform the following steps\"\n",
        "To ssh into the VM:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Note: Make sure the port above matches the port below (in this case it's 8888)\n",
        "\\\n",
        "\\\n",
        "Run each of these commands individually, or copy and paste the one command below:\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "One command:\n",
        "```\n",
        "sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "###3) In this notebook, to connect to this runtime, click the \"connect to local runtime\" option under the connect button, and copy and paste the outputted link with \"locahost: ...\"\n",
        "###4) Finally, run this code segment, which creates a TPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOygMeIt5JqM"
      },
      "source": [
        "GCE_PROJECT_NAME = \"genome-project-319100\" #@param {type:\"string\"}\n",
        "TPU_ZONE = \"us-central1-f\" #@param {type:\"string\"}\n",
        "TPU_NAME = \"mutformer-tpu\" #@param {type:\"string\"}\n",
        "\n",
        "!gcloud alpha compute tpus create $TPU_NAME --accelerator-type=tpu-v2 --version=1.15.5 --zone=$TPU_ZONE ##create new TPU\n",
        "\n",
        "!gsutil iam ch serviceAccount:`gcloud alpha compute tpus describe $TPU_NAME | grep serviceAccount | cut -d' ' -f2`:admin gs://theodore_jiang && echo 'Successfully set permissions!' ##give TPU access to GCS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k0FGNhvN_Qa"
      },
      "source": [
        "#Clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDPM9ml0N-za"
      },
      "source": [
        "if USE_GCP_TPU:\n",
        "  !sudo apt-get -y install git\n",
        "#@markdown ######where to clone the repo into (only value that it can't be is \"mutformer\"):\n",
        "REPO_DESTINATION_PATH = \"code/mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Imports/authorize GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW"
      },
      "source": [
        "if not USE_GCP_TPU:\n",
        "  %tensorflow_version 1.x\n",
        "  from google.colab import auth\n",
        "  print(\"Authorize for GCS:\")\n",
        "  auth.authenticate_user()\n",
        "  print(\"Authorize done\")\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import importlib\n",
        "\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic\n",
        "from mutformer.modeling import BertModel,BertModelModified\n",
        "from mutformer.run_classifier import MrpcWithPredsProcessor,MrpcProcessor,REProcessor ##change this part if you add more modes--\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor      ##--\n",
        "\n",
        "\n",
        "##reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_classifier,\n",
        "                  run_ner_for_pathogenic]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "#@markdown ###### Whether or not to write logs to a file\n",
        "DO_FILE_LOGGING = True #@param {type:\"boolean\"}\n",
        "if DO_FILE_LOGGING:\n",
        "  #@markdown ###### If using file logging, what path to write logs to\n",
        "  FILE_LOGGING_PATH = 'file_logging/spam.log' #@param {type:\"string\"}\n",
        "  if not os.path.exists(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1])):\n",
        "    os.makedirs(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1]))\n",
        "  fh = logging.FileHandler(FILE_LOGGING_PATH)\n",
        "  fh.setLevel(logging.INFO)\n",
        "  fh.setFormatter(formatter)\n",
        "  log.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "if USE_GCP_TPU:\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_NAME, zone=TPU_ZONE, project=GCE_PROJECT_NAME)\n",
        "  TPU_ADDRESS = tpu_cluster_resolver.get_master()\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "      log.info('TPU address is ' + TPU_ADDRESS)\n",
        "      tf.contrib.cloud.configure_gcs(session)\n",
        "else:\n",
        "  if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    log.info(\"Using TPU runtime\")\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "    with tf.Session(TPU_ADDRESS) as session:\n",
        "      log.info('TPU address is ' + TPU_ADDRESS)\n",
        "      # Upload credentials to TPU.\n",
        "      with open('/content/adc.json', 'r') as f:\n",
        "        auth_info = json.load(f)\n",
        "      tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "      \n",
        "  else:\n",
        "    raise Exception('Not connected to TPU runtime, TPU required to run mutformer')\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\": ##change this part if you added more modes\n",
        "  processor = MrpcProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"MRPC_w_ex_data\":\n",
        "  processor = MrpcWithPredsProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"RE\":\n",
        "  processor = REProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"NER\":\n",
        "  processor = NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\", \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0TXw9GtCKz"
      },
      "source": [
        "#Specify location preferences for google drive vs GCS/Mount Drive if needed (for autodetecting number of steps if doing evaluation later)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYsYBUCJMTdz"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "#@markdown ###### Note: for all of these, if using USE_GCP_TPU, all of these parameters must use GCS, because a GCP TPU can't access google drive\n",
        "#@markdown \\\n",
        "#@markdown if not using USE_GCP_TPU, drive path for where the data was stored to detect # of steps in data (this is just to limit interaction with GCS; item can also be left blank and steps will be automatically detected from tfrecords)\n",
        "data_folder_format = \"/content/drive/My Drive/BERT finetuning/MRPC/w_added_modified_bert_mrpc_512\" #@param {type: \"string\"}\n",
        "DRIVE_PATH = \"/content/drive/My Drive\"\n",
        "#@markdown whether to use GCS for communicating with training script, if not, defaults to drive\n",
        "GCS_COMS = False #@param {type:\"boolean\"}\n",
        "#@markdown whether to use GCS for writing predictions, if not, defaults to drive\n",
        "GCS_PREDICTIONS = False #@param {type:\"boolean\"}\n",
        "#@markdown whether to use GCS for writing eval results, if not, defaults to drive\n",
        "GCS_EVAL = False #@param {type:\"boolean\"}\n",
        "\n",
        "COMS_PATH = BUCKET_PATH if GCS_COMS else DRIVE_PATH\n",
        "PREDS_PATH = BUCKET_PATH if GCS_PREDICTIONS else DRIVE_PATH\n",
        "EVALS_PATH = BUCKET_PATH if GCS_EVAL else DRIVE_PATH\n",
        "\n",
        "if USE_GCP_TPU:\n",
        "  FILES_PATH = BUCKET_PATH\n",
        "\n",
        "if \"/content/drive\" in data_folder_format or not GCS_COMS or not GCS_PREDICTIONS or not GCS_EVAL:\n",
        "  from google.colab import drive,auth\n",
        "  !fusermount -u /content/drive\n",
        "  drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run Eval/prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This following section will perform evaluation and prediction on either the eval dataset or the test dataset."
      ],
      "metadata": {
        "id": "1MRlQ61PZ4fl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvuxLWh5_Vm8"
      },
      "source": [
        "###General Setup and definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVe9VfxJ_Y38"
      },
      "source": [
        "#@markdown when testing on the \"test\" dataset, whether or not to ensure all dataponts are predicted (if so, make sure this option was also specified as True during data generation)\n",
        "PRECISE_TESTING = True #@param {type:\"boolean\"}\n",
        "#@markdown maximum batch size the runtime can handle during prediction without OOM for all models being evaluated/tested (for these modela on a colab runtime it's about 1024)\n",
        "MAX_BATCH_SIZE =  512 #@param {type:\"integer\"}\n",
        "\n",
        "def write_metrics(metrics,dir):\n",
        "  gs = metrics[\"global_step\"]\n",
        "  print(\"global step\",gs)\n",
        "\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "  tf.reset_default_graph()  \n",
        "  for key,value in metrics.items():\n",
        "    print(key,value)\n",
        "    x_scalar = tf.constant(value)\n",
        "    first_summary = tf.summary.scalar(name=key, tensor=x_scalar)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        writer = tf.summary.FileWriter(dir)\n",
        "        sess.run(init)\n",
        "        summary = sess.run(first_summary)\n",
        "        writer.add_summary(summary, gs)\n",
        "        writer.flush()\n",
        "        print('Done with writing the scalar summary')\n",
        "    time.sleep(1)\n",
        "  if not os.path.exists(EVALS_PATH+\"/\"+dir):\n",
        "    os.makedirs(EVALS_PATH+\"/\"+dir)\n",
        "  if \"gs:\" in EVALS_PATH:\n",
        "    cmd = \"gsutil cp -r \\\"\"+dir+\"/.\\\" \\\"\"+EVALS_PATH+\"/\"+dir+\"\\\"\"\n",
        "  else:\n",
        "    cmd = \"cp -r \\\"\"+dir+\"/.\\\" \\\"\"+EVALS_PATH+\"/\"+dir+\"\\\"\"\n",
        "  !{cmd}\n",
        "\n",
        "def write_predictions(PREDICTIONS_FOLDER,\n",
        "                      RESTORE_MODEL_NAME,\n",
        "                      result,\n",
        "                      result_trailing,\n",
        "                      shard_id=\"\"):\n",
        "  if not os.path.exists(PREDS_PATH+\"/\"+PREDICTIONS_FOLDER):\n",
        "    os.makedirs(PREDS_PATH+\"/\"+PREDICTIONS_FOLDER)\n",
        "  with tf.gfile.Open(PREDS_PATH+\"/\"+PREDICTIONS_FOLDER+\"/\"+RUN_NAME_format.replace(\"xxx\",RESTORE_MODEL_NAME)+\"_predictions\"+shard_id+\".txt\", \"w\") as writer:\n",
        "    tf.logging.info(\"***** Predict results *****\")\n",
        "    for (i, prediction) in enumerate(result):\n",
        "      output_line = \"\\t\".join([str(k)+\":\"+str(v) for k,v in prediction.items()]) + \"\\n\"\n",
        "      writer.write(output_line)\n",
        "    if result_trailing:\n",
        "      for (i, prediction) in enumerate(result_trailing):\n",
        "        output_line = \"\\t\".join([str(k)+\":\"+str(v) for k,v in prediction.items()]) + \"\\n\"\n",
        "        writer.write(output_line)\n",
        "\n",
        "## dictionary mapping model name to which architecture \n",
        "## to use (BertModel is a classic BERT, BertModelModified \n",
        "## has the convs for multi-residue \"vocabulary\")\n",
        "name2model = {                          \n",
        "    \"modified_large\":BertModelModified,\n",
        "    \"modified_medium\":BertModelModified,\n",
        "    \"modified\":BertModelModified,\n",
        "    \"orig\":BertModel,\n",
        "    \"large\":BertModel\n",
        "}\n",
        "\n",
        "\n",
        "def evaluation_loop(RUN_EVAL,\n",
        "                    RUN_PREDICTION,\n",
        "                    RESTORE_MODEL_NAME,\n",
        "                    EVALUATE_WHILE_PREDICT,\n",
        "                    dataset,\n",
        "                    MODEL,\n",
        "                    total_metrics,\n",
        "                    current_ckpt,\n",
        "                    DATA_SEQ_LENGTH,\n",
        "                    current_data_folder_eval,\n",
        "                    BERT_GCS_DIR,\n",
        "                    USE_LATEST,\n",
        "                    CHECKPOINT_STEP,\n",
        "                    DATA_GCS_DIR_EVAL,\n",
        "                    USING_SHARDS,\n",
        "                    START_SHARD,\n",
        "                    USING_PREDS,\n",
        "                    PRED_NUM,\n",
        "                    GCS_PREDICTIONS_DIR,\n",
        "                    GCS_EVALUATIONS_DIR,\n",
        "                    PREDICTIONS_FOLDER,\n",
        "                    EVALUATIONS_FOLDER,\n",
        "                    LOCAL_EVALUATIONS_DIR,\n",
        "                    CONFIG_FILE):\n",
        "\n",
        "  print(\"Using data from:\",DATA_GCS_DIR_EVAL)\n",
        "\n",
        "  eval_file = os.path.join(DATA_GCS_DIR_EVAL, evaluating_file)\n",
        "\n",
        "  def steps_getter(input_files):\n",
        "    tot_sequences = 0\n",
        "    for input_file in input_files:\n",
        "      print(\"reading:\",input_file)\n",
        "\n",
        "      d = tf.data.TFRecordDataset(input_file)\n",
        "\n",
        "      with tf.Session() as sess:\n",
        "        tot_sequences+=sess.run(d.reduce(0, lambda x,_: x+1))\n",
        "\n",
        "    return tot_sequences\n",
        "  if USING_SHARDS:\n",
        "    shards_folder = DATA_GCS_DIR_EVAL\n",
        "    input_file = os.path.join(DATA_GCS_DIR_EVAL, evaluating_file)\n",
        "    import re\n",
        "    file_name = input_file.split(\"/\")[-1]\n",
        "    shards = [shards_folder + \"/\" + file for file in tf.io.gfile.listdir(shards_folder) if\n",
        "              re.match(file_name + \"_\\d+\", file)]\n",
        "    shards = sorted(shards,key=lambda shard:int(shard.split(\"_\")[-1]))[START_SHARD:]\n",
        "  else:\n",
        "    shards = [eval_file]\n",
        "\n",
        "  if USING_SHARDS:\n",
        "    print(\"\\nUSING SHARDs:\")\n",
        "    for shard in shards:\n",
        "      print(shard)\n",
        "    print(\"\\n\")\n",
        "\n",
        "  if RUN_EVAL:\n",
        "    if dataset==\"dev\":\n",
        "      try:\n",
        "        data_path_eval = \"/content/drive/My Drive/\"+current_data_folder_eval+\"/dev.tsv\"\n",
        "        lines = open(data_path_eval).read().split(\"\\n\")\n",
        "        EVAL_STEPS = int(len(lines)/EVAL_BATCH_SIZE)\n",
        "      except:\n",
        "        SEQUENCES_PER_EPOCH = steps_getter(shards)\n",
        "        EVAL_STEPS = int(SEQUENCES_PER_EPOCH/EVAL_BATCH_SIZE)\n",
        "    else:\n",
        "      SEQUENCES_PER_EPOCH = steps_getter(shards)\n",
        "      EVAL_STEPS = int(SEQUENCES_PER_EPOCH/EVAL_BATCH_SIZE)\n",
        "\n",
        "    print(\"eval steps:\",EVAL_STEPS)\n",
        "\n",
        "  \n",
        "  if EVALUATE_WHILE_PREDICT:\n",
        "    cmd = \"gsutil -m rm -r \"+GCS_PREDICTIONS_DIR\n",
        "    !{cmd}\n",
        "  if USE_LATEST:\n",
        "    RESTORE_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "  else:\n",
        "    latest_ckpt = tf.train.latest_checkpoint(BERT_GCS_DIR).split(\"/\")[-1]\n",
        "    RESTORE_CHECKPOINT = [\".\".join(ckpt.split(\".\")[:-1]) \n",
        "                          for ckpt in tf.io.gfile.listdir(BERT_GCS_DIR) \n",
        "                          if len(ckpt.split(\".\"))==3 and str(CHECKPOINT_STEP) == ckpt.split(\".\")[-2].split(\"-\")[-1]][0]\n",
        "    file_lines = tf.gfile.Open(BERT_GCS_DIR+\"/checkpoint\").read().split(\"\\n\")\n",
        "    file_lines[0] = file_lines[0].replace(latest_ckpt,RESTORE_CHECKPOINT)\n",
        "    RESTORE_CHECKPOINT = BERT_GCS_DIR+\"/\"+RESTORE_CHECKPOINT\n",
        "\n",
        "    tf.gfile.Open(BERT_GCS_DIR+\"/checkpoint\",\"w+\").write(\"\\n\".join(file_lines))\n",
        "  if RUN_EVAL:\n",
        "    if RESTORE_CHECKPOINT==current_ckpt:\n",
        "      return False,None,current_ckpt\n",
        "\n",
        "  current_ckpt=RESTORE_CHECKPOINT\n",
        "  print(\"USING CHECKPOINT:\",current_ckpt)\n",
        "  config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "\n",
        "  model_fn = script.model_fn_builder(\n",
        "      bert_config=config,\n",
        "      num_labels=len(label_list),\n",
        "      init_checkpoint=None,\n",
        "      restore_checkpoint=RESTORE_CHECKPOINT,\n",
        "      init_learning_rate=0,\n",
        "      decay_per_step=0,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=True,\n",
        "      use_one_hot_embeddings=True,\n",
        "      bert=MODEL,\n",
        "      test_results_dir=GCS_PREDICTIONS_DIR,\n",
        "      yield_predictions=EVALUATE_WHILE_PREDICT,\n",
        "      using_preds=USING_PREDS)\n",
        "\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=BERT_GCS_DIR,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=True,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=1,\n",
        "      eval_batch_size=EVAL_BATCH_SIZE,\n",
        "      predict_batch_size=MAX_BATCH_SIZE)\n",
        "    \n",
        "  print(\"USING FILE:\",eval_file)\n",
        "  try:\n",
        "    for n,shard in enumerate(shards):\n",
        "      input_fn = script.file_based_input_fn_builder(\n",
        "            input_file=shard,\n",
        "            seq_length=DATA_SEQ_LENGTH,\n",
        "            is_training=False,\n",
        "            drop_remainder=True,\n",
        "            pred_num=PRED_NUM if USING_PREDS else None)\n",
        "\n",
        "\n",
        "      tf.logging.info(\"***** Running evaluation/prediction *****\")\n",
        "      tf.logging.info(\" Eval Batch size = %d\", EVAL_BATCH_SIZE)\n",
        "      tf.logging.info(\" Predict Batch size = %d\", MAX_BATCH_SIZE)\n",
        "      \n",
        "    \n",
        "      if RUN_EVAL:\n",
        "        eval_metrics = estimator.evaluate(input_fn=input_fn, steps=EVAL_STEPS)\n",
        "        print(\"\\n\\n\\n\\n\\n\\nEVAL METRICS:\")\n",
        "        for k,v in eval_metrics.items():\n",
        "          print(k+\":\",v)\n",
        "        print(\"\\n\\n\\n\\n\\n\\n\\n\")\n",
        "        if dataset == \"dev\":\n",
        "          write_metrics(eval_metrics,LOCAL_EVALUATIONS_DIR)\n",
        "        else:\n",
        "          total_metrics[LOCAL_EVALUATIONS_DIR] = eval_metrics\n",
        "      if RUN_PREDICTION:\n",
        "        result=estimator.predict(input_fn=input_fn)\n",
        "        if PRECISE_TESTING and RUN_PREDICTION and n==len(shards)-1:\n",
        "          run_config_trailing = tf.contrib.tpu.RunConfig(\n",
        "            cluster=tpu_cluster_resolver,\n",
        "            model_dir=BERT_GCS_DIR,\n",
        "            tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "                num_shards=1,\n",
        "                per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "          estimator_trailing = tf.contrib.tpu.TPUEstimator(\n",
        "              use_tpu=True,\n",
        "              model_fn=model_fn,\n",
        "              config=run_config_trailing,\n",
        "              train_batch_size=1,\n",
        "              predict_batch_size=1)\n",
        "          test_file_trailing = os.path.join(DATA_GCS_DIR_EVAL, \"test_trailing.tf_record\")\n",
        "          test_input_fn_trailing = script.file_based_input_fn_builder(\n",
        "              input_file=test_file_trailing,\n",
        "              seq_length=DATA_SEQ_LENGTH,\n",
        "              is_training=False,\n",
        "              drop_remainder=True,\n",
        "              pred_num=PRED_NUM if USING_PREDS else None)\n",
        "          result_trailing=estimator_trailing.predict(input_fn=test_input_fn_trailing)\n",
        "        else:\n",
        "          result_trailing = None\n",
        "        write_predictions(PREDICTIONS_FOLDER,\n",
        "                          RESTORE_MODEL_NAME,\n",
        "                          result,\n",
        "                          result_trailing,\n",
        "                          shard_id=str(START_SHARD+n)if USING_SHARDS else \"\")\n",
        "    return True,total_metrics,current_ckpt\n",
        "  except Exception as e:\n",
        "    print(\"FAILED:\",e)\n",
        "    return False,None,current_ckpt\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following are three different code segments to run:\n",
        "1. For if you benchmarked model/sequence length during finetuning and wish to evaluate each model \\\n",
        "2. For if you benchmarked sequence length/batch size during finetuning and wish to evaluate each model \\\n",
        "3. For only evaluating/predicting using a single model\n",
        "\n",
        "Choose a desired code segment to run, select the desired options for evaluating/predicting, and run that code segment\n",
        "\\\n",
        "\\\n",
        "Note: All evaluation results will be written into the previously specified logging directory either under google drive or GCS. To view the results, use the colab notebook titled \"mutformer processing and viewing finetuning results,\" which can also be used to view prediction results"
      ],
      "metadata": {
        "id": "tM7HXcKbZvn-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgI2CyoFPjyY"
      },
      "source": [
        "###Model/Sequence Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "source": [
        "#@markdown whether or not to run evaluation\n",
        "RUN_EVAL = True #@param {type:\"boolean\"}\n",
        "#@markdown whether or not to run prediction in a seperate loop from evaluation (if using EVALUATE_WHILE_PREDICT, set to False)\n",
        "RUN_PREDICTION = False #@param {type:\"boolean\"}\n",
        "#@markdown if evaluating, whether or not to evaluate and write test results in the same loop; useful when amount of test data is very small and the time it takes to restart a loop is siginificant (the evalution loop itself will be slower due to writing tfevents) (if yes, prediction results will be written in the form of tfevent files into GCS, so use the notebook titled \"mutformer processing and viewing finetuning results\" to view them)\n",
        "EVALUATE_WHILE_PREDICT =  False #@param {type:\"boolean\"}\n",
        "#@markdown whether or not testing/evaluating data was generated in shards (for really large databases)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown if using shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD =   0#@param {type:\"integer\"}\n",
        "#@markdown whether or not external data is being used\n",
        "USING_PREDS = True #@param {type:\"boolean\"}\n",
        "#@markdown if using external data, how many datapoints are included in total\n",
        "PRED_NUM =   22#@param {type:\"integer\"}\n",
        "#@markdown what folder to write predictions into (if using EVALUATE_WHILE_PREDICT or data source was a GCS path, predictions will be written into this folder under GCS, otherwise predictions will be written to this folder under google drive)\n",
        "PREDICTIONS_FOLDER = \"mrpc_loss_spam_model_comparison_final_predictions\" #@param {type:\"string\"}\n",
        "#@markdown what folder to write evaluation results into \n",
        "EVALUATIONS_FOLDER = \"mrpc_loss_spam_model_comparison_final\" #@param {type:\"string\"}\n",
        "#@markdown #####Note: If using EVALUATE_WHILE_PREDICT, prediction results must be read using the previously mentioned colab notebook, otherwise, predictions will be written directly as txts and will be directly accessible from google drive under the folder specified above\n",
        "#@markdown \\\n",
        "#@markdown ###### whether to evaluate/predict on the test set or the dev set (\"test\" or \"dev\") (test set will only run once, dev set will run continuously)\n",
        "dataset = \"test\" #@param{type:\"string\"}\n",
        "#@markdown ###### if using test set, which model ids to evaluate (eval set will only run on the active model, test will run on specified models)\n",
        "models = [\"orig\",\"large\",\"modified\"] #@param\n",
        "#@markdown ###### if using test set, which sequence lengthed models to evaluate\n",
        "lengths = [256,512,1024] #@param\n",
        "#@markdown ###### whether to use the latest checkpoint in the folder (set to false if an intermediate checkpoint should be used)\n",
        "USE_LATEST = False #@param {type:\"boolean\"}\n",
        "#@markdown ###### if not using the latest checkpoint, which checkpoint to use\n",
        "CHECKPOINT_STEP =  7000#@param {type:\"integer\"}\n",
        "\n",
        "if dataset==\"test\":\n",
        "  evaluating_file = \"test.tf_record\"\n",
        "  total_metrics = {}\n",
        "elif dataset==\"dev\":\n",
        "  evaluating_file = \"eval.tf_record\"\n",
        "else:\n",
        "  raise Exception(\"only datasets supported are dev and test\")\n",
        "\n",
        "current_ckpt = \"N/A\"\n",
        "\n",
        "while True:\n",
        "  for MODEL_NAME in models:\n",
        "    for DATA_SEQ_LENGTH in lengths:\n",
        "      if RUN_EVAL:\n",
        "        try:\n",
        "          ##reading the identifiers from drive written by the training script to know what to evaluate\n",
        "          MODEL_NAME = tf.gfile.Open(FILES_PATH+\"/finetuning_run_paired_model.txt\").read()\n",
        "          DATA_SEQ_LENGTH = int(tf.gfile.Open(FILES_PATH+\"/finetuning_run_paired_seq_length.txt\").read())\n",
        "        except:\n",
        "          print(\"Models haven't started training yet...checking again in 60 seconds\")\n",
        "          time.sleep(60)\n",
        "          continue\n",
        "\n",
        "      print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "            \"\\nINPUT MAX SEQ LENGTH:\",DATA_SEQ_LENGTH)\n",
        "      \n",
        "      MODEL = name2model[MODEL_NAME]\n",
        "      current_data_folder_eval= data_folder_format.replace(\"xxx\",str(DATA_SEQ_LENGTH))\n",
        "      RESTORE_MODEL_NAME = MODEL_DIR_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH))\n",
        "\n",
        "      BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR_format.replace(\"xxx\",MODEL_NAME)+\"_\"+str(DATA_SEQ_LENGTH))\n",
        "      DATA_GCS_DIR_EVAL = \"{}/{}\".format(BUCKET_PATH, DATA_DIR_format.replace(\"xxx\",str(DATA_SEQ_LENGTH)))\n",
        "      \n",
        "      GCS_EVALUATIONS_DIR = \"{}/{}\".format(BUCKET_PATH, EVALUATIONS_FOLDER+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)))\n",
        "      LOCAL_EVALUATIONS_DIR = \"{}/{}\".format(EVALUATIONS_FOLDER, RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)))\n",
        "      GCS_PREDICTIONS_DIR = \"{}/{}\".format(BUCKET_PATH, PREDICTIONS_FOLDER+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)))\n",
        "\n",
        "      CONFIG_FILE = \"{}/config.json\".format(BUCKET_PATH+\"/\"+INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "\n",
        "      ##run the evaluation/prediction loop\n",
        "      sucess,total_metrics,current_ckpt = \\\n",
        "              evaluation_loop(RUN_EVAL,\n",
        "                              RUN_PREDICTION,\n",
        "                              RESTORE_MODEL_NAME,\n",
        "                              EVALUATE_WHILE_PREDICT,\n",
        "                              dataset,\n",
        "                              MODEL,\n",
        "                              total_metrics,\n",
        "                              current_ckpt,\n",
        "                              DATA_SEQ_LENGTH,\n",
        "                              current_data_folder_eval,\n",
        "                              BERT_GCS_DIR,\n",
        "                              USE_LATEST,\n",
        "                              CHECKPOINT_STEP,\n",
        "                              DATA_GCS_DIR_EVAL,\n",
        "                              USING_SHARDS,\n",
        "                              START_SHARD,\n",
        "                              USING_PREDS,\n",
        "                              PRED_NUM,\n",
        "                              GCS_PREDICTIONS_DIR,\n",
        "                              GCS_EVALUATIONS_DIR,\n",
        "                              PREDICTIONS_FOLDER,\n",
        "                              EVALUATIONS_FOLDER,\n",
        "                              LOCAL_EVALUATIONS_DIR,\n",
        "                              CONFIG_FILE)\n",
        "\n",
        "\n",
        "      if not sucess:\n",
        "        time.sleep(30)\n",
        "        continue\n",
        "\n",
        "      if RUN_EVAL:\n",
        "        break\n",
        "    if RUN_EVAL:\n",
        "      break\n",
        "  if RUN_PREDICTION:\n",
        "    break\n",
        "if RUN_PREDICTION and RUN_EVAL:\n",
        "  for evals_dir,metrics in total_metrics.items():\n",
        "    print(\"Printing metrics for:\",evals_dir,\"\\n\")\n",
        "    for key,metric in metrics.items():\n",
        "      print(key+\":\",metric)\n",
        "    print(\"\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw3EPxIe98Lg"
      },
      "source": [
        "###Batch Size/Sequence Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN8Fpeb-96nA"
      },
      "source": [
        "#@markdown whether or not to run evaluation\n",
        "RUN_EVAL = False #@param {type:\"boolean\"}\n",
        "#@markdown whether or not to run prediction in a seperate loop from evaluation (if using EVALUATE_WHILE_PREDICT, set to False)\n",
        "RUN_PREDICTION = True #@param {type:\"boolean\"}\n",
        "#@markdown if evaluating, whether or not to evaluate and write test results in the same loop; useful when amount of test data is very small and the time it takes to restart a loop is siginificant (the evalution loop itself will be slower due to writing tfevents) (if yes, prediction results will be written in the form of tfevent files into GCS, so use the notebook titled \"mutformer processing and viewing finetuning results\" to view them)\n",
        "EVALUATE_WHILE_PREDICT =  False #@param {type:\"boolean\"}\n",
        "#@markdown whether or not testing/evaluating data was generated in shards (for really large databases)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown if using shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD =   0#@param {type:\"integer\"}\n",
        "#@markdown whether or not external data is being used\n",
        "USING_PREDS = True #@param {type:\"boolean\"}\n",
        "#@markdown if using external data, how many datapoints are included in total\n",
        "PRED_NUM =   22#@param {type:\"integer\"}\n",
        "#@markdown what folder to write predictions into \n",
        "PREDICTIONS_FOLDER = \"mrpc_loss_spam_model_comparison_final_predictions\" #@param {type:\"string\"}\n",
        "#@markdown what folder to write evaluation results into \n",
        "EVALUATIONS_FOLDER = \"mrpc_loss_spam_model_comparison_final\" #@param {type:\"string\"}\n",
        "#@markdown #####Note: If using EVALUATE_WHILE_PREDICT, prediction results must be read using the notebook titled \"mutformer processing and viewing finetuning results\" , otherwise, predictions will be written directly as txts and will be directly accessible from google drive under the folder specified above\n",
        "#@markdown \\\n",
        "#@markdown ###### whether to evaluate on the test set or the dev set (\"test\" or \"dev\") (test set will only run once, dev set will run continuously)\n",
        "dataset = \"test\" #@param{type:\"string\"}\n",
        "#@markdown which model id to evaluate\n",
        "MODEL_NAME=\"modified\" #@param {type:\"string\"}\n",
        "#@markdown ###### if using test set, which batch sized models to test \n",
        "batch_sizes = [32,16,64] #@param\n",
        "#@markdown ###### if using test set, which sequence lengthed models to test\n",
        "lengths = [256,512,1024] #@param\n",
        "#@markdown ###### whether to use the latest checkpoint in the folder (set to false if an intermediate checkpoint should be used)\n",
        "USE_LATEST = False #@param {type:\"boolean\"}\n",
        "#@markdown ###### if not using the latest checkpoint, which checkpoint to use\n",
        "CHECKPOINT_STEP =  7000#@param {type:\"integer\"}\n",
        "\n",
        "if dataset==\"test\":                  \n",
        "  evaluating_file = \"test.tf_record\" \n",
        "  total_metrics = {}  ## a dictionary for all metrics to  \n",
        "                      ## print at the end during testing, \n",
        "                      ## not necessary during evaluation   \n",
        "elif dataset==\"dev\":                \n",
        "  evaluating_file = \"eval.tf_record\" \n",
        "  total_metrics = None\n",
        "else:\n",
        "  raise Exception(\"only datasets supported are dev and test\")\n",
        "\n",
        "current_ckpt = \"N/A\"\n",
        "\n",
        "while True:\n",
        "  for BATCH_SIZE in batch_sizes:\n",
        "    for DATA_SEQ_LENGTH in lengths:\n",
        "      if RUN_EVAL:\n",
        "        try:\n",
        "          ##reading the identifiers from drive written by the training script to know what to evaluate\n",
        "          BATCH_SIZE = int(tf.gfile.Open(COMS_PATH+\"/finetuning_run_paired_batch_size.txt\").read())\n",
        "          DATA_SEQ_LENGTH = int(tf.gfile.Open(COMS_PATH+\"/finetuning_run_paired_seq_length.txt\").read())\n",
        "        except:\n",
        "          print(\"Models haven't started training yet...checking again in 60 seconds\")\n",
        "          time.sleep(60)\n",
        "          continue\n",
        "      print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "            \"\\nINPUT MAX SEQ LENGTH:\",DATA_SEQ_LENGTH,\n",
        "            \"\\nBATCH_SIZE_FINETUNED_ON:\",BATCH_SIZE,\"\\n\\n\\n\")\n",
        "\n",
        "      MODEL = name2model[MODEL_NAME]\n",
        "      current_data_folder_eval= data_folder_format.replace(\"xxx\",str(DATA_SEQ_LENGTH))\n",
        "      RESTORE_MODEL_NAME = MODEL_DIR_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE))\n",
        "\n",
        "      BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, RESTORE_MODEL_NAME)\n",
        "      DATA_GCS_DIR_EVAL = \"{}/{}\".format(BUCKET_PATH, DATA_DIR_format.replace(\"xxx\",str(DATA_SEQ_LENGTH)))\n",
        "      \n",
        "      GCS_PREDICTIONS_DIR = \"{}/{}\".format(BUCKET_PATH, PREDICTIONS_FOLDER+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE)))\n",
        "      GCS_EVALUATIONS_DIR = \"{}/{}\".format(BUCKET_PATH, EVALUATIONS_FOLDER+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE)))\n",
        "      LOCAL_EVALUATIONS_DIR = \"{}/{}\".format(EVALUATIONS_DIR, RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE)))\n",
        "\n",
        "      CONFIG_FILE = \"{}/config.json\".format(BUCKET_PATH+\"/\"+INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "      \n",
        "      ##run the evaluation/prediction loop\n",
        "      sucess,total_metrics,current_ckpt = \\\n",
        "              evaluation_loop(RUN_EVAL,\n",
        "                              RUN_PREDICTION,\n",
        "                              RESTORE_MODEL_NAME,\n",
        "                              EVALUATE_WHILE_PREDICT,\n",
        "                              dataset,\n",
        "                              MODEL,\n",
        "                              total_metrics,\n",
        "                              current_ckpt,\n",
        "                              DATA_SEQ_LENGTH,\n",
        "                              current_data_folder_eval,\n",
        "                              BERT_GCS_DIR,\n",
        "                              USE_LATEST,\n",
        "                              CHECKPOINT_STEP,\n",
        "                              DATA_GCS_DIR_EVAL,\n",
        "                              USING_SHARDS,\n",
        "                              START_SHARD,\n",
        "                              USING_PREDS,\n",
        "                              PRED_NUM,\n",
        "                              GCS_PREDICTIONS_DIR,\n",
        "                              GCS_EVALUATIONS_DIR,\n",
        "                              PREDICTIONS_FOLDER,\n",
        "                              EVALUATIONS_FOLDER,\n",
        "                              LOCAL_EVALUATIONS_DIR,\n",
        "                              CONFIG_FILE)\n",
        "\n",
        "      if not sucess:\n",
        "        time.sleep(30)\n",
        "        continue\n",
        "      if RUN_EVAL:\n",
        "        break\n",
        "    if RN_EVAL:\n",
        "      break\n",
        "  if RUN_PREDICTION:\n",
        "    break\n",
        "if RUN_PREDICTION and RUN_EVAL:\n",
        "  for evals_dir,metrics in total_metrics.items():\n",
        "    print(\"Printing metrics for:\",evals_dir,\"\\n\")\n",
        "    for key,metric in metrics.items():\n",
        "      print(key+\":\",metric)\n",
        "    print(\"\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3RKscL93Fw2"
      },
      "source": [
        "###Just one model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfcXh3N93JdG"
      },
      "source": [
        "#@markdown whether or not to run evaluation\n",
        "RUN_EVAL = False #@param {type:\"boolean\"}\n",
        "#@markdown whether or not to run prediction in a seperate loop from evaluation (if using EVALUATE_WHILE_PREDICT, set to False)\n",
        "RUN_PREDICTION = True #@param {type:\"boolean\"}\n",
        "#@markdown if evaluating, whether or not to evaluate and write test results in the same loop; useful when amount of test data is very small and the time it takes to restart a loop is siginificant (the evalution loop itself will be slower due to writing tfevents) (if yes, prediction results will be written in the form of tfevent files into GCS, so use the notebook titled \"mutformer processing and viewing finetuning results\" to view them)\n",
        "EVALUATE_WHILE_PREDICT =  False #@param {type:\"boolean\"}\n",
        "#@markdown whether or not testing/evaluating data was generated in shards (for really large databases)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown if using shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD =   0#@param {type:\"integer\"}\n",
        "#@markdown whether or not external data is being used\n",
        "USING_PREDS = True #@param {type:\"boolean\"}\n",
        "#@markdown if using external data, how many datapoints are included in total\n",
        "PRED_NUM =   22#@param {type:\"integer\"}\n",
        "#@markdown what folder to write predictions into \n",
        "PREDICTIONS_FOLDER = \"added_preds_only_others\" #@param {type:\"string\"}\n",
        "#@markdown what folder to write evaluation results into \n",
        "EVALUATIONS_FOLDER = \"added_preds_only_others\" #@param {type:\"string\"}\n",
        "#@markdown #####Note: If using EVALUATE_WHILE_PREDICT, prediction results must be read using the previously mentioned colab notebook, otherwise, predictions will be written directly as txts and will be directly accessible from google drive under the folder specified above\n",
        "#@markdown \\\n",
        "#@markdown ###### whether to evaluate on the test set or the dev set (\"test\" or \"dev\") (test set will only run once, dev set will run continuously)\n",
        "dataset = \"test\" #@param{type:\"string\"}\n",
        "#@markdown which model id to evaluate (to determine which architecture to use)\n",
        "MODEL_NAME=\"modified_large\" #@param {type:\"string\"}\n",
        "#@markdown ###### what sequence length to use\n",
        "DATA_SEQ_LENGTH =  512 #@param\n",
        "#@markdown ###### identifier for the model to use (replaces \"xxx\" from the variable \"MODEL_DIR_format\")\n",
        "model_name_extension = \"added_preds_only_others_512_32\" #@param {type:\"string\"}\n",
        "#@markdown ###### whether to use the latest checkpoint in the folder (set to false if an intermediate checkpoint should be used)\n",
        "USE_LATEST = False #@param {type:\"boolean\"}\n",
        "#@markdown ###### if not using the latest checkpoint, which checkpoint to use\n",
        "CHECKPOINT_STEP =  7000#@param {type:\"integer\"}\n",
        "\n",
        "if dataset==\"test\":\n",
        "  evaluating_file = \"test.tf_record\"\n",
        "  total_metrics = {}\n",
        "elif dataset==\"dev\":\n",
        "  evaluating_file = \"eval.tf_record\"\n",
        "  total_metrics = None\n",
        "else:\n",
        "  raise Exception(\"only datasets supported are dev and test\")\n",
        "\n",
        "current_ckpt = \"N/A\"\n",
        "\n",
        "while True:\n",
        "  print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "      \"\\nINPUT MAX SEQ LENGTH:\",DATA_SEQ_LENGTH)\n",
        "  MODEL = name2model[MODEL_NAME]\n",
        "  current_data_folder_eval= data_folder_format.replace(\"xxx\",str(DATA_SEQ_LENGTH))\n",
        "  RESTORE_MODEL_NAME = MODEL_DIR_format.replace(\"xxx\",model_name_extension)\n",
        "\n",
        "  BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, RESTORE_MODEL_NAME)  \n",
        "  DATA_GCS_DIR_EVAL = \"{}/{}\".format(BUCKET_PATH, DATA_DIR_format.replace(\"xxx\",str(DATA_SEQ_LENGTH)))\n",
        "\n",
        "  GCS_PREDICTIONS_DIR = \"{}/{}\".format(BUCKET_PATH, PREDICTIONS_FOLDER+\"/\"+RUN_NAME_format.replace(\"xxx\",model_name_extension))\n",
        "  GCS_EVALUATIONS_DIR = \"{}/{}\".format(BUCKET_PATH, EVALUATIONS_FOLDER+\"/\"+RUN_NAME_format.replace(\"xxx\",model_name_extension))\n",
        "  LOCAL_EVALUATIONS_DIR = \"{}/{}\".format(EVALUATIONS_FOLDER, RUN_NAME_format.replace(\"xxx\",model_name_extension))\n",
        "\n",
        "  CONFIG_FILE = \"{}/config.json\".format(BUCKET_PATH+\"/\"+INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "\n",
        "  ##run the evaluation/prediction loop\n",
        "  sucess,total_metrics,current_ckpt = \\\n",
        "          evaluation_loop(RUN_EVAL,\n",
        "                          RUN_PREDICTION,\n",
        "                          RESTORE_MODEL_NAME,\n",
        "                          EVALUATE_WHILE_PREDICT,\n",
        "                          dataset,\n",
        "                          MODEL,\n",
        "                          total_metrics,\n",
        "                          current_ckpt,\n",
        "                          DATA_SEQ_LENGTH,\n",
        "                          current_data_folder_eval,\n",
        "                          BERT_GCS_DIR,\n",
        "                          USE_LATEST,\n",
        "                          CHECKPOINT_STEP,\n",
        "                          DATA_GCS_DIR_EVAL,\n",
        "                          USING_SHARDS,\n",
        "                          START_SHARD,\n",
        "                          USING_PREDS,\n",
        "                          PRED_NUM,\n",
        "                          GCS_PREDICTIONS_DIR,\n",
        "                          GCS_EVALUATIONS_DIR,\n",
        "                          PREDICTIONS_FOLDER,\n",
        "                          EVALUATIONS_FOLDER,\n",
        "                          LOCAL_EVALUATIONS_DIR,\n",
        "                          CONFIG_FILE)\n",
        "\n",
        "  if not sucess and RUN_PREDICTION:\n",
        "    time.sleep(30)\n",
        "    continue\n",
        "\n",
        "  if RUN_PREDICTION:\n",
        "    break\n",
        "if RUN_PREDICTION and RUN_EVAL:\n",
        "  for evals_dir,metrics in total_metrics.items():\n",
        "    print(\"Printing metrics for:\",evals_dir,\"\\n\")\n",
        "    for key,metric in metrics.items():\n",
        "      print(key+\":\",metric)\n",
        "    print(\"\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}