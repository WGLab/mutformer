{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mutformer finetuning benchmark-eval",
      "provenance": [],
      "collapsed_sections": [
        "SgI2CyoFPjyY"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown Which task to perform: options are \"MRPC\" for paired sequence method, \"RE\" for single sequence method, or \"NER\" for single sequance per residue prediction (if you add more modes make sure to change the corresponding code segments)\n",
        "MODE = \"MRPC\" #@param {type:\"string\"}\n",
        "MAX_SEQ_LENGTH =  1024#@param {type:\"integer\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "#@markdown ###### For if multiple models are being evaluated: xxx is the placeholder for the individual model identifier (if only one is being evaluated replace xx with the actual name of the model)\n",
        "#@markdown \\\n",
        "#@markdown folder for where to save the finetuned model\n",
        "MODEL_DIR_format = \"bert_model_mrpc_xxx\" #@param {type:\"string\"}\n",
        "#@markdown folder for the pretrained model\n",
        "INIT_MODEL_DIR_format = \"bert_model_xxx\" #@param {type:\"string\"}\n",
        "DATA_DIR_format = \"MRPC_all_snps\" #@param {type:\"string\"}\n",
        "LOGGING_DIR = \"mrpc_loss_spam_model_comparison_final\" #@param {type:\"string\"}\n",
        "#@markdown specify a header for all output locations (set to \"\" to disable)\n",
        "RUN_NAME_format = \"MRPC_xxx\" #@param {type:\"string\"}\n",
        "VOC_FNAME = \"vocab.txt\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Training procedure config\n",
        "EVAL_BATCH_SIZE =  64 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8 #@param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k0FGNhvN_Qa"
      },
      "source": [
        "#Clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDPM9ml0N-za"
      },
      "source": [
        "#@markdown ######where to clone the repo into (only value that it can't be is \"mutformer\"):\n",
        "REPO_DESTINATION_PATH = \"code/mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://tianqitheodorejiang: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@github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "print(\"Authorize for GCS:\")\n",
        "auth.authenticate_user()\n",
        "print(\"Authorize done\")\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import nltk\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "\n",
        "print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())))\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic\n",
        "from mutformer.modeling import BertModel,BertModelModified\n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor ##change this part if you add more modes--\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor      ##--\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "#@markdown ###### Whether or not to write logs to a file\n",
        "DO_FILE_LOGGING = True #@param {type:\"boolean\"}\n",
        "if DO_FILE_LOGGING:\n",
        "  #@markdown ###### If using file logging, what path to write logs to\n",
        "  FILE_LOGGING_PATH = 'file_logging/spam.log' #@param {type:\"string\"}\n",
        "  if not os.path.exists(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1])):\n",
        "    os.makedirs(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1]))\n",
        "  fh = logging.FileHandler(FILE_LOGGING_PATH)\n",
        "  fh.setLevel(logging.INFO)\n",
        "  fh.setFormatter(formatter)\n",
        "  log.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  raise Exception('Not connected to TPU runtime. TPU runtime must be used ')\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\": ##change this part if you added more modes\n",
        "  processor = MrpcProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"RE\":\n",
        "  processor = REProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"NER\":\n",
        "  processor = NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\", \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0TXw9GtCKz"
      },
      "source": [
        "#Specify Data location/Mount Drive if needed (for autodetecting number of steps if doing evaluation later)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYsYBUCJMTdz"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "#@markdown folder in drive where the eval/test data is stored (can be a GCS path for large database inference, in this case, the folder path isn't actually used since prediction should be performed); xxx is the placeholder for the sequence length (if only using one single test set just put the actual folder here without xxx)\n",
        "data_folder_format = \"gs://theodore_jiang/MRPC_all_snp_benchmark\" #@param {type: \"string\"}from google.colab import drive,auth\n",
        "if \"/content/drive\" in data_folder_format:\n",
        "  !fusermount -u /content/drive\n",
        "  drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  FILES_PATH = \"/content/drive/My Drive\"\n",
        "else:\n",
        "  FILES_PATH = \"gs://\"+BUCKET_NAME\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Run Eval/prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrvDfJZq27Ei"
      },
      "source": [
        "This following section can perform evaluation and prediction on either the eval dataset or the test dataset. There are three different code segments to run:\\\n",
        "1.For if you benchmarked model/sequence length during finetuning and wish to evaluate each model \\\n",
        "2.For if you benchmarked sequence length/batch size during finetuning and wish to evaluate each model \\\n",
        "3.For only evaluating/predicting using a single model\n",
        "\n",
        "Choose a desired code segment to run, select the desired options for evaluating/predicting and run only that specific code segment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq-ElKw7TekB"
      },
      "source": [
        "Note: All evaluation results will be written into the previously specified logging directory under the mounted google drive. To view the results, use the colab notebook titled \"mutformer processing and viewing finetuning results\"\n",
        "\n",
        "Depending on whether or not EVALUATE_WHILE_PREDICT is used, prediction results will either be written into GCS or google drive. To view them, the colab notebook titled \"mutformer processing and viewing finetuning results\" can also be used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvuxLWh5_Vm8"
      },
      "source": [
        "###General Setup and definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVe9VfxJ_Y38"
      },
      "source": [
        "#@markdown when testing on the \"test\" dataset, whether or not to ensure all dataponts are predicted (if so, make sure this option was also specified as True during data generation)\n",
        "PRECISE_TESTING = False #@param {type:\"boolean\"}\n",
        "#@markdown maximum batch size the runtime can handle during prediction without OOM for all models being evaluated/tested (for these modela on a colab runtime it's about 1024)\n",
        "MAX_BATCH_SIZE =  1024 #@param {type:\"integer\"}\n",
        "\n",
        "def write_metrics(metrics,dir):\n",
        "  gs = metrics[\"global_step\"]\n",
        "  print(\"global step\",gs)\n",
        "\n",
        "  tf.compat.v1.disable_eager_execution()\n",
        "  tf.reset_default_graph()  \n",
        "  for key,value in metrics.items():\n",
        "    print(key,value)\n",
        "    x_scalar = tf.constant(value)\n",
        "    first_summary = tf.summary.scalar(name=key, tensor=x_scalar)\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        writer = tf.summary.FileWriter(dir)\n",
        "        sess.run(init)\n",
        "        summary = sess.run(first_summary)\n",
        "        writer.add_summary(summary, gs)\n",
        "        writer.flush()\n",
        "        print('Done with writing the scalar summary')\n",
        "    time.sleep(1)\n",
        "  if not os.path.exists(FILES_PATH+\"/\"+dir):\n",
        "    os.makedirs(FILES_PATH+\"/\"+dir)\n",
        "  cmd = \"cp -r \\\"\"+dir+\"/.\\\" \\\"\"+FILES_PATH+\"/\"+dir+\"\\\"\"\n",
        "  !{cmd}\n",
        "\n",
        "def write_predictions(PREDICTIONS_FOLDER,\n",
        "                      RESTORE_MODEL_NAME,\n",
        "                      result,\n",
        "                      result_trailing):\n",
        "  if not os.path.exists(FILES_PATH+\"/\"+PREDICTIONS_FOLDER):\n",
        "    os.makedirs(FILES_PATH+\"/\"+PREDICTIONS_FOLDER)\n",
        "  with tf.gfile.Open(FILES_PATH+\"/\"+PREDICTIONS_FOLDER+\"/\"+RUN_NAME_format.replace(\"xxx\",RESTORE_MODEL_NAME)+\"_predictions.txt\", \"w\") as writer:\n",
        "    tf.logging.info(\"***** Predict results *****\")\n",
        "    for (i, prediction) in enumerate(result):\n",
        "      output_line = \"\\t\".join([str(k)+\":\"+str(v) for k,v in prediction.items()]) + \"\\n\"\n",
        "      writer.write(output_line)\n",
        "    if result_trailing:\n",
        "      for (i, prediction) in enumerate(result_trailing):\n",
        "        output_line = \"\\t\".join([str(k)+\":\"+str(v) for k,v in prediction.items()]) + \"\\n\"\n",
        "        writer.write(output_line)\n",
        "\n",
        "## dictionary mapping model name to which architecture \n",
        "## to use (BertModel is a classic BERT, BertModelModified \n",
        "## has the convs for multi-residue \"vocabulary\")\n",
        "name2model = {                          \n",
        "    \"modified_large\":BertModelModified,\n",
        "    \"modified_medium\":BertModelModified,\n",
        "    \"modified\":BertModelModified,\n",
        "    \"orig\":BertModel,\n",
        "    \"large\":BertModel\n",
        "}\n",
        "\n",
        "\n",
        "def evaluation_loop(RUN_EVAL,\n",
        "                    RUN_PREDICTION,\n",
        "                    RESTORE_MODEL_NAME,\n",
        "                    EVALUATE_WHILE_PREDICT,\n",
        "                    dataset,\n",
        "                    MODEL,\n",
        "                    total_metrics,\n",
        "                    current_ckpt,\n",
        "                    DATA_SEQ_LENGTH,\n",
        "                    current_data_folder_eval,\n",
        "                    BERT_GCS_DIR,\n",
        "                    DATA_GCS_DIR_EVAL,\n",
        "                    USING_SHARDS,\n",
        "                    GCS_PREDICTIONS_DIR,\n",
        "                    GCS_LOGGING_DIR,\n",
        "                    LOCAL_LOGGING_DIR,\n",
        "                    CONFIG_FILE):\n",
        "\n",
        "  print(\"Using data from:\",DATA_GCS_DIR_EVAL)\n",
        "  if RUN_EVAL:\n",
        "    if dataset==\"dev\":\n",
        "      try:\n",
        "        data_path_eval = \"/content/drive/My Drive/\"+current_data_folder_eval+\"/dev.tsv\"\n",
        "        lines = open(data_path_eval).read().split(\"\\n\")\n",
        "        EVAL_STEPS = int(len(lines)/EVAL_BATCH_SIZE)\n",
        "      except:\n",
        "        def steps_getter(input_files):\n",
        "          tot_sequences = 0\n",
        "          for input_file in input_files:\n",
        "            print(\"reading:\",input_file)\n",
        "\n",
        "            d = tf.data.TFRecordDataset(input_file)\n",
        "\n",
        "            with tf.Session() as sess:\n",
        "              tot_sequences+=sess.run(d.reduce(0, lambda x,_: x+1))\n",
        "\n",
        "          return tot_sequences\n",
        "        SEQUENCES_PER_EPOCH = steps_getter([DATA_GCS_DIR_EVAL+\"/eval.tf_record\"])\n",
        "        EVAL_STEPS = int(SEQUENCES_PER_EPOCH/EVAL_BATCH_SIZE)\n",
        "    else:\n",
        "      def steps_getter(input_files):\n",
        "        tot_sequences = 0\n",
        "        for input_file in input_files:\n",
        "          print(\"reading:\",input_file)\n",
        "\n",
        "          d = tf.data.TFRecordDataset(input_file)\n",
        "\n",
        "          with tf.Session() as sess:\n",
        "            tot_sequences+=sess.run(d.reduce(0, lambda x,_: x+1))\n",
        "\n",
        "        return tot_sequences\n",
        "      SEQUENCES_PER_EPOCH = steps_getter([DATA_GCS_DIR_EVAL+\"/test.tf_record\"])\n",
        "      EVAL_STEPS = int(SEQUENCES_PER_EPOCH/EVAL_BATCH_SIZE)\n",
        "\n",
        "  print(\"eval steps:\",EVAL_STEPS)\n",
        "\n",
        "  \n",
        "  if EVALUATE_WHILE_PREDICT:\n",
        "    cmd = \"gsutil -m rm -r \"+GCS_PREDICTIONS_DIR\n",
        "    !{cmd}\n",
        "\n",
        "\n",
        "  RESTORE_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "\n",
        "  if RUN_EVAL:\n",
        "    if RESTORE_CHECKPOINT==current_ckpt:\n",
        "      return False,None,current_ckpt\n",
        "\n",
        "  current_ckpt=RESTORE_CHECKPOINT\n",
        "\n",
        "  config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "\n",
        "  model_fn = script.model_fn_builder(\n",
        "      bert_config=config,\n",
        "      logging_dir=GCS_LOGGING_DIR,\n",
        "      num_labels=len(label_list),\n",
        "      init_checkpoint=None,\n",
        "      restore_checkpoint=RESTORE_CHECKPOINT,\n",
        "      init_learning_rate=0,\n",
        "      decay_per_step=0,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=True,\n",
        "      use_one_hot_embeddings=True,\n",
        "      bert=MODEL,\n",
        "      test_results_dir=GCS_PREDICTIONS_DIR,\n",
        "      yield_predictions=EVALUATE_WHILE_PREDICT)\n",
        "\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=BERT_GCS_DIR,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "  def max_multiple_under_value(max_value,multiple_base):\n",
        "      return int(max_value/multiple_base)\n",
        "\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=True,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=1,\n",
        "      eval_batch_size=EVAL_BATCH_SIZE,\n",
        "      predict_batch_size=MAX_BATCH_SIZE)\n",
        "    \n",
        "  eval_file = os.path.join(DATA_GCS_DIR_EVAL, evaluating_file)\n",
        "\n",
        "  eval_input_fn = script.file_based_input_fn_builder(\n",
        "        input_file=eval_file,\n",
        "        shards_folder=DATA_GCS_DIR_EVAL if USING_SHARDS else None,\n",
        "        seq_length=DATA_SEQ_LENGTH,\n",
        "        is_training=False,\n",
        "        drop_remainder=True)\n",
        "\n",
        "\n",
        "  tf.logging.info(\"***** Running evaluation *****\")\n",
        "  tf.logging.info(\"  Batch size = %d\", EVAL_BATCH_SIZE)\n",
        "\n",
        "  try:\n",
        "    if RUN_EVAL:\n",
        "      eval_metrics = estimator.evaluate(input_fn=eval_input_fn, steps=EVAL_STEPS)\n",
        "      print(\"\\n\\n\\n\\n\\n\\nEVAL METRICS:\")\n",
        "      for k,v in eval_metrics.items():\n",
        "        print(k+\":\",v)\n",
        "      print(\"\\n\\n\\n\\n\\n\\n\\n\")\n",
        "      if dataset == \"dev\":\n",
        "        write_metrics(eval_metrics,LOCAL_LOGGING_DIR)\n",
        "      else:\n",
        "        total_metrics[LOCAL_LOGGING_DIR] = eval_metrics\n",
        "    if RUN_PREDICTION:\n",
        "      result=estimator.predict(input_fn=eval_input_fn)\n",
        "      if PRECISE_TESTING and dataset==\"test\":\n",
        "        run_config_trailing = tf.contrib.tpu.RunConfig(\n",
        "          cluster=tpu_cluster_resolver,\n",
        "          model_dir=BERT_GCS_DIR,\n",
        "          tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "              num_shards=1,\n",
        "              per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "        estimator_trailing = tf.contrib.tpu.TPUEstimator(\n",
        "            use_tpu=True,\n",
        "            model_fn=model_fn,\n",
        "            config=run_config_trailing,\n",
        "            train_batch_size=1,\n",
        "            predict_batch_size=1)\n",
        "        test_file_trailing = os.path.join(DATA_GCS_DIR_EVAL, \"test_trailing.tf_record\")\n",
        "        test_input_fn_trailing = script.file_based_input_fn_builder(\n",
        "            input_file=test_file_trailing,\n",
        "            seq_length=DATA_SEQ_LENGTH,\n",
        "            is_training=False,\n",
        "            drop_remainder=True)\n",
        "        result_trailing=estimator_trailing.predict(input_fn=test_input_fn_trailing)\n",
        "      else:\n",
        "        result_trailing = None\n",
        "      write_predictions(PREDICTIONS_FOLDER,\n",
        "                        RESTORE_MODEL_NAME,\n",
        "                        result,\n",
        "                        result_trailing)\n",
        "    return True,total_metrics,current_ckpt\n",
        "  except Exception as e:\n",
        "    print(\"FAILED:\",e)\n",
        "    return False,None,current_ckpt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgI2CyoFPjyY"
      },
      "source": [
        "###Model/Sequence Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "source": [
        "#@markdown whether or not to run evaluation\n",
        "RUN_EVAL = False #@param {type:\"boolean\"}\n",
        "#@markdown whether or not to run prediction in a seperate loop from evaluation (if using EVALUATE_WHILE_PREDICT, set to False)\n",
        "RUN_PREDICTION = True #@param {type:\"boolean\"}\n",
        "#@markdown if evaluating, whether or not to evaluate and write test results in the same loop; useful when amount of test data is very small and the time it takes to restart a loop is siginificant (the evalution loop itself will be slower due to writing tfevents) (if yes, prediction results will be written in the form of tfevent files into GCS, so use the notebook titled \"mutformer processing and viewing finetuning results\" to view them)\n",
        "EVALUATE_WHILE_PREDICT =  False #@param {type:\"boolean\"}\n",
        "#@markdown whether or not testing/evaluating data was generated in shards (for really large databases)\n",
        "USING_SHARDS = True #@param {type:\"boolean\"}\n",
        "#@markdown what folder to write predictions into (if using EVALUATE_WHILE_PREDICT, predictions will be written into this folder under GCS, otherwise predictions will be written to this folder under google drive)\n",
        "PREDICTIONS_FOLDER = \"mrpc_loss_spam_model_comparison_final_predictions\" #@param {type:\"string\"}\n",
        "#@markdown #####Note: If using EVALUATE_WHILE_PREDICT, prediction results must be read using the previously mentioned colab notebook, otherwise, predictions will be written directly as txts and will be directly accessible from google drive under the folder specified above\n",
        "#@markdown \\\n",
        "#@markdown ###### whether to evaluate/predict on the test set or the dev set (\"test\" or \"dev\") (test set will only run once, dev set will run continuously)\n",
        "dataset = \"test\" #@param{type:\"string\"}\n",
        "#@markdown ###### if using test set, which model ids to evaluate (eval set will only run on the active model, test will run on specified models)\n",
        "models = [\"orig\",\"large\",\"modified\"] #@param\n",
        "#@markdown ###### if using test set, which sequence lengthed models to evaluate\n",
        "lengths = [256,512,1024] #@param\n",
        "\n",
        "if dataset==\"test\":\n",
        "  evaluating_file = \"test.tf_record\"\n",
        "  total_metrics = {}\n",
        "elif dataset==\"dev\":\n",
        "  evaluating_file = \"eval.tf_record\"\n",
        "else:\n",
        "  raise Exception(\"only datasets supported are dev and test\")\n",
        "\n",
        "current_ckpt = \"N/A\"\n",
        "\n",
        "while True:\n",
        "  for MODEL_NAME in models:\n",
        "    for DATA_SEQ_LENGTH in lengths:\n",
        "      if dataset == \"dev\":\n",
        "        try:\n",
        "          ##reading the identifiers from drive written by the training script to know what to evaluate\n",
        "          MODEL_NAME = open(FILES_PATH+\"/finetuning_run_paired_model.txt\").read()\n",
        "          DATA_SEQ_LENGTH = int(open(FILES_PATH+\"/finetuning_run_paired_seq_length.txt\").read())\n",
        "        except:\n",
        "          print(\"Models haven't started training yet...checking again in 60 seconds\")\n",
        "          time.sleep(60)\n",
        "          continue\n",
        "\n",
        "      print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "            \"\\nINPUT MAX SEQ LENGTH:\",DATA_SEQ_LENGTH)\n",
        "      \n",
        "      MODEL = name2model[MODEL_NAME]\n",
        "      current_data_folder_eval= data_folder_format.replace(\"xxx\",str(DATA_SEQ_LENGTH))\n",
        "      RESTORE_MODEL_NAME = MODEL_DIR_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH))\n",
        "\n",
        "      BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR_format.replace(\"xxx\",MODEL_NAME)+\"_\"+str(DATA_SEQ_LENGTH))\n",
        "      DATA_GCS_DIR_EVAL = \"{}/{}\".format(BUCKET_PATH, DATA_DIR_format.replace(\"xxx\",str(DATA_SEQ_LENGTH)))\n",
        "      \n",
        "      GCS_LOGGING_DIR = \"{}/{}\".format(BUCKET_PATH, LOGGING_DIR+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)))\n",
        "      LOCAL_LOGGING_DIR = \"{}/{}\".format(LOGGING_DIR, RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)))\n",
        "      GCS_PREDICTIONS_DIR = \"{}/{}\".format(BUCKET_PATH, PREDICTIONS_FOLDER+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)))\n",
        "\n",
        "      CONFIG_FILE = \"{}/config.json\".format(BUCKET_PATH+\"/\"+INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "\n",
        "      ##run the evaluation/prediction loop\n",
        "      sucess,total_metrics,current_ckpt = \\\n",
        "              evaluation_loop(RUN_EVAL,\n",
        "                              RUN_PREDICTION,\n",
        "                              RESTORE_MODEL_NAME,\n",
        "                              EVALUATE_WHILE_PREDICT,\n",
        "                              dataset,\n",
        "                              MODEL,\n",
        "                              total_metrics,\n",
        "                              current_ckpt,\n",
        "                              DATA_SEQ_LENGTH,\n",
        "                              current_data_folder_eval,\n",
        "                              BERT_GCS_DIR,\n",
        "                              DATA_GCS_DIR_EVAL,\n",
        "                              USING_SHARDS,\n",
        "                              GCS_PREDICTIONS_DIR,\n",
        "                              GCS_LOGGING_DIR,\n",
        "                              LOCAL_LOGGING_DIR,\n",
        "                              CONFIG_FILE)\n",
        "\n",
        "      if not sucess:\n",
        "        time.sleep(30)\n",
        "        continue\n",
        "\n",
        "      if dataset==\"dev\":\n",
        "        break\n",
        "    if dataset==\"dev\":\n",
        "      break\n",
        "  if dataset==\"test\":\n",
        "    break\n",
        "if dataset == \"test\" and RUN_EVAL:\n",
        "  for logging_dir,metrics in total_metrics.items():\n",
        "    print(\"Printing metrics for:\",logging_dir,\"\\n\")\n",
        "    for key,metric in metrics.items():\n",
        "      print(key+\":\",metric)\n",
        "    print(\"\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw3EPxIe98Lg"
      },
      "source": [
        "###Batch Size/Sequence Length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UN8Fpeb-96nA"
      },
      "source": [
        "#@markdown whether or not to run evaluation\n",
        "RUN_EVAL = False #@param {type:\"boolean\"}\n",
        "#@markdown whether or not to run prediction in a seperate loop from evaluation (if using EVALUATE_WHILE_PREDICT, set to False)\n",
        "RUN_PREDICTION = True #@param {type:\"boolean\"}\n",
        "#@markdown if evaluating, whether or not to evaluate and write test results in the same loop; useful when amount of test data is very small and the time it takes to restart a loop is siginificant (the evalution loop itself will be slower due to writing tfevents) (if yes, prediction results will be written in the form of tfevent files into GCS, so use the notebook titled \"mutformer processing and viewing finetuning results\" to view them)\n",
        "EVALUATE_WHILE_PREDICT =  False #@param {type:\"boolean\"}\n",
        "#@markdown whether or not testing/evaluating data was generated in shards (for really large databases)\n",
        "USING_SHARDS = True #@param {type:\"boolean\"}\n",
        "#@markdown what folder to write predictions into (if using EVALUATE_WHILE_PREDICT, predictions will be written into this folder under GCS, otherwise predictions will be written to this folder under google drive)\n",
        "PREDICTIONS_FOLDER = \"mrpc_loss_spam_model_comparison_final_predictions\" #@param {type:\"string\"}\n",
        "#@markdown #####Note: If using EVALUATE_WHILE_PREDICT, prediction results must be read using the previously mentioned colab notebook, otherwise, predictions will be written directly as txts and will be directly accessible from google drive under the folder specified above\n",
        "#@markdown \\\n",
        "#@markdown ###### whether to evaluate on the test set or the dev set (\"test\" or \"dev\") (test set will only run once, dev set will run continuously)\n",
        "dataset = \"test\" #@param{type:\"string\"}\n",
        "#@markdown which model id to evaluate\n",
        "MODEL_NAME=\"modified_large\" #@param {type:\"string\"}\n",
        "#@markdown ###### if using test set, which batch sized models to evaluate (eval will only run on the active model, test will run on specified models)\n",
        "batch_sizes = [32,16,64] #@param\n",
        "#@markdown ###### if using test set, which sequence lengthed models to evaluate\n",
        "lengths = [256,512,1024] #@param\n",
        "\n",
        "if dataset==\"test\":                  ## a dictionary for all metrics to \n",
        "  evaluating_file = \"test.tf_record\" ## print at the end during testing,\n",
        "  total_metrics = {}                 ## not necessary during evaluation   \n",
        "elif dataset==\"dev\":\n",
        "  evaluating_file = \"eval.tf_record\"\n",
        "  total_metrics = None\n",
        "else:\n",
        "  raise Exception(\"only datasets supported are dev and test\")\n",
        "\n",
        "current_ckpt = \"N/A\"\n",
        "\n",
        "while True:\n",
        "  for BATCH_SIZE in batch_sizes:\n",
        "    for DATA_SEQ_LENGTH in lengths:\n",
        "      if dataset == \"dev\":\n",
        "        try:\n",
        "          ##reading the identifiers from drive written by the training script to know what to evaluate\n",
        "          BATCH_SIZE = int(open(FILES_PATH+\"/finetuning_run_paired_batch_size.txt\").read())\n",
        "          DATA_SEQ_LENGTH = int(open(FILES_PATH+\"/finetuning_run_paired_seq_length.txt\").read())\n",
        "        except:\n",
        "          print(\"Models haven't started training yet...checking again in 60 seconds\")\n",
        "          time.sleep(60)\n",
        "          continue\n",
        "      print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "            \"\\nINPUT MAX SEQ LENGTH:\",DATA_SEQ_LENGTH,\n",
        "            \"\\nBATCH_SIZE_FINETUNED_ON:\",BATCH_SIZE,\"\\n\\n\\n\")\n",
        "\n",
        "      MODEL = name2model[MODEL_NAME]\n",
        "      current_data_folder_eval= data_folder_format.replace(\"xxx\",str(DATA_SEQ_LENGTH))\n",
        "      RESTORE_MODEL_NAME = MODEL_DIR_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE))\n",
        "\n",
        "      BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, RESTORE_MODEL_NAME)\n",
        "      DATA_GCS_DIR_EVAL = \"{}/{}\".format(BUCKET_PATH, DATA_DIR_format.replace(\"xxx\",str(DATA_SEQ_LENGTH)))\n",
        "      \n",
        "      GCS_PREDICTIONS_DIR = \"{}/{}\".format(BUCKET_PATH, PREDICTIONS_FOLDER+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE)))\n",
        "      GCS_LOGGING_DIR = \"{}/{}\".format(BUCKET_PATH, LOGGING_DIR+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE)))\n",
        "      LOCAL_LOGGING_DIR = \"{}/{}\".format(LOGGING_DIR, RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE)))\n",
        "\n",
        "      CONFIG_FILE = \"{}/config.json\".format(BUCKET_PATH+\"/\"+INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "      \n",
        "      ##run the evaluation/prediction loop\n",
        "      sucess,total_metrics,current_ckpt = \\\n",
        "              evaluation_loop(RUN_EVAL,\n",
        "                              RUN_PREDICTION,\n",
        "                              RESTORE_MODEL_NAME,\n",
        "                              EVALUATE_WHILE_PREDICT,\n",
        "                              dataset,\n",
        "                              MODEL,\n",
        "                              total_metrics,\n",
        "                              current_ckpt,\n",
        "                              DATA_SEQ_LENGTH,\n",
        "                              current_data_folder_eval,\n",
        "                              BERT_GCS_DIR,\n",
        "                              DATA_GCS_DIR_EVAL,\n",
        "                              USING_SHARDS,\n",
        "                              GCS_PREDICTIONS_DIR,\n",
        "                              GCS_LOGGING_DIR,\n",
        "                              LOCAL_LOGGING_DIR,\n",
        "                              CONFIG_FILE)\n",
        "\n",
        "      if not sucess:\n",
        "        time.sleep(30)\n",
        "        continue\n",
        "      if dataset==\"dev\":\n",
        "        break\n",
        "    if dataset==\"dev\":\n",
        "      break\n",
        "  if dataset==\"test\":\n",
        "    break\n",
        "if dataset == \"test\" and RUN_EVAL:\n",
        "  for logging_dir,metrics in total_metrics.items():\n",
        "    print(\"Printing metrics for:\",logging_dir,\"\\n\")\n",
        "    for key,metric in metrics.items():\n",
        "      print(key+\":\",metric)\n",
        "    print(\"\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3RKscL93Fw2"
      },
      "source": [
        "###Just one model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfcXh3N93JdG"
      },
      "source": [
        "#@markdown whether or not to run evaluation\n",
        "RUN_EVAL = True #@param {type:\"boolean\"}\n",
        "#@markdown whether or not to run prediction in a seperate loop from evaluation (if using EVALUATE_WHILE_PREDICT, set to False)\n",
        "RUN_PREDICTION = False #@param {type:\"boolean\"}\n",
        "#@markdown if evaluating, whether or not to evaluate and write test results in the same loop; useful when amount of test data is very small and the time it takes to restart a loop is siginificant (the evalution loop itself will be slower due to writing tfevents) (if yes, prediction results will be written in the form of tfevent files into GCS, so use the notebook titled \"mutformer processing and viewing finetuning results\" to view them)\n",
        "EVALUATE_WHILE_PREDICT =  False #@param {type:\"boolean\"}\n",
        "#@markdown whether or not testing/evaluating data was generated in shards (for really large databases)\n",
        "USING_SHARDS = True #@param {type:\"boolean\"}\n",
        "#@markdown what folder to write predictions into (if using EVALUATE_WHILE_PREDICT, predictions will be written into this folder under GCS, otherwise predictions will be written to this folder under google drive)\n",
        "PREDICTIONS_FOLDER = \"all_snp_prediction\" #@param {type:\"string\"}\n",
        "#@markdown #####Note: If using EVALUATE_WHILE_PREDICT, prediction results must be read using the previously mentioned colab notebook, otherwise, predictions will be written directly as txts and will be directly accessible from google drive under the folder specified above\n",
        "#@markdown \\\n",
        "#@markdown ###### whether to evaluate on the test set or the dev set (\"test\" or \"dev\") (test set will only run once, dev set will run continuously)\n",
        "dataset = \"test\" #@param{type:\"string\"}\n",
        "#@markdown which model id to evaluate\n",
        "MODEL_NAME=\"modified_medium\" #@param {type:\"string\"}\n",
        "#@markdown ###### if using test set, which sequence lengthed models to evaluate\n",
        "DATA_SEQ_LENGTH = 512 #@param\n",
        "\n",
        "if dataset==\"test\":\n",
        "  evaluating_file = \"test.tf_record\"\n",
        "  total_metrics = {}\n",
        "elif dataset==\"dev\":\n",
        "  evaluating_file = \"eval.tf_record\"\n",
        "else:\n",
        "  raise Exception(\"only datasets supported are dev and test\")\n",
        "\n",
        "current_ckpt = \"N/A\"\n",
        "\n",
        "while True:\n",
        "  print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "      \"\\nINPUT MAX SEQ LENGTH:\",DATA_SEQ_LENGTH,\n",
        "  MODEL = name2model[MODEL_NAME]\n",
        "  current_data_folder_eval= data_folder_format\n",
        "  RESTORE_MODEL_NAME = MODEL_DIR_format.replace(\"xxx\",MODEL_NAME)\n",
        "\n",
        "  BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR_format)  \n",
        "  DATA_GCS_DIR_EVAL = \"{}/{}\".format(BUCKET_PATH, DATA_DIR_format)\n",
        "\n",
        "  GCS_PREDICTIONS_DIR = \"{}/{}\".format(BUCKET_PATH, PREDICTIONS_FOLDER+\"/\"+RUN_NAME_format)\n",
        "  GCS_LOGGING_DIR = \"{}/{}\".format(BUCKET_PATH, LOGGING_DIR+\"/\"+RUN_NAME_format)\n",
        "  LOCAL_LOGGING_DIR = \"{}/{}\".format(LOGGING_DIR, RUN_NAME_format)\n",
        "\n",
        "  CONFIG_FILE = \"{}/config.json\".format(BUCKET_PATH+\"/\"+INIT_MODEL_DIR_format)\n",
        "\n",
        "  ##run the evaluation/prediction loop\n",
        "  sucess,total_metrics,current_ckpt = \\\n",
        "          evaluation_loop(RUN_EVAL,\n",
        "                          RUN_PREDICTION,\n",
        "                          RESTORE_MODEL_NAME,\n",
        "                          EVALUATE_WHILE_PREDICT,\n",
        "                          dataset,\n",
        "                          MODEL,\n",
        "                          total_metrics,\n",
        "                          current_ckpt,\n",
        "                          DATA_SEQ_LENGTH,\n",
        "                          current_data_folder_eval,\n",
        "                          BERT_GCS_DIR,\n",
        "                          DATA_GCS_DIR_EVAL,\n",
        "                          USING_SHARDS,\n",
        "                          GCS_PREDICTIONS_DIR,\n",
        "                          GCS_LOGGING_DIR,\n",
        "                          LOCAL_LOGGING_DIR,\n",
        "                          CONFIG_FILE)\n",
        "\n",
        "  if not sucess:\n",
        "    time.sleep(30)\n",
        "    continue\n",
        "\n",
        "  if dataset==\"test\":\n",
        "    break\n",
        "if dataset == \"test\" and RUN_EVAL:\n",
        "  for logging_dir,metrics in total_metrics.items():\n",
        "    print(\"Printing metrics for:\",logging_dir,\"\\n\")\n",
        "    for key,metric in metrics.items():\n",
        "      print(key+\":\",metric)\n",
        "    print(\"\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}