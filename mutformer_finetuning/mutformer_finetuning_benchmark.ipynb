{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VudE2umODMnI"
      },
      "source": [
        "#Finetuning Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzHNq3MSTTea"
      },
      "source": [
        "This notebook performs finetuning tests with varying parameters to train the best possible model.\n",
        "\n",
        "Note: If using a TPU from Google Cloud (not the Colab TPU), make sure to run this notebook on a VM with access to all GCP APIs, and make sure TPUs are enabled for the GCP project"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downgrade Python and Tensorflow \n",
        "\n",
        "(the default python version in Colab does not support Tensorflow 1.15)\n",
        "\n",
        "* **Note** that because the Python used in this notebook is not the default path, syntax highlighting most likely will not function."
      ],
      "metadata": {
        "id": "8fgfq-Cx1cKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. First, download and install Python version 3.7:"
      ],
      "metadata": {
        "id": "TeczSx7xQqSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py37_22.11.1-1-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py37\" --user"
      ],
      "metadata": {
        "id": "nFsHjOgF1b8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Then, reload the webpage (not restart runtime) to allow Colab to recognize the newly installed python\n",
        "####3. Finally, run the following commands to install tensorflow 1.15:"
      ],
      "metadata": {
        "id": "dsNWer6HQsIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow==1.15"
      ],
      "metadata": {
        "id": "WaXLRzqHQtdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "outputs": [],
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown If preferred, a GCP TPU/runtime can be used to run this notebook (instructions below)\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "#@markdown How many TPU scores the TPU has: if using colab, NUM_TPU_CORES is 8.\n",
        "NUM_TPU_CORES = 8 #@param {type:\"number\"}\n",
        "#@markdown Which mode to use (a different mode means a different finetuning task): options are:\n",
        "#@markdown * \"MRPC\" - paired sequence method\n",
        "#@markdown * \"MRPC_w_ex_data\" - paired sequence method with external data\n",
        "#@markdown * \"RE\" - single sequence method\n",
        "#@markdown * \"NER\" - single sequence per residue prediction \n",
        "#@markdown \n",
        "#@markdown You can add more modes by creating a new processor and/or a new model_fn inside of the \"mutformer_model_code\" folder downloaded from github, then changing the corresponding code snippets in the code segment named \"Authorize for GCS, Imports, and General Setup\" (also edit the dropdown below).\n",
        "MODE = \"MRPC_w_ex_data\" #@param   [\"MRPC_w_ex_data\", \"MRPC\", \"RE\", \"NER\"]   {type:\"string\"} \n",
        "             ####      ^^^^^ dropdown list for all modes ^^^^^\n",
        "#@markdown Name of the GCS bucket to use (Make sure to set this to the name of your own GCS  bucket):\n",
        "BUCKET_NAME = \"\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown Where in GCS the data needs to be loaded from (should be the same as the OUTPUT_DATA_DIR variable in the data generation script):\n",
        "PROCESSED_DATA_DIR = \"compiled_finetune_data/MRPC_ex_data_all_finetune_update_loaded\" #@param {type:\"string\"}\n",
        "#@markdown Which folder to store the logs in (the LOGGING_DIR variable can be the same across all finetuning notebooks)\n",
        "LOGGING_DIR = \"MutFormer_finetuning_newbut_try3_logs\" #@param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaNhPg7sG2DS"
      },
      "source": [
        "#If running on a GCP runtime, follow these instructions to set it up:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gicp021G3Xd"
      },
      "source": [
        "###1) Create a VM from the GCP website\n",
        "###2) Open a command prompt on your computer and perform the following steps\"\n",
        "To ssh into the VM, run:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Note: Make sure the port above matches the port below (in this case it's 8888)\n",
        "\\\n",
        "\\\n",
        "In the new command prompt that popped out, either run each of the commands below individually, or copy and paste the one liner below:\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "One command:\n",
        "```\n",
        "sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "###3) In this notebook, click the \"connect to local runtime\" option under the connect button, and copy and paste the link outputted by command prompt with \"locahost: ...\"\n",
        "###4) Finally, run this code segment, which creates a TPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbFX7QwQG67i"
      },
      "outputs": [],
      "source": [
        "GCE_PROJECT_NAME = \"\" #@param {type:\"string\"}\n",
        "TPU_ZONE = \"us-central1-f\" #@param {type:\"string\"}\n",
        "TPU_NAME = \"mutformer-tpu\" #@param {type:\"string\"}\n",
        "\n",
        "!gcloud alpha compute tpus create $TPU_NAME --accelerator-type=tpu-v2 --version=1.15.5 --zone=$TPU_ZONE ##create new TPU\n",
        "\n",
        "!gsutil iam ch serviceAccount:`gcloud alpha compute tpus describe $TPU_NAME | grep serviceAccount | cut -d' ' -f2`:admin $BUCKET_PATH && echo 'Successfully set permissions!' ##give TPU access to GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXZaQIt1SXQv"
      },
      "source": [
        "#Clone the MutFormer repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L470I2VGCLTe"
      },
      "outputs": [],
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git\n",
        "#@markdown Where to clone the repo into:\n",
        "REPO_DESTINATION_PATH = \"mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Authorize for GCS, Imports, and General Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9S4CiOh3RzFW"
      },
      "outputs": [],
      "source": [
        "if not GCP_RUNTIME:\n",
        "  def authenticate_user(): ##authentication function that uses link authentication instead of popup\n",
        "    if os.path.exists(\"/content/.config/application_default_credentials.json\"): \n",
        "      return\n",
        "    print(\"Authorize for runtime GCS:\")\n",
        "    !gcloud auth login --no-launch-browser\n",
        "    print(\"Authorize for TPU GCS:\")\n",
        "    !gcloud auth application-default login  --no-launch-browser\n",
        "  authenticate_user()\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import importlib\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "if REPO_DESTINATION_PATH == \"mutformer\":\n",
        "  if os.path.exists(\"mutformer_code\"):\n",
        "    shutil.rmtree(\"mutformer_code\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH,\"mutformer_code\")\n",
        "  REPO_DESTINATION_PATH = \"mutformer_code\"\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic  #### <<<<< if you added more modes, change these imports to import the correct processors, \n",
        "from mutformer.modeling import BertModel,BertModelModified                                        #### <<<<< correct training scripts (i.e. run_classifier and run_ner_for_pathogenic), and\n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor,MrpcWithExDataProcessor            #### <<<<< correct model classes\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor  \n",
        "\n",
        "##reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_classifier,\n",
        "                  run_ner_for_pathogenic]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "#@markdown Whether or not to write logs to a file\n",
        "DO_FILE_LOGGING = True #@param {type:\"boolean\"}\n",
        "if DO_FILE_LOGGING:\n",
        "  #@markdown * If using file logging, what path to write logs to\n",
        "  FILE_LOGGING_PATH = 'file_logging/spam.log' #@param {type:\"string\"}\n",
        "  if not os.path.exists(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1])):\n",
        "    os.makedirs(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1]))\n",
        "  fh = logging.FileHandler(FILE_LOGGING_PATH)\n",
        "  fh.setLevel(logging.INFO)\n",
        "  fh.setFormatter(formatter)\n",
        "  log.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    ##upload credentials to TPU.\n",
        "    with open(\"/content/.config/application_default_credentials.json\", 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\":      ####       vvvvv if you added more modes, change this part to set the processors and training scripts correctly vvvvv\n",
        "  processor = MrpcProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = False\n",
        "elif MODE==\"MRPC_w_ex_data\":\n",
        "  processor = MrpcWithExDataProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = True\n",
        "elif MODE==\"RE\":\n",
        "  processor = REProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = False\n",
        "elif MODE==\"NER\":\n",
        "  processor = NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "  USING_EX_DATA = False\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\",\\\"MRPC_w_ex_data\\\" \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()\n",
        "                      ####       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "# Run Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdWEQQ0vWBKU"
      },
      "source": [
        "This following section will perform finetuning tests for testing different models' performance with different parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ_lmHSa7ct_"
      },
      "source": [
        "###General definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Bpaa3Eo7h8W"
      },
      "outputs": [],
      "source": [
        "def latest_checkpoint(dir):\n",
        "  cmd = \"gsutil ls \"+dir\n",
        "  files = !{cmd}\n",
        "  for file in files:\n",
        "    if \"model.ckpt\" in file:\n",
        "      return file.replace(\".\"+file.split(\".\")[-1],\"\")\n",
        "\n",
        "def training_loop(BATCH_SIZE,\n",
        "                  RESUMING,\n",
        "                  PLANNED_TOTAL_STEPS,\n",
        "                  DECAY_PER_STEP,\n",
        "                  MAX_SEQ_LENGTH,\n",
        "                  MODEL_NAME,\n",
        "                  MODEL,\n",
        "                  INIT_CHECKPOINT_DIR,\n",
        "                  GCS_OUTPUT_MODEL_DIR,\n",
        "                  DATA_GCS_DIR,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  USING_EX_DATA,\n",
        "                  EX_DATA_NUM,\n",
        "                  GCS_LOGGING_DIR,\n",
        "                  CONFIG_FILE,\n",
        "                  FREEZING=None):\n",
        "  \n",
        "  tf.logging.info(\"Using data from: \"+DATA_GCS_DIR)\n",
        "  tf.logging.info(\"Loading model from: \"+INIT_CHECKPOINT_DIR)\n",
        "\n",
        "\n",
        "  RESTORE_CHECKPOINT = None if not RESUMING else tf.train.latest_checkpoint(GCS_OUTPUT_MODEL_DIR)\n",
        "  if not RESUMING:\n",
        "    cmd = \"gsutil -m rm -r \"+GCS_OUTPUT_MODEL_DIR\n",
        "    !{cmd}\n",
        "\n",
        "  try: \n",
        "    INIT_CHECKPOINT = tf.train.latest_checkpoint(INIT_CHECKPOINT_DIR)\n",
        "  except:\n",
        "    INIT_CHECKPOINT = latest_checkpoint(INIT_CHECKPOINT_DIR)\n",
        "  print(\"init checkpoint:\",INIT_CHECKPOINT,\"restore/save checkpont:\",RESTORE_CHECKPOINT)\n",
        "\n",
        "  config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "  if not tf.io.gfile.exists(GCS_OUTPUT_MODEL_DIR+\"/config.json\"):\n",
        "    tf.io.gfile.copy(CONFIG_FILE,GCS_OUTPUT_MODEL_DIR+\"/config.json\")\n",
        "\n",
        "  model_fn = script.model_fn_builder(\n",
        "      bert_config=config,\n",
        "      logging_dir=GCS_LOGGING_DIR,\n",
        "      num_labels=len(label_list),\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      restore_checkpoint=RESTORE_CHECKPOINT,\n",
        "      init_learning_rate=INIT_LEARNING_RATE,\n",
        "      decay_per_step=DECAY_PER_STEP,\n",
        "      num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "      use_tpu=True,\n",
        "      use_one_hot_embeddings=True,\n",
        "      bert=MODEL,\n",
        "      weight_decay=WEIGHT_DECAY,\n",
        "      epsilon=1e-6, ##epsilon is used to prevent dividing by zero\n",
        "      clip_grads=False,\n",
        "      freezing_x_layers=FREEZING,\n",
        "      using_ex_data=USING_EX_DATA)\n",
        "\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=GCS_OUTPUT_MODEL_DIR,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "      keep_checkpoint_max=KEEP_N_CHECKPOINTS_AT_A_TIME,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=True,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=BATCH_SIZE)\n",
        "  \n",
        "  train_file_name = \"train.tf_record\"\n",
        "  train_file = os.path.join(DATA_GCS_DIR, train_file_name)\n",
        "\n",
        "  if USING_SHARDS:\n",
        "    shards_folder = DATA_GCS_DIR\n",
        "    input_file = os.path.join(DATA_GCS_DIR, train_file_name)\n",
        "    import re\n",
        "    file_name = input_file.split(\"/\")[-1]\n",
        "    shards = [shards_folder + \"/\" + file for file in tf.io.gfile.listdir(shards_folder) if\n",
        "              re.match(file_name + \"_\\d+\", file)]\n",
        "    shards = sorted(shards,key=lambda shard:int(shard.split(\"_\")[-1]))[START_SHARD:]\n",
        "  else:\n",
        "    shards = [train_file]\n",
        "\n",
        "  if USING_SHARDS:\n",
        "    print(\"\\nUSING SHARDs:\")\n",
        "    for shard in shards:\n",
        "      print(shard)\n",
        "    print(\"\\n\")\n",
        "\n",
        "  tf.logging.info(\"***** Running training *****\")\n",
        "  tf.logging.info(\"  Batch size = %d\", BATCH_SIZE)\n",
        "  for n,shard in enumerate(shards):\n",
        "      train_input_fn = script.file_based_input_fn_builder(\n",
        "          input_file=shard,\n",
        "          seq_length=MAX_SEQ_LENGTH,\n",
        "          is_training=True,\n",
        "          drop_remainder=True,\n",
        "          pred_num=EX_DATA_NUM if USING_EX_DATA else None)\n",
        "      estimator.train(input_fn=train_input_fn, max_steps=PLANNED_TOTAL_STEPS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrgQPrH4kZV7"
      },
      "source": [
        "###Training Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqe2NOckb6OR"
      },
      "source": [
        "Following are two code segments for running the finetuning train loop. They correspond according to:\n",
        "  1. Model/sequence length: different model architectures will be tested using a fixed batch size on data of varying sequence lengths \\\n",
        "  2. Freezing/batch sie: the same model architecture is tested based on varying batch sizes and number of layers being frozen during training\n",
        "\n",
        "Note: One may write more training loops for more tests based on a similar format to these two example training loops below, i.e. batch size/sequence length.\n",
        "\\\n",
        "\\\n",
        "Note: During training, evaluation results on the training dataset will be written into GCS. To view these results, use the colab notebook titled \"mutformer processing and viewing finetuning results.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AjnVKSMlVXz"
      },
      "source": [
        "####Model/sequence length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "outputs": [],
      "source": [
        "#@markdown ### IO config\n",
        "#@markdown Folder in GCS where the pretrained models needs to be loaded from:\n",
        "INIT_MODEL_DIR = \"\" #@param {type:\"string\"}\n",
        "#@markdown Folder for where to save the finetuned models:\n",
        "OUTPUT_MODEL_DIR = \"\" #@param {type:\"string\"}\n",
        "#@markdown Which folder inside of LOGGING_DIR to store the logs in\n",
        "RUN_NAME = \"\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ### Training procedure config\n",
        "#@markdown Batch size to use\n",
        "BATCH_SIZE =  16#@param {type:\"integer\"}\n",
        "#@markdown The training loop will loop through a list of pretrained models and a list of sequence lengths, training a model for each combination of pretrained model and sequence length\n",
        "#@markdown * List of pretrained models to load (should indicate the names of the model folders inside the specified INIT_MODEL_DIR\n",
        "MODELS =  [\"MutFormer_em_adap8L\"]#@param\n",
        "#@markdown * List of model architectures for each model in the \"MODELS\" list defined in the entry above: each position in this list must correctly indicate the model architecture of its corresponding model folder in the list \"MODELS\" (BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture without integrated convs, MutFormer_embedded_convs indicates MutFormer with integrated convolutions).\n",
        "MODEL_ARCHITECTURES = [\"MutFormer_embedded_convs\"] #@param\n",
        "#@markdown * List of sequence lengths to test\n",
        "MAX_SEQ_LENGTHS = [1024] #@param\n",
        "#@markdown Whether or not to resume training from a previous checkpoint; if no, always train from scratch\n",
        "RESUMING = False #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not data was generated in shards (for really large databases)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown If training data was generated in shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Training uses a linear learning rate.\n",
        "#@markdown * Start learning rate: training will start with this learning rate on the step that learning rate warmup is complete\n",
        "INIT_LEARNING_RATE =  2e-6 #@param {type:\"number\"}\n",
        "#@markdown * End learning rate: training will alter the learning rate every step linearly so that it finishes with this learning rate on the last step.\n",
        "END_LEARNING_RATE = 3e-8 #@param {type:\"number\"}\n",
        "#@markdown How many steps during training to perform learning rate warmup for (start from learning rate 0 and increase linearly to INIT_LEARNING_RATE): Set to 0 for no warmup.\n",
        "NUM_WARMUP_STEPS =  0#@param {type:\"integer\"}\n",
        "#@markdown What weight decay value to use (MutFormer uses 0.01; a higher weight decay is more resistant to exploding gradients, but also limits the model's ability to learn)\n",
        "WEIGHT_DECAY = 0.01 #@param {type:\"number\"}\n",
        "#@markdown Save a checkpoint every this amount of steps:\n",
        "SAVE_CHECKPOINTS_STEPS =   1000#@param {type:\"integer\"}\n",
        "#@markdown TPUEstimator will keep this number of checkpoints at a time; older checkpoints will all be deleted:\n",
        "KEEP_N_CHECKPOINTS_AT_A_TIME =  100#@param {type:\"integer\"}\n",
        "#@markdown Stopping condition for training can be set by either a certain number of sequences or a certain number of steps. from below, PLANNED_TOTAL_STEPS will override PLANNED_TOTAL_SEQUENCES_SEEN; therefore, if using PLANNED_TOTAL_SEQUENCES_SEEN, set PLANNED_TOTAL_STEPS to -1.\n",
        "#@markdown \n",
        "#@markdown * Option 1: How many sequences the model should train on before stopping:\n",
        "PLANNED_TOTAL_SEQUENCES_SEEN =  2e5 #@param {type:\"number\"}\n",
        "#@markdown * Option 2: How many steps the model should train for before stopping (number of total sequences trained on will depend on the batch size used).\n",
        "PLANNED_TOTAL_STEPS =  14000#@param {type:\"number\"}\n",
        "\n",
        "\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS != -1 else PLANNED_TOTAL_SEQUENCES_SEEN//BATCH_SIZE\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/(PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS!=-1 else PLANNED_TOTAL_SEQUENCES_SEEN/BATCH_SIZE) \n",
        "\n",
        "DATA_INFOS = [[\"N/A\" for MODEL_NAME in MODELS]            ##create an empty 2D list to store all\n",
        "              for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS]      ##the data info dictionaries\n",
        "                                                                                   \n",
        "for M,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "  for m,MODEL_NAME in enumerate(MODELS):\n",
        "    print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "          \"\\nINPUT MAX SEQ LENGTH:\",MAX_SEQ_LENGTH)\n",
        "\n",
        "\n",
        "    MODEL = getattr(modeling, MODEL_ARCHITECTURES[m])\n",
        "    INIT_CHECKPOINT_DIR = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME\n",
        "    GCS_OUTPUT_MODEL_DIR = BUCKET_PATH+\"/\"+OUTPUT_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "    DATA_GCS_DIR = BUCKET_PATH+\"/\"+PROCESSED_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "    \n",
        "    GCS_LOGGING_DIR = BUCKET_PATH+\"/\"+LOGGING_DIR+\"/\"+RUN_NAME+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "    CONFIG_FILE = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME+\"/config.json\"\n",
        "    \n",
        "    if DATA_INFOS[M][m] == \"N/A\":\n",
        "      DATA_INFOS[M][m] = json.load(tf.gfile.Open(DATA_GCS_DIR+\"/info.json\"))\n",
        "    \n",
        "    EX_DATA_NUM = DATA_INFOS[M][m][\"ex_data_num\"] if USING_EX_DATA else 0\n",
        "    \n",
        "    training_loop(BATCH_SIZE,\n",
        "                  RESUMING,\n",
        "                  PLANNED_TOTAL_STEPS,\n",
        "                  DECAY_PER_STEP,\n",
        "                  MAX_SEQ_LENGTH,\n",
        "                  MODEL_NAME,\n",
        "                  MODEL,\n",
        "                  INIT_CHECKPOINT_DIR,\n",
        "                  GCS_OUTPUT_MODEL_DIR,\n",
        "                  DATA_GCS_DIR,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  USING_EX_DATA,\n",
        "                  EX_DATA_NUM,\n",
        "                  GCS_LOGGING_DIR,\n",
        "                  CONFIG_FILE)\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP6CPmRoIWOm"
      },
      "source": [
        "####Freezing/Batch Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOuxQ5NeGM4v"
      },
      "outputs": [],
      "source": [
        "#@markdown ### IO config\n",
        "#@markdown Folder in GCS where the pretrained models needs to be loaded from:\n",
        "INIT_MODEL_DIR = \"\" #@param {type:\"string\"}\n",
        "#@markdown Folder for where to save the finetuned models:\n",
        "OUTPUT_MODEL_DIR = \"\" #@param {type:\"string\"}\n",
        "#@markdown Which folder inside of LOGGING_DIR to store the logs in\n",
        "RUN_NAME = \"\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ### Training procedure config\n",
        "FREEZINGS = [8,6,5] #@param\n",
        "#@markdown Batch size to use\n",
        "BATCH_SIZES =  [16,64] #@param\n",
        "#@markdown The training loop will loop through a list of pretrained models and a list of sequence lengths, training a model for each combination of pretrained model and sequence length\n",
        "#@markdown * Model Name to use (should indicate the name of a model folder inside the specified INIT_MODEL_DIR\n",
        "MODEL_NAME =  \"MutFormer_em_adap8L\" #@param {type:\"string\"}\n",
        "#@markdown * Model architecture to use. Must correctly correspond to the model indicated by the model folder specified by the above \"MODEL_NAME\" parameter (BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture without integrated convs, MutFormer_embedded_convs indicates MutFormer with integrated convolutions).\n",
        "MODEL_ARCHITECTURE = \"MutFormer_embedded_convs\" #@param\n",
        "#@markdown * List of sequence lengths to test\n",
        "MAX_SEQ_LENGTH = 512 #@param\n",
        "#@markdown Whether or not to resume training from a previous checkpoint; if no, always train from scratch\n",
        "RESUMING = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not data was generated in shards (for really large databases)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown If training data was generated in shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Training uses a linear learning rate.\n",
        "#@markdown * Start learning rate: training will start with this learning rate on the step that learning rate warmup is complete\n",
        "INIT_LEARNING_RATE =  1e-5 #@param {type:\"number\"}\n",
        "#@markdown * End learning rate: training will alter the learning rate every step linearly so that it finishes with this learning rate on the last step.\n",
        "END_LEARNING_RATE = 3e-9 #@param {type:\"number\"}\n",
        "#@markdown How many steps during training to perform learning rate warmup for (start from learning rate 0 and increase linearly to INIT_LEARNING_RATE): Set to 0 for no warmup.\n",
        "NUM_WARMUP_STEPS =  0#@param {type:\"integer\"}\n",
        "#@markdown What weight decay value to use (MutFormer uses 0.01; a higher weight decay is more resistant to exploding gradients, but also limits the model's ability to learn)\n",
        "WEIGHT_DECAY = 0.01 #@param {type:\"number\"}\n",
        "#@markdown Save a checkpoint every this amount of steps:\n",
        "SAVE_CHECKPOINTS_STEPS =   1000#@param {type:\"integer\"}\n",
        "#@markdown TPUEstimator will keep this number of checkpoints at a time; older checkpoints outside this range will all be deleted:\n",
        "KEEP_N_CHECKPOINTS_AT_A_TIME =  100#@param {type:\"integer\"}\n",
        "#@markdown Stopping condition for training can be set by either a certain number of sequences or a certain number of steps. from below, PLANNED_TOTAL_STEPS will override PLANNED_TOTAL_SEQUENCES_SEEN; therefore, if using PLANNED_TOTAL_SEQUENCES_SEEN, set PLANNED_TOTAL_STEPS to -1.\n",
        "#@markdown \n",
        "#@markdown * Option 1: How many sequences the model should train on before stopping:\n",
        "PLANNED_TOTAL_SEQUENCES_SEEN =  2e5 #@param {type:\"number\"}\n",
        "#@markdown * Option 2: How many steps the model should train for before stopping (number of total sequences trained on will depend on the batch size used).\n",
        "PLANNED_TOTAL_STEPS =  14000#@param {type:\"number\"}\n",
        "\n",
        "\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS != -1 else PLANNED_TOTAL_SEQUENCES_SEEN//BATCH_SIZE\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/(PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS!=-1 else PLANNED_TOTAL_SEQUENCES_SEEN/BATCH_SIZE) \n",
        "\n",
        "DATA_INFOS = [[\"N/A\" for BATCH_SIZE in BATCH_SIZES]            ##create an empty 2D list to store all\n",
        "              for FREEZING in FREEZINGS]      ##the data info dictionaries\n",
        "                                                                                   \n",
        "for M,FREEZING in enumerate(FREEZINGS):\n",
        "  for m,BATCH_SIZE in enumerate(BATCH_SIZES):\n",
        "    print(\"\\n\\n\\nFreezing layers:\",FREEZING,\n",
        "          \"\\nBATCH SIZE:\",BATCH_SIZE)\n",
        "\n",
        "\n",
        "    MODEL = getattr(modeling, MODEL_ARCHITECTURE)\n",
        "    INIT_CHECKPOINT_DIR = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME\n",
        "    GCS_OUTPUT_MODEL_DIR = BUCKET_PATH+\"/\"+OUTPUT_MODEL_DIR+\"/fl_\"+str(FREEZING)+\"_bs_\"+str(BATCH_SIZE)\n",
        "    DATA_GCS_DIR = BUCKET_PATH+\"/\"+PROCESSED_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "    \n",
        "    GCS_LOGGING_DIR = BUCKET_PATH+\"/\"+LOGGING_DIR+\"/\"+RUN_NAME+\"/fl_\"+str(FREEZING)+\"_bs_\"+str(BATCH_SIZE)\n",
        "\n",
        "    CONFIG_FILE = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME+\"/config.json\"\n",
        "    \n",
        "    if DATA_INFOS[M][m] == \"N/A\":\n",
        "      DATA_INFOS[M][m] = json.load(tf.gfile.Open(DATA_GCS_DIR+\"/info.json\"))\n",
        "    \n",
        "    EX_DATA_NUM = DATA_INFOS[M][m][\"ex_data_num\"] if USING_EX_DATA else 0\n",
        "    \n",
        "    training_loop(BATCH_SIZE,\n",
        "                  RESUMING,\n",
        "                  PLANNED_TOTAL_STEPS,\n",
        "                  DECAY_PER_STEP,\n",
        "                  MAX_SEQ_LENGTH,\n",
        "                  MODEL_NAME,\n",
        "                  MODEL,\n",
        "                  INIT_CHECKPOINT_DIR,\n",
        "                  GCS_OUTPUT_MODEL_DIR,\n",
        "                  DATA_GCS_DIR,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  USING_EX_DATA,\n",
        "                  EX_DATA_NUM,\n",
        "                  GCS_LOGGING_DIR,\n",
        "                  CONFIG_FILE,\n",
        "                  FREEZING=FREEZING,)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "SaNhPg7sG2DS"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "name": "py37",
      "display_name": "Python 3.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}