{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "mutformer_finetuning_benchmark.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SaNhPg7sG2DS",
        "9AjnVKSMlVXz",
        "WQ322RGr5ykF"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Finetuning Script"
      ],
      "metadata": {
        "id": "VudE2umODMnI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzHNq3MSTTea"
      },
      "source": [
        "This notebook performs finetuning with varying models, batch sizes, and sequence lengths in order to find the best model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown If preferred, a GCP TPU/runtime can be used to run this notebook (instructions below)\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "#@markdown How many TPU scores the TPU has: if using colab, NUM_TPU_CORES is 8.\n",
        "NUM_TPU_CORES = 8 #@param {type:\"number\"}\n",
        "#@markdown Which mode to use (a different mode means a different finetuning task): options are:\n",
        "#@markdown * \"MRPC\" - paired sequence method\n",
        "#@markdown * \"MRPC_w_ex_data\" - paired sequence method with external data\n",
        "#@markdown * \"RE\" - single sequence method\n",
        "#@markdown * \"NER\" - single sequence per residue prediction \n",
        "#@markdown \n",
        "#@markdown You can add more modes by creating a new processor and/or a new model_fn inside of the \"mutformer_model_code\" folder downloaded from github, then changing the corresponding code snippets in the code segment named \"Authorize for GCS, Imports, and General Setup\" (also edit the dropdown below).\n",
        "MODE = \"RE\" #@param   [\"MRPC_w_ex_data\", \"MRPC\", \"RE\", \"NER\"]   {type:\"string\"} \n",
        "             ####      ^^^^^ dropdown list for all modes ^^^^^\n",
        "#@markdown Name of the GCS bucket to use:\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown Where in GCS the data needs to be loaded from (should be the same as the OUTPUT_DATA_DIR variable in the data generation script):\n",
        "PROCESSED_DATA_DIR = \"RE_finetune_update_loaded\" #@param {type:\"string\"}\n",
        "#@markdown Which folder to store the logs in (the LOGGING_DIR variable can be the same across all finetuning notebooks)\n",
        "LOGGING_DIR = \"MutFormer_finetuning_updated_logs\" #@param {type:\"string\"}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaNhPg7sG2DS"
      },
      "source": [
        "#If running on a GCP runtime, follow these instructions to set it up:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gicp021G3Xd"
      },
      "source": [
        "###1) Create a VM from the GCP website\n",
        "###2) Open a command prompt on your computer and perform the following steps\"\n",
        "To ssh into the VM, run:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Note: Make sure the port above matches the port below (in this case it's 8888)\n",
        "\\\n",
        "\\\n",
        "In the new command prompt that popped out, either run each of the commands below individually, or copy and paste the one liner below:\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "One command:\n",
        "```\n",
        "sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "###3) In this notebook, click the \"connect to local runtime\" option under the connect button, and copy and paste the link outputted by command prompt with \"locahost: ...\"\n",
        "###4) Finally, run this code segment, which creates a TPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbFX7QwQG67i"
      },
      "source": [
        "GCE_PROJECT_NAME = \"genome-project-319100\" #@param {type:\"string\"}\n",
        "TPU_ZONE = \"us-central1-f\" #@param {type:\"string\"}\n",
        "TPU_NAME = \"mutformer-tpu\" #@param {type:\"string\"}\n",
        "\n",
        "!gcloud alpha compute tpus create $TPU_NAME --accelerator-type=tpu-v2 --version=1.15.5 --zone=$TPU_ZONE ##create new TPU\n",
        "\n",
        "!gsutil iam ch serviceAccount:`gcloud alpha compute tpus describe $TPU_NAME | grep serviceAccount | cut -d' ' -f2`:admin gs://theodore_jiang && echo 'Successfully set permissions!' ##give TPU access to GCS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXZaQIt1SXQv"
      },
      "source": [
        "#Clone the MutFormer repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L470I2VGCLTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b00bb80-c215-4d95-c2b9-2c11f7713eb2"
      },
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git\n",
        "#@markdown Where to clone the repo into:\n",
        "REPO_DESTINATION_PATH = \"mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mutformer'...\n",
            "remote: Enumerating objects: 614, done.\u001b[K\n",
            "remote: Counting objects: 100% (415/415), done.\u001b[K\n",
            "remote: Compressing objects: 100% (335/335), done.\u001b[K\n",
            "remote: Total 614 (delta 299), reused 111 (delta 78), pack-reused 199\u001b[K\n",
            "Receiving objects: 100% (614/614), 2.14 MiB | 12.87 MiB/s, done.\n",
            "Resolving deltas: 100% (410/410), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Authorize for GCS, Imports, and General Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df249b7d-041a-4e64-b8a8-dc478ba17f99"
      },
      "source": [
        "if not GCP_RUNTIME:\n",
        "  from google.colab import auth\n",
        "  print(\"Authorize for GCS:\")\n",
        "  auth.authenticate_user()\n",
        "  print(\"Authorize done\")\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow.compat.v1 as tf\n",
        "import time\n",
        "import importlib\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "if REPO_DESTINATION_PATH == \"mutformer\":\n",
        "  if os.path.exists(\"mutformer_code\"):\n",
        "    shutil.rmtree(\"mutformer_code\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH,\"mutformer_code\")\n",
        "  REPO_DESTINATION_PATH = \"mutformer_code\"\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic  #### <<<<< if you added more modes, change these imports to import the correct processors, \n",
        "from mutformer.modeling import BertModel,BertModelModified                                        #### <<<<< correct training scripts (i.e. run_classifier and run_ner_for_pathogenic), and\n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor,MrpcWithExDataProcessor            #### <<<<< correct model classes\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor  \n",
        "\n",
        "##reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_classifier,\n",
        "                  run_ner_for_pathogenic]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "#@markdown Whether or not to write logs to a file\n",
        "DO_FILE_LOGGING = True #@param {type:\"boolean\"}\n",
        "if DO_FILE_LOGGING:\n",
        "  #@markdown * If using file logging, what path to write logs to\n",
        "  FILE_LOGGING_PATH = 'file_logging/spam.log' #@param {type:\"string\"}\n",
        "  if not os.path.exists(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1])):\n",
        "    os.makedirs(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1]))\n",
        "  fh = logging.FileHandler(FILE_LOGGING_PATH)\n",
        "  fh.setLevel(logging.INFO)\n",
        "  fh.setFormatter(formatter)\n",
        "  log.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    ##upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\":      ####       vvvvv if you added more modes, change this part to set the processors and training scripts correctly vvvvv\n",
        "  processor = MrpcProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = False\n",
        "elif MODE==\"MRPC_w_ex_data\":\n",
        "  processor = MrpcWithExDataProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = True\n",
        "elif MODE==\"RE\":\n",
        "  processor = REProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = False\n",
        "elif MODE==\"NER\":\n",
        "  processor = NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "  USING_EX_DATA = False\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\",\\\"MRPC_w_ex_data\\\" \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()\n",
        "                      ####       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Authorize for GCS:\n",
            "Authorize done\n",
            "WARNING:tensorflow:From /content/mutformer/optimization.py:82: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-01-07 03:33:47,372 - tensorflow - INFO - Using TPU runtime\n",
            "2022-01-07 03:33:47,396 - tensorflow - INFO - TPU address is grpc://10.93.7.66:8470\n",
            "2022-01-07 03:33:47,398 - tensorflow - WARNING - \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "# Run Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This following section will perform finetuning tests for testing different models' performance with different parameters."
      ],
      "metadata": {
        "id": "pdWEQQ0vWBKU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ_lmHSa7ct_"
      },
      "source": [
        "###General definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bpaa3Eo7h8W"
      },
      "source": [
        "def latest_checkpoint(dir):\n",
        "  cmd = \"gsutil ls \"+dir\n",
        "  files = !{cmd}\n",
        "  for file in files:\n",
        "    if \"model.ckpt\" in file:\n",
        "      return file.replace(\".\"+file.split(\".\")[-1],\"\")\n",
        "\n",
        "def training_loop(BATCH_SIZE,\n",
        "                  RESUMING,\n",
        "                  PLANNED_TOTAL_STEPS,\n",
        "                  DECAY_PER_STEP,\n",
        "                  MAX_SEQ_LENGTH,\n",
        "                  MODEL_NAME,\n",
        "                  MODEL,\n",
        "                  INIT_CHECKPOINT_DIR,\n",
        "                  GCS_OUTPUT_MODEL_DIR,\n",
        "                  DATA_GCS_DIR,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  USING_EX_DATA,\n",
        "                  EX_DATA_NUM,\n",
        "                  GCS_LOGGING_DIR,\n",
        "                  CONFIG_FILE):\n",
        "  \n",
        "  RESTORE_CHECKPOINT = None if not RESUMING else tf.train.latest_checkpoint(GCS_OUTPUT_MODEL_DIR)\n",
        "  if not RESUMING:\n",
        "    cmd = \"gsutil -m rm -r \"+GCS_OUTPUT_MODEL_DIR\n",
        "    !{cmd}\n",
        "  tf.logging.info(\"Using data from: \"+DATA_GCS_DIR)\n",
        "\n",
        "  try: \n",
        "    INIT_CHECKPOINT = tf.train.latest_checkpoint(INIT_CHECKPOINT_DIR)\n",
        "  except:\n",
        "    INIT_CHECKPOINT = latest_checkpoint(INIT_CHECKPOINT_DIR)\n",
        "  tf.logging.info(\"init checkpoint:\"+str(INIT_CHECKPOINT)+\", restore/save checkpont:\"+str(RESTORE_CHECKPOINT))\n",
        "\n",
        "  config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "  if not tf.io.gfile.exists(GCS_OUTPUT_MODEL_DIR+\"/config.json\"):\n",
        "    tf.io.gfile.copy(CONFIG_FILE,GCS_OUTPUT_MODEL_DIR+\"/config.json\")\n",
        "\n",
        "  model_fn = script.model_fn_builder(\n",
        "      bert_config=config,\n",
        "      logging_dir=GCS_LOGGING_DIR,\n",
        "      num_labels=len(label_list),\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      restore_checkpoint=RESTORE_CHECKPOINT,\n",
        "      init_learning_rate=INIT_LEARNING_RATE,\n",
        "      decay_per_step=DECAY_PER_STEP,\n",
        "      num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "      use_tpu=True,\n",
        "      use_one_hot_embeddings=True,\n",
        "      bert=MODEL,\n",
        "      weight_decay=WEIGHT_DECAY,\n",
        "      epsilon=1e-6, ##epsilon is used to prevent dividing by zero\n",
        "      clip_grads=False,\n",
        "      using_ex_data=USING_EX_DATA)\n",
        "\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=GCS_OUTPUT_MODEL_DIR,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "      keep_checkpoint_max=KEEP_N_CHECKPOINTS_AT_A_TIME,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=True,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=BATCH_SIZE)\n",
        "  \n",
        "  train_file_name = \"train.tf_record\"\n",
        "  train_file = os.path.join(DATA_GCS_DIR, train_file_name)\n",
        "\n",
        "  if USING_SHARDS:\n",
        "    shards_folder = DATA_GCS_DIR\n",
        "    input_file = os.path.join(DATA_GCS_DIR, train_file_name)\n",
        "    import re\n",
        "    file_name = input_file.split(\"/\")[-1]\n",
        "    shards = [shards_folder + \"/\" + file for file in tf.io.gfile.listdir(shards_folder) if\n",
        "              re.match(file_name + \"_\\d+\", file)]\n",
        "    shards = sorted(shards,key=lambda shard:int(shard.split(\"_\")[-1]))[START_SHARD:]\n",
        "  else:\n",
        "    shards = [train_file]\n",
        "\n",
        "  if USING_SHARDS:\n",
        "    tf.logging.info(\"\\nUSING SHARDs:\")\n",
        "    for shard in shards:\n",
        "      tf.logging.info(str(shard))\n",
        "    tf.logging.info(\"\\n\")\n",
        "\n",
        "  tf.logging.info(\"***** Running training *****\")\n",
        "  tf.logging.info(\"  Batch size = %d\", BATCH_SIZE)\n",
        "  for n,shard in enumerate(shards):\n",
        "      train_input_fn = script.file_based_input_fn_builder(\n",
        "          input_file=shard,\n",
        "          seq_length=MAX_SEQ_LENGTH,\n",
        "          is_training=True,\n",
        "          drop_remainder=True,\n",
        "          pred_num=EX_DATA_NUM if USING_EX_DATA else None)\n",
        "      estimator.train(input_fn=train_input_fn, max_steps=PLANNED_TOTAL_STEPS)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training Loops"
      ],
      "metadata": {
        "id": "lrgQPrH4kZV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following are three code segments to run. These options are:\n",
        "1. Model/sequence length: different model architectures will be tested using a fixed batch size on data of varying sequence lengths \\\n",
        "2. Sequence length/batch size: one model architecture will be tested using varying batch sizes on data of varying sequence lengths\\\n",
        "3. One model: one model architecture will be tested using a fixed batch size on a fixed set of data of a given sequence length\n",
        "\n",
        "Note: During training, evaluation results on the training dataset will be written into GCS. To view these results, use the colab notebook titled \"mutformer processing and viewing finetuning results.\""
      ],
      "metadata": {
        "id": "Xqe2NOckb6OR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AjnVKSMlVXz"
      },
      "source": [
        "####Model/sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "source": [
        "#@markdown ### IO config\n",
        "#@markdown Folder in GCS where the pretrained models needs to be loaded from:\n",
        "INIT_MODEL_DIR = \"pretrained_models\" #@param {type:\"string\"}\n",
        "#@markdown Folder for where to save the finetuned model\n",
        "OUTPUT_MODEL_DIR = \"bert_model_re_mn_sl_try8\" #@param {type:\"string\"}\n",
        "#@markdown Which folder inside of LOGGING_DIR to store the logs in\n",
        "RUN_NAME = \"RE_updated_mn_sl_try8\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ### Training procedure config\n",
        "#@markdown Train batch size to use:\n",
        "BATCH_SIZE =  16#@param {type:\"integer\"}\n",
        "#@markdown The training loop will loop through a list of pretrained models and a list of sequence lengths, training a model for each combination of pretrained model and sequence length\n",
        "#@markdown * List of pretrained models to load (should indicate the names of the model folders inside the specified INIT_MODEL_DIR\n",
        "MODELS = [\"MutBERT8L\",\"MutBERT10L\",\"MutFormer8L\"] #@param\n",
        "#@markdown * List of model architectures for each model in the \"MODELS\" list defined in the entry above: each position in this list must correctly indicate the model architecture of its corresponding model folder in the list \"MODELS\" (BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture).\n",
        "MODEL_ARCHITECTURES = [BertModel,BertModel,BertModelModified] #@param\n",
        "#@markdown * List of sequence lengths to test\n",
        "MAX_SEQ_LENGTHS = [512,1024,256,128,64] #@param\n",
        "#@markdown Whether or not to resume training from a previous checkpoint; if no, always train from scratch\n",
        "RESUMING = False #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not data was generated in shards (for really large databases)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown If training data was generated in shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Training uses a linear learning rate.\n",
        "#@markdown * Start learning rate: training will start with this learning rate on the step that learning rate warmup is complete\n",
        "INIT_LEARNING_RATE =  1e-5 #@param {type:\"number\"}\n",
        "#@markdown * End learning rate: training will alter the learning rate every step linearly so that it finishes with this learning rate on the last step.\n",
        "END_LEARNING_RATE = 1e-6 #@param {type:\"number\"}\n",
        "#@markdown How many steps during training to perform learning rate warmup for (start from learning rate 0 and increase to INIT_LEARNING_RATE): Set to 0 for no warmup.\n",
        "NUM_WARMUP_STEPS =  0#@param {type:\"integer\"}\n",
        "#@markdown What weight decay value to use (MutFormer uses 0.01; a higher weight decay is more resistant to exploding gradients, but also limits the model's ability to learn)\n",
        "WEIGHT_DECAY = 0.01 #@param {type:\"number\"}\n",
        "#@markdown Save a checkpoint every this amount of steps:\n",
        "SAVE_CHECKPOINTS_STEPS =   500#@param {type:\"integer\"}\n",
        "#@markdown TPUEstimator will keep this number of checkpoints at a time; older checkpoints will all be deleted:\n",
        "KEEP_N_CHECKPOINTS_AT_A_TIME =  10#@param {type:\"integer\"}\n",
        "#@markdown How many sequences should the model train on before stopping:\n",
        "PLANNED_TOTAL_SEQUENCES_SEEN =  256000 #@param {type:\"number\"}\n",
        "#@markdown How many steps should the model train for before stopping (number of total sequences trained on will depend on the batch size used). NOTE: PLANNED_TOTAL_STEPS will override PLANNED_TOTAL_SEQUENCES_SEEN; if using PLANNED_TOTAL_SEQUENCES_SEEN, set PLANNED_TOTAL_STEPS to -1 (PLANNED TOTAL STEPS will be based on the train batch size used, which can be specified later)\n",
        "PLANNED_TOTAL_STEPS =  4000#@param {type:\"number\"}\n",
        "\n",
        "\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS != -1 else PLANNED_TOTAL_SEQUENCES_SEEN//BATCH_SIZE\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/(PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS!=-1 else PLANNED_TOTAL_SEQUENCES_SEEN/BATCH_SIZE) \n",
        "\n",
        "DATA_INFOS = [[\"N/A\" for MODEL_NAME in MODELS]            ##create an empty 2D list to store all\n",
        "              for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS]      ##the data info dictionaries\n",
        "                                                                                   \n",
        "for M,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "  for m,MODEL_NAME in enumerate(MODELS):\n",
        "    tf.logging.info(\"\\n\\n\\nMODEL NAME:\"+MODEL_NAME+\n",
        "          \"\\nINPUT MAX SEQ LENGTH:\"+str(MAX_SEQ_LENGTH))\n",
        "\n",
        "\n",
        "    MODEL = MODEL_ARCHITECTURES[m]\n",
        "    INIT_CHECKPOINT_DIR = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME\n",
        "    GCS_OUTPUT_MODEL_DIR = BUCKET_PATH+\"/\"+OUTPUT_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "    DATA_GCS_DIR = BUCKET_PATH+\"/\"+PROCESSED_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "    \n",
        "    GCS_LOGGING_DIR = BUCKET_PATH+\"/\"+LOGGING_DIR+\"/\"+RUN_NAME+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "    CONFIG_FILE = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME+\"/config.json\"\n",
        "    \n",
        "    if DATA_INFOS[M][m] == \"N/A\":\n",
        "      DATA_INFOS[M][m] = json.load(tf.gfile.Open(DATA_GCS_DIR+\"/info.json\"))\n",
        "    \n",
        "    EX_DATA_NUM = DATA_INFOS[M][m][\"ex_data_num\"] if USING_EX_DATA else 0\n",
        "    \n",
        "    training_loop(BATCH_SIZE,\n",
        "                  RESUMING,\n",
        "                  PLANNED_TOTAL_STEPS,\n",
        "                  DECAY_PER_STEP,\n",
        "                  MAX_SEQ_LENGTH,\n",
        "                  MODEL_NAME,\n",
        "                  MODEL,\n",
        "                  INIT_CHECKPOINT_DIR,\n",
        "                  GCS_OUTPUT_MODEL_DIR,\n",
        "                  DATA_GCS_DIR,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  USING_EX_DATA,\n",
        "                  EX_DATA_NUM,\n",
        "                  GCS_LOGGING_DIR,\n",
        "                  CONFIG_FILE)\n",
        "  \n",
        "4  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ322RGr5ykF"
      },
      "source": [
        "####Batch size/sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IytLW0VbgOZz"
      },
      "source": [
        "#@markdown ### IO config\n",
        "#@markdown Folder in GCS where the pretrained models needs to be loaded from:\n",
        "INIT_MODEL_DIR = \"\" #@param {type:\"string\"}\n",
        "#@markdown Name of the folder to the pretrained model to load from inside INIT_MODEL_DIR\n",
        "MODEL_NAME=\"bert_model_modified_large\" #@param {type:\"string\"}\n",
        "#@markdown Model architecture to use BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture\n",
        "MODEL_ARCHITECTURE = BertModelModified #@param\n",
        "#@markdown Folder for where to save the finetuned model\n",
        "OUTPUT_MODEL_DIR = \"bert_model_mrpc_adding_preds\" #@param {type:\"string\"}\n",
        "#@markdown Which folder inside of LOGGING_DIR to store the logs in\n",
        "RUN_NAME = \"MRPC_adding_preds_w_mutformer12L\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ### Training procedure config\n",
        "#@markdown The training loop will loop through a list of batch sizes and a list of sequence lengths, training a model for each combination of batch size and sequence length\n",
        "#@markdown * List of batch sizes to test\n",
        "BATCH_SIZES = [64] #@param\n",
        "#@markdown * List of sequence lengths to test\n",
        "MAX_SEQ_LENGTHS = [1024] #@param\n",
        "#@markdown Whether or not to resume training from a previous checkpoint; if no, always train from scratch\n",
        "RESUMING = False #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not data was generated in shards (for really large databases)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown If training data was generated in shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Training uses a linear learning rate.\n",
        "#@markdown * Start learning rate: training will start with this learning rate on the step that learning rate warmup is complete\n",
        "INIT_LEARNING_RATE =  1e-5 #@param {type:\"number\"}\n",
        "#@markdown * End learning rate: training will alter the learning rate every step linearly so that it finishes with this learning rate on the last step.\n",
        "END_LEARNING_RATE = 5e-7 #@param {type:\"number\"}\n",
        "#@markdown How many steps during training to perform learning rate warmup for (start from learning rate 0 and increase to INIT_LEARNING_RATE): Set to 0 for no warmup.\n",
        "NUM_WARMUP_STEPS = 10 #@param {type:\"integer\"}\n",
        "#@markdown What weight decay value to use (MutFormer uses 0.01; a higher weight decay is more resistant to exploding gradients, but also limits the model's ability to learn)\n",
        "WEIGHT_DECAY = 0.01 #@param {type:\"number\"}\n",
        "#@markdown Save a checkpoint every this amount of steps:\n",
        "SAVE_CHECKPOINTS_STEPS =  1000 #@param {type:\"integer\"}\n",
        "#@markdown TPUEstimator will keep this number of checkpoints at a time; older checkpoints will all be deleted:\n",
        "KEEP_N_CHECKPOINTS_AT_A_TIME =  10#@param {type:\"integer\"}\n",
        "#@markdown How many sequences should the model train on before stopping:\n",
        "PLANNED_TOTAL_SEQUENCES_SEEN =  2e5 #@param {type:\"number\"}\n",
        "#@markdown How many steps should the model train for before stopping (number of total sequences trained on will depend on the batch size used). NOTE: PLANNED_TOTAL_STEPS will override PLANNED_TOTAL_SEQUENCES_SEEN; if using PLANNED_TOTAL_SEQUENCES_SEEN, set PLANNED_TOTAL_STEPS to -1 (PLANNED TOTAL STEPS will be based on the train batch size used, which can be specified later)\n",
        "PLANNED_TOTAL_STEPS = 8000 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS != -1 else PLANNED_TOTAL_SEQUENCES_SEEN//BATCH_SIZE\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/(PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS!=-1 else PLANNED_TOTAL_SEQUENCES_SEEN/BATCH_SIZE) \n",
        "\n",
        "DATA_INFOS = [[\"N/A\" for BATCH_SIZE in BATCH_SIZES]            ##create an empty 2D list to store all\n",
        "              for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS]           ##the data info dictionaries\n",
        "\n",
        "for M,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "    for B,BATCH_SIZE in enumerate(BATCH_SIZES):\n",
        "        tf.logging.info(\"\\nINPUT MAX SEQ LENGTH:\"+str(MAX_SEQ_LENGTH)+\n",
        "              \"\\nTRAIN_BATCH_SIZE:\"+str(BATCH_SIZE)+\"\\n\\n\\n\")\n",
        "       \n",
        "        MODEL = MODEL_ARCHITECTURE\n",
        "        INIT_CHECKPOINT_DIR = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME\n",
        "        GCS_OUTPUT_MODEL_DIR = BUCKET_PATH+\"/\"+OUTPUT_MODEL_DIR+\"/bs_\"+str(BATCH_SIZE)+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "        DATA_GCS_DIR = BUCKET_PATH+\"/\"+PROCESSED_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "      \n",
        "        GCS_LOGGING_DIR = BUCKET_PATH+\"/\"+LOGGING_DIR+\"/\"+RUN_NAME+\"/bs_\"+str(BATCH_SIZE)+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "        \n",
        "        CONFIG_FILE = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME+\"/config.json\"\n",
        "        \n",
        "        if DATA_INFOS[M][B] == \"N/A\":\n",
        "          DATA_INFOS[M][B] = json.load(tf.gfile.Open(DATA_GCS_DIR+\"/info.json\"))\n",
        "        \n",
        "        EX_DATA_NUM = DATA_INFOS[M][B][\"ex_data_num\"] if USING_EX_DATA else 0\n",
        "\n",
        "        training_loop(BATCH_SIZE,\n",
        "                      RESUMING,\n",
        "                      PLANNED_TOTAL_STEPS,\n",
        "                      DECAY_PER_STEP,\n",
        "                      MAX_SEQ_LENGTH,\n",
        "                      MODEL_NAME,\n",
        "                      MODEL,\n",
        "                      INIT_CHECKPOINT_DIR,\n",
        "                      GCS_OUTPUT_MODEL_DIR,\n",
        "                      DATA_GCS_DIR,\n",
        "                      USING_SHARDS,\n",
        "                      START_SHARD,\n",
        "                      USING_EX_DATA,\n",
        "                      EX_DATA_NUM,\n",
        "                      GCS_LOGGING_DIR,\n",
        "                      CONFIG_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tswiJYA_qCUq"
      },
      "source": [
        "####One model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK3tf7aWqIiR"
      },
      "source": [
        "#@markdown ### IO config\n",
        "#@markdown Folder in GCS where the pretrained models needs to be loaded from:\n",
        "INIT_MODEL_DIR = \"pretrained_models\" #@param {type:\"string\"}\n",
        "#@markdown Name of the folder to the pretrained model to load from inside INIT_MODEL_DIR\n",
        "MODEL_NAME=\"MutFormer12L\" #@param {type:\"string\"}\n",
        "#@markdown Model architecture to use BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture\n",
        "MODEL_ARCHITECTURE = BertModelModified #@param\n",
        "#@markdown Folder for where to save the finetuned model\n",
        "OUTPUT_MODEL_DIR = \"bert_model_mrpc_just_others_12L_try7\" #@param {type:\"string\"}\n",
        "#@markdown Which folder inside of LOGGING_DIR to store the logs in\n",
        "RUN_NAME = \"MRPC_just_others_12L_try7\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ### Training procedure config\n",
        "#@markdown Batch size to use\n",
        "BATCH_SIZE = 32 #@param {type:\"integer\"}\n",
        "#@markdown Maximum sequence length to use\n",
        "MAX_SEQ_LENGTH = 512 #@param {type:\"integer\"}\n",
        "#@markdown Whether or not to resume training from a previous checkpoint; if no, always train from scratch\n",
        "RESUMING = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not data was generated in shards (for really large databases)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown * If using shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Training uses a linear learning rate.\n",
        "#@markdown * Start learning rate: training will start with this learning rate on the step that learning rate warmup is complete\n",
        "INIT_LEARNING_RATE =  1e-5 #@param {type:\"number\"}\n",
        "#@markdown * End learning rate: training will alter the learning rate every step linearly so that it finishes with this learning rate on the last step.\n",
        "END_LEARNING_RATE =  1e-6#@param {type:\"number\"}\n",
        "#@markdown How many steps during training to perform learning rate warmup for (start from learning rate 0 and increase to INIT_LEARNING_RATE): Set to 0 for no warmup.\n",
        "NUM_WARMUP_STEPS = 10 #@param {type:\"integer\"}\n",
        "#@markdown What weight decay value to use (MutFormer uses 0.01; a higher weight decay is more resistant to exploding gradients, but also limits the model's ability to learn)\n",
        "WEIGHT_DECAY = 0.01 #@param {type:\"number\"}\n",
        "#@markdown Save a checkpoint every this amount of steps:\n",
        "SAVE_CHECKPOINTS_STEPS =  1000 #@param {type:\"integer\"}\n",
        "#@markdown TPUEstimator will keep this number of checkpoints at a time; older checkpoints will all be deleted:\n",
        "KEEP_N_CHECKPOINTS_AT_A_TIME =  10 #@param {type:\"integer\"}\n",
        "#@markdown How many sequences should the model train on before stopping:\n",
        "PLANNED_TOTAL_SEQUENCES_SEEN =  2e5 #@param {type:\"number\"}\n",
        "#@markdown How many steps should the model train for before stopping (number of total sequences trained on will depend on the batch size used). NOTE: PLANNED_TOTAL_STEPS will override PLANNED_TOTAL_SEQUENCES_SEEN; if using PLANNED_TOTAL_SEQUENCES_SEEN, set PLANNED_TOTAL_STEPS to -1 (PLANNED TOTAL STEPS will be based on the train batch size used, which can be specified later)\n",
        "PLANNED_TOTAL_STEPS = 10000 #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS != -1 else PLANNED_TOTAL_SEQUENCES_SEEN//BATCH_SIZE\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/(PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS!=-1 else PLANNED_TOTAL_SEQUENCES_SEEN/BATCH_SIZE) \n",
        "\n",
        "\n",
        "MODEL = MODEL_ARCHITECTURE\n",
        "INIT_CHECKPOINT_DIR = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME\n",
        "GCS_OUTPUT_MODEL_DIR = BUCKET_PATH+\"/\"+OUTPUT_MODEL_DIR\n",
        "DATA_GCS_DIR = BUCKET_PATH+\"/\"+PROCESSED_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "GCS_LOGGING_DIR = BUCKET_PATH+\"/\"+LOGGING_DIR+\"/\"+RUN_NAME\n",
        "\n",
        "CONFIG_FILE = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME+\"/config.json\"\n",
        "\n",
        "DATA_INFO = json.load(tf.gfile.Open(DATA_GCS_DIR+\"/info.json\"))   ##get the data info dictionary\n",
        "EX_DATA_NUM = DATA_INFO[\"ex_data_num\"] if USING_EX_DATA else 0\n",
        "\n",
        "training_loop(BATCH_SIZE,\n",
        "              RESUMING,\n",
        "              PLANNED_TOTAL_STEPS,\n",
        "              DECAY_PER_STEP,\n",
        "              MAX_SEQ_LENGTH,\n",
        "              MODEL_NAME,\n",
        "              MODEL,\n",
        "              INIT_CHECKPOINT_DIR,\n",
        "              GCS_OUTPUT_MODEL_DIR,\n",
        "              DATA_GCS_DIR,\n",
        "              USING_SHARDS,\n",
        "              START_SHARD,\n",
        "              USING_EX_DATA,\n",
        "              EX_DATA_NUM,\n",
        "              GCS_LOGGING_DIR,\n",
        "              CONFIG_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
