{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mutformer finetuning benchmark",
      "provenance": [],
      "collapsed_sections": [
        "9AjnVKSMlVXz"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzHNq3MSTTea"
      },
      "source": [
        "Performs finetuning with varying batch sizes, models, and sequence lengths in order to find the best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown Which task to perform: options are \"MRPC\" for paired sequence method, \"RE\" for single sequence method, or \"NER\" for single sequance per residue prediction (if you add more modes make sure to change the corresponding code segments)\n",
        "MODE = \"MRPC\" #@param {type:\"string\"}\n",
        "MAX_SEQ_LENGTH =  1024#@param {type:\"integer\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "#@markdown ###### For if multiple models fine tuned: xxx is the placeholder for the individual model identifier (if only one is being evaluated replace xx with the actual name of the model)\n",
        "#@markdown \\\n",
        "#@markdown folder for where to save the finetuned model\n",
        "MODEL_DIR_format = \"bert_model_mrpc_xxx\" #@param {type:\"string\"}\n",
        "#@markdown folder for the pretrained model\n",
        "INIT_MODEL_DIR_format = \"bert_model_xxx\" #@param {type:\"string\"}\n",
        "DATA_DIR_format = \"MRPC_xxx\" #@param {type:\"string\"}\n",
        "LOGGING_DIR = \"mrpc_loss_spam_model_comparison_final\" #@param {type:\"string\"}\n",
        "RUN_NAME_format = \"MRPC_xxx\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Training procedure config\n",
        "INIT_LEARNING_RATE =  1e-5 #@param {type:\"number\"}\n",
        "END_LEARNING_RATE = 1e-6 #@param {type:\"number\"}\n",
        "SAVE_CHECKPOINTS_STEPS =  100 #@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8 #@param {type:\"number\"}\n",
        "PLANNED_TOTAL_SEQUENCES_SEEN =  2e5 #@param {type:\"number\"}\n",
        "#@markdown PLANNED_TOTAL_STEPS will override PLANNED_TOTAL_SEQUENCES_SEEN; if using PLANNED_TOTAL_SEQUENCES_SEEN, set PLANNED_TOTAL_STEPS to -1 (PLANNED TOTAL STEPS will be based on the train batch size used)\n",
        "PLANNED_TOTAL_STEPS = 10000 #@param {type:\"number\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0TXw9GtCKz"
      },
      "source": [
        "#Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYsYBUCJMTdz"
      },
      "source": [
        "from google.colab import drive,auth\n",
        "import os\n",
        "import shutil\n",
        "!fusermount -u /content/drive\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "DRIVE_PATH = \"/content/drive/My Drive\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXZaQIt1SXQv"
      },
      "source": [
        "#Clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L470I2VGCLTe"
      },
      "source": [
        "#@markdown ######where to clone the repo into (only value that it can't be is \"mutformer\"):\n",
        "REPO_DESTINATION_PATH = \"code/mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://tianqitheodorejiang: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@github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import auth\n",
        "\n",
        "print(\"Authorize for GCS:\")\n",
        "auth.authenticate_user()\n",
        "print(\"Authorize done\")\n",
        "\n",
        "\n",
        "print(time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())))\n",
        "\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic\n",
        "from mutformer.modeling import BertModel,BertModelModified\n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor ##change this part if you add more modes--\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor      ##--\n",
        "  \n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "#@markdown ###### Whether or not to write logs to a file\n",
        "DO_FILE_LOGGING = True #@param {type:\"boolean\"}\n",
        "if DO_FILE_LOGGING:\n",
        "  #@markdown ###### If using file logging, what path to write logs to\n",
        "  FILE_LOGGING_PATH = 'file_logging/spam.log' #@param {type:\"string\"}\n",
        "  if not os.path.exists(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1])):\n",
        "    os.makedirs(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1]))\n",
        "  fh = logging.FileHandler(FILE_LOGGING_PATH)\n",
        "  fh.setLevel(logging.INFO)\n",
        "  fh.setFormatter(formatter)\n",
        "  log.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "# create formatter and add it to the handlers\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "log.handlers = [fh,ch]\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\": ##change this part if you added more modes\n",
        "  processor = MrpcProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"RE\":\n",
        "  processor = REProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"NER\":\n",
        "  processor = NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\", \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "# Run Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ_lmHSa7ct_"
      },
      "source": [
        "###General definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bpaa3Eo7h8W"
      },
      "source": [
        "BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "name2model = {\n",
        "    \"modified_large\":BertModelModified,\n",
        "    \"modified_medium\":BertModelModified,\n",
        "    \"modified\":BertModelModified,\n",
        "    \"orig\":BertModel,\n",
        "    \"large\":BertModel\n",
        "}\n",
        "\n",
        "def latest_checkpoint(dir):\n",
        "  cmd = \"gsutil ls \"+dir\n",
        "  files = !{cmd}\n",
        "  for file in files:\n",
        "    if \"model.ckpt\" in file:\n",
        "      return file.replace(\".\"+file.split(\".\")[-1],\"\")\n",
        "\n",
        "def training_loop(BATCH_SIZE,\n",
        "                  RESUMING,\n",
        "                  PLANNED_TOTAL_STEPS,\n",
        "                  DECAY_PER_STEP,\n",
        "                  DATA_SEQ_LENGTH,\n",
        "                  MODEL_NAME,\n",
        "                  MODEL,\n",
        "                  INIT_CHECKPOINT_DIR,\n",
        "                  BERT_GCS_DIR,\n",
        "                  DATA_GCS_DIR,\n",
        "                  GCS_LOGGING_DIR,\n",
        "                  CONFIG_FILE):\n",
        "  RESTORE_CHECKPOINT = None if not RESUMING else tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "  if not RESUMING:\n",
        "    cmd = \"gsutil -m rm -r \"+BERT_GCS_DIR\n",
        "    !{cmd}\n",
        "\n",
        "  ## if using a directory with only a single checkpoint and no \"checkpoint\" file, \n",
        "  ## tf.train.latest_checkpoint will not work, so get fold name manually via latest_checkpoint(dir)\n",
        "  try: \n",
        "    INIT_CHECKPOINT = tf.train.latest_checkpoint(INIT_CHECKPOINT_DIR)\n",
        "  except:\n",
        "    INIT_CHECKPOINT = latest_checkpoint(INIT_CHECKPOINT_DIR)\n",
        "  print(\"init checkpoint:\",INIT_CHECKPOINT,\"restore/save checkpont\",RESTORE_CHECKPOINT)\n",
        "\n",
        "  config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "  config.hidden_dropout_prob = 0.1\n",
        "  config.attention_probs_dropout_prob = 0.1\n",
        "\n",
        "  model_fn = script.model_fn_builder(\n",
        "      bert_config=config,\n",
        "      logging_dir=GCS_LOGGING_DIR,\n",
        "      num_labels=len(label_list),\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      restore_checkpoint=RESTORE_CHECKPOINT,\n",
        "      init_learning_rate=INIT_LEARNING_RATE,\n",
        "      decay_per_step=DECAY_PER_STEP,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=True,\n",
        "      use_one_hot_embeddings=True,\n",
        "      bert=MODEL,\n",
        "      weight_decay=0.01,\n",
        "      epsilon=1e-6,\n",
        "      clip_grads=False)\n",
        "\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=BERT_GCS_DIR,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=True,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=BATCH_SIZE)\n",
        "    \n",
        "  train_file = os.path.join(DATA_GCS_DIR, \"train.tf_record\")\n",
        "\n",
        "  tf.logging.info(\"***** Running training *****\")\n",
        "  tf.logging.info(\"  Batch size = %d\", BATCH_SIZE)\n",
        "  train_input_fn = script.file_based_input_fn_builder(\n",
        "      input_file=train_file,\n",
        "      seq_length=DATA_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  \n",
        "  ##writing data to drive so that the parallel eval script can know what to do\n",
        "  open(DRIVE_PATH+\"/finetuning_run_paired_model.txt\",\"w+\").write(MODEL_NAME)\n",
        "  open(DRIVE_PATH+\"/finetuning_run_paired_seq_length.txt\",\"w+\").write(str(DATA_SEQ_LENGTH))\n",
        "  open(DRIVE_PATH+\"/finetuning_run_paired_batch_size.txt\",\"w+\").write(str(BATCH_SIZE))\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=PLANNED_TOTAL_STEPS)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AjnVKSMlVXz"
      },
      "source": [
        "####Model/sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "source": [
        "#@markdown train batch size to use\n",
        "BATCH_SIZE=16 #@param\n",
        "#@markdown list of models to test\n",
        "models = [\"modified_medium\",\"modified_large\"] #@param\n",
        "#@markdown list of maximum sequence lengths to test\n",
        "lengths = [256,512,1024] #@param\n",
        "#@markdown whether or not to resume training from a previous finetuned checkpoint; if no, always train from pretrained model\n",
        "RESUMING = False #@param {type:\"boolean\"}\n",
        "\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS != -1 else PLANNED_TOTAL_SEQUENCES_SEEN//BATCH_SIZE\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/(PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS!=-1 else PLANNED_TOTAL_SEQUENCES_SEEN/TRAIN_BATCH_SIZE) \n",
        "\n",
        "for DATA_SEQ_LENGTH in lengths:\n",
        "  for MODEL_NAME in models:\n",
        "    print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "          \"\\nINPUT MAX SEQ LENGTH:\",DATA_SEQ_LENGTH,\n",
        "          \"\\nTRAIN_BATCH_SIZE:\",BATCH_SIZE,\"\\n\\n\\n\")\n",
        "\n",
        "    MODEL = name2model[MODEL_NAME]\n",
        "    INIT_CHECKPOINT_DIR = \"{}/{}\".format(BUCKET_PATH, INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "    BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)))\n",
        "    DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, DATA_DIR_format.replace(\"xxx\",str(DATA_SEQ_LENGTH)))\n",
        "    \n",
        "    GCS_LOGGING_DIR = \"{}/{}\".format(BUCKET_PATH, LOGGING_DIR+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)))\n",
        "\n",
        "    CONFIG_FILE = \"{}/config.json\".format(BUCKET_PATH+\"/\"+INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "\n",
        "    training_loop(BATCH_SIZE,\n",
        "                  RESUMING,\n",
        "                  PLANNED_TOTAL_STEPS,\n",
        "                  DECAY_PER_STEP,\n",
        "                  DATA_SEQ_LENGTH,\n",
        "                  MODEL_NAME,\n",
        "                  MODEL,\n",
        "                  INIT_CHECKPOINT_DIR,\n",
        "                  BERT_GCS_DIR,\n",
        "                  DATA_GCS_DIR,\n",
        "                  GCS_LOGGING_DIR,\n",
        "                  CONFIG_FILE)\n",
        "    \n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ322RGr5ykF"
      },
      "source": [
        "####Batch size/sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IytLW0VbgOZz"
      },
      "source": [
        "#@markdown list of batch sizes to test\n",
        "batch_sizes = [32,16,64] #@param\n",
        "#@markdown list of maximum sequence lengths to test\n",
        "lengths = [256,512,1024] #@param\n",
        "#@markdown model to use\n",
        "MODEL_NAME=\"modified_large\" #@param {type:\"string\"}\n",
        "#@markdown whether or not to resume training from a previous finetuned checkpoint; if no, always train from pretrained model\n",
        "RESUMING = True #@param {type:\"boolean\"}\n",
        "\n",
        "BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS != -1 else PLANNED_TOTAL_SEQUENCES_SEEN//BATCH_SIZE\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/(PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS!=-1 else PLANNED_TOTAL_SEQUENCES_SEEN/TRAIN_BATCH_SIZE) \n",
        "\n",
        "for DATA_SEQ_LENGTH in lengths:\n",
        "    for BATCH_SIZE in batch_sizes:\n",
        "        print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "              \"\\nINPUT MAX SEQ LENGTH:\",DATA_SEQ_LENGTH,\n",
        "              \"\\nTRAIN_BATCH_SIZE:\",BATCH_SIZE,\"\\n\\n\\n\")\n",
        "       \n",
        "        MODEL = name2model[MODEL_NAME]\n",
        "        INIT_CHECKPOINT_DIR = \"{}/{}\".format(BUCKET_PATH, INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "        BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE)))\n",
        "        DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, DATA_DIR_format.replace(\"xxx\",str(DATA_SEQ_LENGTH)))\n",
        "      \n",
        "        GCS_LOGGING_DIR = \"{}/{}\".format(BUCKET_PATH, LOGGING_DIR+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE)))\n",
        "        \n",
        "        CONFIG_FILE = \"{}/config.json\".format(BUCKET_PATH+\"/\"+INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "\n",
        "        training_loop(BATCH_SIZE,\n",
        "                      RESUMING,\n",
        "                      PLANNED_TOTAL_STEPS,\n",
        "                      DECAY_PER_STEP,\n",
        "                      DATA_SEQ_LENGTH,\n",
        "                      MODEL_NAME,\n",
        "                      MODEL,\n",
        "                      INIT_CHECKPOINT_DIR,\n",
        "                      BERT_GCS_DIR,\n",
        "                      DATA_GCS_DIR,\n",
        "                      GCS_LOGGING_DIR,\n",
        "                      CONFIG_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tswiJYA_qCUq"
      },
      "source": [
        "###Train a single model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK3tf7aWqIiR"
      },
      "source": [
        "#@markdown batch size to use\n",
        "BATCH_SIZE = 32 #@param\n",
        "#@markdown maximum sequence length to use\n",
        "DATA_SEQ_LENGTH = 512 #@param\n",
        "#@markdown model to use\n",
        "MODEL_NAME=\"modified\" #@param {type:\"string\"}\n",
        "#@markdown whether or not to resume training from a previous checkpoint; if no, always train from scratch\n",
        "RESUMING = False #@param {type:\"boolean\"}\n",
        "\n",
        "print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "      \"\\nINPUT MAX SEQ LENGTH:\",DATA_SEQ_LENGTH,\n",
        "      \"\\nTRAIN_BATCH_SIZE:\",BATCH_SIZE,\"\\n\\n\\n\")\n",
        "\n",
        "MODEL = name2model[MODEL_NAME]\n",
        "INIT_CHECKPOINT_DIR = \"{}/{}\".format(BUCKET_PATH, INIT_MODEL_DIR_format)\n",
        "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR_format)\n",
        "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, DATA_DIR_format)\n",
        "\n",
        "GCS_LOGGING_DIR = \"{}/{}\".format(BUCKET_PATH, LOGGING_DIR+\"/\"+RUN_NAME_format)\n",
        "\n",
        "CONFIG_FILE = \"{}/config.json\".format(BUCKET_PATH+\"/\"+INIT_MODEL_DIR_format)\n",
        "\n",
        "training_loop(BATCH_SIZE,\n",
        "              RESUMING,\n",
        "              PLANNED_TOTAL_STEPS,\n",
        "              DECAY_PER_STEP,\n",
        "              DATA_SEQ_LENGTH,\n",
        "              MODEL_NAME,\n",
        "              MODEL,\n",
        "              INIT_CHECKPOINT_DIR,\n",
        "              BERT_GCS_DIR,\n",
        "              DATA_GCS_DIR,\n",
        "              GCS_LOGGING_DIR,\n",
        "              CONFIG_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}