{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "mutformer_finetuning_benchmark.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SaNhPg7sG2DS",
        "9AjnVKSMlVXz",
        "WQ322RGr5ykF"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzHNq3MSTTea"
      },
      "source": [
        "Performs finetuning with varying batch sizes, models, and sequence lengths in order to find the best model. Note that support for running this file on a GCP TPU is not included since this file should not need more memory than google colab provides and does not require constant uptime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown If preferred, a GCP TPU/runtime can be used to run this notebook\n",
        "USE_GCP_TPU = False #@param {type:\"boolean\"}\n",
        "#@markdown Which task to perform: options are \"MRPC\" for paired sequence method, \"MRPC_w_preds\" for paired sequence method with external data, \"RE\" for single sequence method, or \"NER\" for single sequance per residue prediction (if you add more modes make sure to change the corresponding code segments)\n",
        "MODE = \"MRPC_w_preds\" #@param {type:\"string\"}\n",
        "MAX_SEQ_LENGTH =  1024#@param {type:\"integer\"}\n",
        "PROCESSES = 2 #@param {type:\"integer\"}\n",
        "BUCKET_NAME = \"theodore_jiang\" #@param {type:\"string\"}\n",
        "#@markdown ###### For if multiple models fine tuned: xxx is the placeholder for the individual model identifier (if only one is being evaluated replace xx with the actual name of the model)\n",
        "#@markdown \\\n",
        "#@markdown folder for where to save the finetuned model\n",
        "MODEL_DIR_format = \"bert_model_mrpc_adding_preds_xxx\" #@param {type:\"string\"}\n",
        "#@markdown folder for the pretrained model\n",
        "INIT_MODEL_DIR_format = \"bert_model_xxx\" #@param {type:\"string\"}\n",
        "DATA_DIR_format = \"MRPC_adding_preds_xxx\" #@param {type:\"string\"}\n",
        "RUN_NAME_format = \"MRPC_adding_preds_xxx\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Training procedure config\n",
        "INIT_LEARNING_RATE =  1e-5 #@param {type:\"number\"}\n",
        "END_LEARNING_RATE = 5e-7 #@param {type:\"number\"}\n",
        "SAVE_CHECKPOINTS_STEPS =  1000 #@param {type:\"integer\"}\n",
        "#@markdown ###### TPUEstimator will keep this number of checkpoints; older checkpoints will all be deleted\n",
        "KEEP_N_CHECKPOINTS_AT_A_TIME =  10#@param {type:\"integer\"}\n",
        "NUM_TPU_CORES = 8 #@param {type:\"number\"}\n",
        "PLANNED_TOTAL_SEQUENCES_SEEN =  2e5 #@param {type:\"number\"}\n",
        "#@markdown PLANNED_TOTAL_STEPS will override PLANNED_TOTAL_SEQUENCES_SEEN; if using PLANNED_TOTAL_SEQUENCES_SEEN, set PLANNED_TOTAL_STEPS to -1 (PLANNED TOTAL STEPS will be based on the train batch size used)\n",
        "PLANNED_TOTAL_STEPS = 8000 #@param {type:\"number\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaNhPg7sG2DS"
      },
      "source": [
        "#If running on a GCP TPU, use these commands prior to running this notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gicp021G3Xd"
      },
      "source": [
        "To ssh into the VM:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Make sure the port above matches the port below (in this case it's 8888)\n",
        "\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "\n",
        "(one command):sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "And then copy and paste the outputted link with \"locahost: ...\" into the colab connect to local runtime option\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "art6GQEJG5ri"
      },
      "source": [
        "###Also run this code segment, which creates a TPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbFX7QwQG67i"
      },
      "source": [
        "GCE_PROJECT_NAME = \"genome-project-319100\" #@param {type:\"string\"}\n",
        "TPU_ZONE = \"us-central1-f\" #@param {type:\"string\"}\n",
        "TPU_NAME = \"mutformer-tpu\" #@param {type:\"string\"}\n",
        "\n",
        "!gcloud alpha compute tpus create $TPU_NAME --accelerator-type=tpu-v2 --version=1.15.5 --zone=$TPU_ZONE ##create new TPU\n",
        "\n",
        "!gsutil iam ch serviceAccount:`gcloud alpha compute tpus describe $TPU_NAME | grep serviceAccount | cut -d' ' -f2`:admin gs://theodore_jiang && echo 'Successfully set permissions!' ##give TPU access to GCS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXZaQIt1SXQv"
      },
      "source": [
        "#Clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L470I2VGCLTe"
      },
      "source": [
        "if USE_GCP_TPU:\n",
        "  !sudo apt-get -y install git\n",
        "#@markdown ######where to clone the repo into (only value that it can't be is \"mutformer\"):\n",
        "REPO_DESTINATION_PATH = \"code/mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S4CiOh3RzFW"
      },
      "source": [
        "if not USE_GCP_TPU:\n",
        "  %tensorflow_version 1.x\n",
        "  from google.colab import auth\n",
        "  print(\"Authorize for GCS:\")\n",
        "  auth.authenticate_user()\n",
        "  print(\"Authorize done\")\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import importlib\n",
        "\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic\n",
        "from mutformer.modeling import BertModel,BertModelModified\n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor,MrpcWithPredsProcessor ##change this part if you add more modes--\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor      ##--\n",
        "\n",
        "##reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_classifier,\n",
        "                  run_ner_for_pathogenic]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "#@markdown ###### Whether or not to write logs to a file\n",
        "DO_FILE_LOGGING = True #@param {type:\"boolean\"}\n",
        "if DO_FILE_LOGGING:\n",
        "  #@markdown ###### If using file logging, what path to write logs to\n",
        "  FILE_LOGGING_PATH = 'file_logging/spam.log' #@param {type:\"string\"}\n",
        "  if not os.path.exists(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1])):\n",
        "    os.makedirs(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1]))\n",
        "  fh = logging.FileHandler(FILE_LOGGING_PATH)\n",
        "  fh.setLevel(logging.INFO)\n",
        "  fh.setFormatter(formatter)\n",
        "  log.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "# create formatter and add it to the handlers\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "log.handlers = [fh,ch]\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    # Upload credentials to TPU.\n",
        "    with open('/content/adc.json', 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\": ##change this part if you added more modes\n",
        "  processor = MrpcProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"MRPC_w_preds\":\n",
        "  processor = MrpcWithPredsProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"RE\":\n",
        "  processor = REProcessor()\n",
        "  script = run_classifier\n",
        "elif MODE==\"NER\":\n",
        "  processor = NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\", \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb0TXw9GtCKz"
      },
      "source": [
        "#Select preference for communication with eval script/Mount drive if necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYsYBUCJMTdz"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "#@markdown ###### Note: for all of these, if using USE_GCP_TPU, all of these parameters must use GCS, because a GCP TPU can't access google drive\n",
        "#@markdown \\\n",
        "DRIVE_PATH = \"/content/drive/My Drive\"\n",
        "BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "#@markdown whether to use GCS for communicating with eval script, if not, defaults to drive. Note that by defualt, training logs have to be stored in GCS because the Google TPU requires it\n",
        "GCS_COMS = False #@param {type:\"boolean\"}\n",
        "\n",
        "COMS_PATH = BUCKET_PATH if GCS_COMS else DRIVE_PATH\n",
        "\n",
        "if not GCS_COMS:\n",
        "  from google.colab import drive,auth\n",
        "  !fusermount -u /content/drive\n",
        "  drive.flush_and_unmount()\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "# Run Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ_lmHSa7ct_"
      },
      "source": [
        "###General definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bpaa3Eo7h8W"
      },
      "source": [
        "name2model = {\n",
        "    \"modified_large\":BertModelModified,\n",
        "    \"modified_medium\":BertModelModified,\n",
        "    \"modified\":BertModelModified,\n",
        "    \"orig\":BertModel,\n",
        "    \"large\":BertModel\n",
        "}\n",
        "\n",
        "def latest_checkpoint(dir):\n",
        "  cmd = \"gsutil ls \"+dir\n",
        "  files = !{cmd}\n",
        "  for file in files:\n",
        "    if \"model.ckpt\" in file:\n",
        "      return file.replace(\".\"+file.split(\".\")[-1],\"\")\n",
        "\n",
        "def training_loop(BATCH_SIZE,\n",
        "                  RESUMING,\n",
        "                  PLANNED_TOTAL_STEPS,\n",
        "                  DECAY_PER_STEP,\n",
        "                  DATA_SEQ_LENGTH,\n",
        "                  MODEL_NAME,\n",
        "                  MODEL,\n",
        "                  INIT_CHECKPOINT_DIR,\n",
        "                  BERT_GCS_DIR,\n",
        "                  DATA_GCS_DIR,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  USING_PREDS,\n",
        "                  PRED_NUM,\n",
        "                  GCS_LOGGING_DIR,\n",
        "                  CONFIG_FILE):\n",
        "  \n",
        "  RESTORE_CHECKPOINT = None if not RESUMING else tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "  if not RESUMING:\n",
        "    cmd = \"gsutil -m rm -r \"+BERT_GCS_DIR\n",
        "    !{cmd}\n",
        "\n",
        "  ## if using a directory with only a single checkpoint and no \"checkpoint\" file, \n",
        "  ## tf.train.latest_checkpoint will not work, so get fold name manually via latest_checkpoint(dir)\n",
        "  try: \n",
        "    INIT_CHECKPOINT = tf.train.latest_checkpoint(INIT_CHECKPOINT_DIR)\n",
        "  except:\n",
        "    INIT_CHECKPOINT = latest_checkpoint(INIT_CHECKPOINT_DIR)\n",
        "  print(\"init checkpoint:\",INIT_CHECKPOINT,\"restore/save checkpont:\",RESTORE_CHECKPOINT)\n",
        "\n",
        "  config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "  config.hidden_dropout_prob = 0.1\n",
        "  config.attention_probs_dropout_prob = 0.1\n",
        "\n",
        "  model_fn = script.model_fn_builder(\n",
        "      bert_config=config,\n",
        "      logging_dir=GCS_LOGGING_DIR,\n",
        "      num_labels=len(label_list),\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      restore_checkpoint=RESTORE_CHECKPOINT,\n",
        "      init_learning_rate=INIT_LEARNING_RATE,\n",
        "      decay_per_step=DECAY_PER_STEP,\n",
        "      num_warmup_steps=10,\n",
        "      use_tpu=True,\n",
        "      use_one_hot_embeddings=True,\n",
        "      bert=MODEL,\n",
        "      weight_decay=0.01,\n",
        "      epsilon=1e-6,\n",
        "      clip_grads=False,\n",
        "      using_preds=USING_PREDS)\n",
        "\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=BERT_GCS_DIR,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "      keep_checkpoint_max=KEEP_N_CHECKPOINTS_AT_A_TIME,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=True,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=BATCH_SIZE)\n",
        "  \n",
        "  train_file_name = \"train.tf_record\"\n",
        "  train_file = os.path.join(DATA_GCS_DIR, train_file_name)\n",
        "\n",
        "  if USING_SHARDS:\n",
        "    shards_folder = DATA_GCS_DIR\n",
        "    input_file = os.path.join(DATA_GCS_DIR, train_file_name)\n",
        "    import re\n",
        "    file_name = input_file.split(\"/\")[-1]\n",
        "    shards = [shards_folder + \"/\" + file for file in tf.io.gfile.listdir(shards_folder) if\n",
        "              re.match(file_name + \"_\\d+\", file)]\n",
        "    shards = sorted(shards,key=lambda shard:int(shard.split(\"_\")[-1]))[START_SHARD:]\n",
        "  else:\n",
        "    shards = [train_file]\n",
        "\n",
        "  if USING_SHARDS:\n",
        "    print(\"\\nUSING SHARDs:\")\n",
        "    for shard in shards:\n",
        "      print(shard)\n",
        "    print(\"\\n\")\n",
        "\n",
        "  tf.logging.info(\"***** Running training *****\")\n",
        "  tf.logging.info(\"  Batch size = %d\", BATCH_SIZE)\n",
        "  for n,shard in enumerate(shards):\n",
        "      train_input_fn = script.file_based_input_fn_builder( ##if using external data with MRPC_w_preds, make sure to specify \"pred_num=xxx\"\n",
        "          input_file=shard,\n",
        "          seq_length=DATA_SEQ_LENGTH,\n",
        "          is_training=True,\n",
        "          drop_remainder=True,\n",
        "          pred_num=PRED_NUM if USING_PREDS else None)\n",
        "\n",
        "      ##writing data to drive so that the parallel eval script can know which model to evaluate\n",
        "      try:\n",
        "        tf.gfile.Open(COMS_PATH+\"/finetuning_run_paired_model.txt\",\"w+\").write(MODEL_NAME)\n",
        "        tf.gfile.Open(COMS_PATH+\"/finetuning_run_paired_seq_length.txt\",\"w+\").write(str(DATA_SEQ_LENGTH))\n",
        "        tf.gfile.Open(COMS_PATH+\"/finetuning_run_paired_batch_size.txt\",\"w+\").write(str(BATCH_SIZE))\n",
        "      except:\n",
        "        pass\n",
        "      estimator.train(input_fn=train_input_fn, max_steps=PLANNED_TOTAL_STEPS)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AjnVKSMlVXz"
      },
      "source": [
        "####Model/sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "source": [
        "#@markdown train batch size to use\n",
        "BATCH_SIZE=16 #@param\n",
        "#@markdown list of models to test\n",
        "models = [\"modified_medium\",\"modified_large\"] #@param\n",
        "#@markdown list of maximum sequence lengths to test\n",
        "lengths = [256,512,1024] #@param\n",
        "LOGGING_DIR = \"mrpc_loss_spam_model_comparison_final\" #@param {type:\"string\"}\n",
        "#@markdown whether or not to resume training from a previous finetuned checkpoint; if no, always train from pretrained model\n",
        "RESUMING = False #@param {type:\"boolean\"}\n",
        "#@markdown whether or not external data is being used\n",
        "USING_PREDS = True #@param {type:\"boolean\"}\n",
        "#@markdown if using external data, how many datapoints are included in total\n",
        "PRED_NUM =   27#@param {type:\"integer\"}\n",
        "\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS != -1 else PLANNED_TOTAL_SEQUENCES_SEEN//BATCH_SIZE\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/(PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS!=-1 else PLANNED_TOTAL_SEQUENCES_SEEN/TRAIN_BATCH_SIZE) \n",
        "\n",
        "for DATA_SEQ_LENGTH in lengths:\n",
        "  for MODEL_NAME in models:\n",
        "    print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "          \"\\nINPUT MAX SEQ LENGTH:\",DATA_SEQ_LENGTH,\n",
        "          \"\\nTRAIN_BATCH_SIZE:\",BATCH_SIZE,\"\\n\\n\\n\")\n",
        "\n",
        "    MODEL = name2model[MODEL_NAME]\n",
        "    INIT_CHECKPOINT_DIR = \"{}/{}\".format(BUCKET_PATH, INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "    BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)))\n",
        "    DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, DATA_DIR_format.replace(\"xxx\",str(DATA_SEQ_LENGTH)))\n",
        "    \n",
        "    GCS_LOGGING_DIR = \"{}/{}\".format(BUCKET_PATH, LOGGING_DIR+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)))\n",
        "\n",
        "    CONFIG_FILE = \"{}/config.json\".format(BUCKET_PATH+\"/\"+INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "\n",
        "    training_loop(BATCH_SIZE,\n",
        "                  RESUMING,\n",
        "                  PLANNED_TOTAL_STEPS,\n",
        "                  DECAY_PER_STEP,\n",
        "                  DATA_SEQ_LENGTH,\n",
        "                  MODEL_NAME,\n",
        "                  MODEL,\n",
        "                  INIT_CHECKPOINT_DIR,\n",
        "                  BERT_GCS_DIR,\n",
        "                  DATA_GCS_DIR,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  USING_PREDS,\n",
        "                  PRED_NUM,\n",
        "                  GCS_LOGGING_DIR,\n",
        "                  CONFIG_FILE)\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ322RGr5ykF"
      },
      "source": [
        "####Batch size/sequence length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IytLW0VbgOZz"
      },
      "source": [
        "#@markdown list of batch sizes to test\n",
        "batch_sizes = [64] #@param\n",
        "#@markdown list of maximum sequence lengths to test\n",
        "lengths = [1024] #@param\n",
        "#@markdown model to use\n",
        "MODEL_NAME=\"modified_large\" #@param {type:\"string\"}\n",
        "LOGGING_DIR = \"mrpc_loss_spam_model_comparison_final\" #@param {type:\"string\"}\n",
        "#@markdown whether or not to resume training from a previous finetuned checkpoint; if no, always train from pretrained model\n",
        "RESUMING = False #@param {type:\"boolean\"}\n",
        "#@markdown whether or not external data is being used\n",
        "USING_PREDS = True #@param {type:\"boolean\"}\n",
        "#@markdown if using external data, how many datapoints are included in total\n",
        "PRED_NUM =   27#@param {type:\"integer\"}\n",
        "\n",
        "BUCKET_PATH = \"gs://{}\".format(BUCKET_NAME)\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS != -1 else PLANNED_TOTAL_SEQUENCES_SEEN//BATCH_SIZE\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/(PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS!=-1 else PLANNED_TOTAL_SEQUENCES_SEEN/TRAIN_BATCH_SIZE) \n",
        "\n",
        "for DATA_SEQ_LENGTH in lengths:\n",
        "    for BATCH_SIZE in batch_sizes:\n",
        "        print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "              \"\\nINPUT MAX SEQ LENGTH:\",DATA_SEQ_LENGTH,\n",
        "              \"\\nTRAIN_BATCH_SIZE:\",BATCH_SIZE,\"\\n\\n\\n\")\n",
        "       \n",
        "        MODEL = name2model[MODEL_NAME]\n",
        "        INIT_CHECKPOINT_DIR = \"{}/{}\".format(BUCKET_PATH, INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "        BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE)))\n",
        "        DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, DATA_DIR_format.replace(\"xxx\",str(DATA_SEQ_LENGTH)))\n",
        "      \n",
        "        GCS_LOGGING_DIR = \"{}/{}\".format(BUCKET_PATH, LOGGING_DIR+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE)))\n",
        "        \n",
        "        CONFIG_FILE = \"{}/config.json\".format(BUCKET_PATH+\"/\"+INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "\n",
        "        training_loop(BATCH_SIZE,\n",
        "                      RESUMING,\n",
        "                      PLANNED_TOTAL_STEPS,\n",
        "                      DECAY_PER_STEP,\n",
        "                      DATA_SEQ_LENGTH,\n",
        "                      MODEL_NAME,\n",
        "                      MODEL,\n",
        "                      INIT_CHECKPOINT_DIR,\n",
        "                      BERT_GCS_DIR,\n",
        "                      DATA_GCS_DIR,\n",
        "                      USING_SHARDS,\n",
        "                      START_SHARD,\n",
        "                      USING_PREDS,\n",
        "                      PRED_NUM,\n",
        "                      GCS_LOGGING_DIR,\n",
        "                      CONFIG_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tswiJYA_qCUq"
      },
      "source": [
        "###Train a single model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK3tf7aWqIiR"
      },
      "source": [
        "#@markdown batch size to use\n",
        "BATCH_SIZE = 32 #@param\n",
        "#@markdown maximum sequence length to use\n",
        "DATA_SEQ_LENGTH = 512 #@param\n",
        "#@markdown model to use\n",
        "MODEL_NAME=\"modified_large\" #@param {type:\"string\"}\n",
        "LOGGING_DIR = \"mrpc_loss_spam_model_comparison_final\" #@param {type:\"string\"}\n",
        "#@markdown whether or not to resume training from a previous checkpoint; if no, always train from scratch\n",
        "RESUMING = True #@param {type:\"boolean\"}\n",
        "#@markdown ###### identifier for the model to use (replaces \"xxx\" from the variable \"MODEL_DIR_format\")\n",
        "model_name_extension = \"added_preds_512_32\" #@param {type:\"string\"}\n",
        "#@markdown whether or not training data was generated in shards (for really large databases)\n",
        "USING_SHARDS = True #@param {type:\"boolean\"}\n",
        "#@markdown if using shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD =   0#@param {type:\"integer\"}\n",
        "#@markdown whether or not external data is being used\n",
        "USING_PREDS = True #@param {type:\"boolean\"}\n",
        "#@markdown if using external data, how many datapoints are included in total\n",
        "PRED_NUM =   27#@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS != -1 else PLANNED_TOTAL_SEQUENCES_SEEN//BATCH_SIZE\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/(PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS!=-1 else PLANNED_TOTAL_SEQUENCES_SEEN/TRAIN_BATCH_SIZE) \n",
        "\n",
        "\n",
        "print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "      \"\\nINPUT MAX SEQ LENGTH:\",DATA_SEQ_LENGTH,\n",
        "      \"\\nTRAIN_BATCH_SIZE:\",BATCH_SIZE,\"\\n\\n\\n\")\n",
        "\n",
        "MODEL = name2model[MODEL_NAME]\n",
        "INIT_CHECKPOINT_DIR = \"{}/{}\".format(BUCKET_PATH, INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR_format.replace(\"xxx\",model_name_extension))\n",
        "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, DATA_DIR_format.replace(\"xxx\",str(DATA_SEQ_LENGTH)))\n",
        "\n",
        "GCS_LOGGING_DIR = \"{}/{}\".format(BUCKET_PATH, LOGGING_DIR+\"/\"+RUN_NAME_format.replace(\"xxx\",MODEL_NAME+\"_\"+str(DATA_SEQ_LENGTH)+\"_\"+str(BATCH_SIZE)))\n",
        "\n",
        "CONFIG_FILE = \"{}/config.json\".format(BUCKET_PATH+\"/\"+INIT_MODEL_DIR_format.replace(\"xxx\",MODEL_NAME))\n",
        "\n",
        "\n",
        "training_loop(BATCH_SIZE,\n",
        "              RESUMING,\n",
        "              PLANNED_TOTAL_STEPS,\n",
        "              DECAY_PER_STEP,\n",
        "              DATA_SEQ_LENGTH,\n",
        "              MODEL_NAME,\n",
        "              MODEL,\n",
        "              INIT_CHECKPOINT_DIR,\n",
        "              BERT_GCS_DIR,\n",
        "              DATA_GCS_DIR,\n",
        "              USING_SHARDS,\n",
        "              START_SHARD,\n",
        "              USING_PREDS,\n",
        "              PRED_NUM,\n",
        "              GCS_LOGGING_DIR,\n",
        "              CONFIG_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}