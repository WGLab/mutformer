{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VudE2umODMnI"
      },
      "source": [
        "#Finetuning Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzHNq3MSTTea"
      },
      "source": [
        "This notebook performs finetuning tests with varying parameters to train the best possible model.\n",
        "\n",
        "Note: If using a TPU from Google Cloud (not the Colab TPU), make sure to run this notebook on a VM with access to all GCP APIs, and make sure TPUs are enabled for the GCP project"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Downgrade Python and Tensorflow \n",
        "\n",
        "(the default python version in Colab does not support Tensorflow 1.15)\n",
        "\n",
        "* **Note** that because the Python used in this notebook is not the default path, syntax highlighting most likely will not function."
      ],
      "metadata": {
        "id": "8fgfq-Cx1cKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1. First, download and install Python version 3.7:"
      ],
      "metadata": {
        "id": "TeczSx7xQqSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py37_22.11.1-1-Linux-x86_64.sh\n",
        "!chmod +x mini.sh\n",
        "!bash ./mini.sh -b -f -p /usr/local\n",
        "!conda install -q -y jupyter\n",
        "!conda install -q -y google-colab -c conda-forge\n",
        "!python -m ipykernel install --name \"py37\" --user"
      ],
      "metadata": {
        "id": "nFsHjOgF1b8n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d97447a-6c07-48b9-8da7-d14812260953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-04 21:20:55--  https://repo.anaconda.com/miniconda/Miniconda3-py37_22.11.1-1-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86308321 (82M) [application/x-sh]\n",
            "Saving to: ‘mini.sh’\n",
            "\n",
            "mini.sh             100%[===================>]  82.31M   181MB/s    in 0.5s    \n",
            "\n",
            "2023-04-04 21:20:56 (181 MB/s) - ‘mini.sh’ saved [86308321/86308321]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "                                                                                     \n",
            "Installing base environment...\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: ...working... failed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - jupyter\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    anyio-3.5.0                |   py37h06a4308_0         165 KB\n",
            "    argon2-cffi-21.3.0         |     pyhd3eb1b0_0          15 KB\n",
            "    argon2-cffi-bindings-21.2.0|   py37h7f8727e_0          33 KB\n",
            "    attrs-22.1.0               |   py37h06a4308_0          84 KB\n",
            "    babel-2.11.0               |   py37h06a4308_0         6.8 MB\n",
            "    backcall-0.2.0             |     pyhd3eb1b0_0          13 KB\n",
            "    beautifulsoup4-4.11.1      |   py37h06a4308_0         185 KB\n",
            "    bleach-4.1.0               |     pyhd3eb1b0_0         123 KB\n",
            "    ca-certificates-2023.01.10 |       h06a4308_0         120 KB\n",
            "    conda-23.1.0               |   py37h06a4308_0         937 KB\n",
            "    dbus-1.13.18               |       hb2f20db_0         504 KB\n",
            "    debugpy-1.5.1              |   py37h295c915_0         1.7 MB\n",
            "    decorator-5.1.1            |     pyhd3eb1b0_0          12 KB\n",
            "    defusedxml-0.7.1           |     pyhd3eb1b0_0          23 KB\n",
            "    entrypoints-0.4            |   py37h06a4308_0          16 KB\n",
            "    expat-2.4.9                |       h6a678d5_0         156 KB\n",
            "    fontconfig-2.14.1          |       h52c9d5c_1         281 KB\n",
            "    freetype-2.12.1            |       h4a9f257_0         626 KB\n",
            "    glib-2.69.1                |       he621ea3_2         1.9 MB\n",
            "    gst-plugins-base-1.14.1    |       h6a678d5_1         2.2 MB\n",
            "    gstreamer-1.14.1           |       h5eee18b_1         1.7 MB\n",
            "    icu-58.2                   |       he6710b0_3        10.5 MB\n",
            "    importlib_resources-5.2.0  |     pyhd3eb1b0_1          21 KB\n",
            "    ipykernel-6.15.2           |   py37h06a4308_0         189 KB\n",
            "    ipython-7.31.1             |   py37h06a4308_1        1002 KB\n",
            "    ipython_genutils-0.2.0     |     pyhd3eb1b0_1          27 KB\n",
            "    ipywidgets-7.6.5           |     pyhd3eb1b0_1         105 KB\n",
            "    jedi-0.18.1                |   py37h06a4308_1         980 KB\n",
            "    jinja2-3.1.2               |   py37h06a4308_0         209 KB\n",
            "    jpeg-9e                    |       h5eee18b_1         262 KB\n",
            "    json5-0.9.6                |     pyhd3eb1b0_0          21 KB\n",
            "    jsonschema-4.17.3          |   py37h06a4308_0         138 KB\n",
            "    jupyter-1.0.0              |   py37h06a4308_8           7 KB\n",
            "    jupyter_client-7.4.9       |   py37h06a4308_0         204 KB\n",
            "    jupyter_console-6.4.4      |   py37h06a4308_0          42 KB\n",
            "    jupyter_core-4.11.2        |   py37h06a4308_0          80 KB\n",
            "    jupyter_server-1.23.4      |   py37h06a4308_0         382 KB\n",
            "    jupyterlab-3.5.3           |   py37h06a4308_0         4.4 MB\n",
            "    jupyterlab_pygments-0.1.2  |             py_0           8 KB\n",
            "    jupyterlab_server-2.19.0   |   py37h06a4308_0          80 KB\n",
            "    jupyterlab_widgets-1.0.0   |     pyhd3eb1b0_1         109 KB\n",
            "    libpng-1.6.39              |       h5eee18b_0         304 KB\n",
            "    libsodium-1.0.18           |       h7b6447c_0         244 KB\n",
            "    libuuid-1.41.5             |       h5eee18b_0          27 KB\n",
            "    libxcb-1.15                |       h7f8727e_0         505 KB\n",
            "    libxml2-2.9.14             |       h74e7548_0         718 KB\n",
            "    markupsafe-2.1.1           |   py37h7f8727e_0          21 KB\n",
            "    matplotlib-inline-0.1.6    |   py37h06a4308_0          16 KB\n",
            "    mistune-0.8.4              |py37h14c3975_1001          54 KB\n",
            "    nbclassic-0.5.2            |   py37h06a4308_0         6.1 MB\n",
            "    nbclient-0.5.13            |   py37h06a4308_0          91 KB\n",
            "    nbconvert-6.4.4            |   py37h06a4308_0         493 KB\n",
            "    nbformat-5.7.0             |   py37h06a4308_0         133 KB\n",
            "    nest-asyncio-1.5.6         |   py37h06a4308_0          14 KB\n",
            "    notebook-6.5.2             |   py37h06a4308_0         508 KB\n",
            "    notebook-shim-0.2.2        |   py37h06a4308_0          22 KB\n",
            "    openssl-1.1.1t             |       h7f8727e_0         3.7 MB\n",
            "    packaging-22.0             |   py37h06a4308_0          68 KB\n",
            "    pandocfilters-1.5.0        |     pyhd3eb1b0_0          11 KB\n",
            "    parso-0.8.3                |     pyhd3eb1b0_0          70 KB\n",
            "    pcre-8.45                  |       h295c915_0         207 KB\n",
            "    pexpect-4.8.0              |     pyhd3eb1b0_3          53 KB\n",
            "    pickleshare-0.7.5          |  pyhd3eb1b0_1003          13 KB\n",
            "    pkgutil-resolve-name-1.3.10|   py37h06a4308_0           9 KB\n",
            "    prometheus_client-0.14.1   |   py37h06a4308_0          90 KB\n",
            "    prompt-toolkit-3.0.36      |   py37h06a4308_0         571 KB\n",
            "    prompt_toolkit-3.0.36      |       hd3eb1b0_0           5 KB\n",
            "    psutil-5.9.0               |   py37h5eee18b_0         328 KB\n",
            "    ptyprocess-0.7.0           |     pyhd3eb1b0_2          17 KB\n",
            "    pygments-2.11.2            |     pyhd3eb1b0_0         759 KB\n",
            "    pyqt-5.9.2                 |   py37h05f1152_2         4.5 MB\n",
            "    pyrsistent-0.18.0          |   py37heee7806_0          95 KB\n",
            "    python-dateutil-2.8.2      |     pyhd3eb1b0_0         233 KB\n",
            "    python-fastjsonschema-2.16.2|   py37h06a4308_0         230 KB\n",
            "    pytz-2022.7                |   py37h06a4308_0         207 KB\n",
            "    pyzmq-23.2.0               |   py37h6a678d5_0         438 KB\n",
            "    qt-5.9.7                   |       h5867ecd_1        68.5 MB\n",
            "    qtconsole-5.4.0            |   py37h06a4308_0         189 KB\n",
            "    qtpy-2.2.0                 |   py37h06a4308_0          84 KB\n",
            "    send2trash-1.8.0           |     pyhd3eb1b0_1          19 KB\n",
            "    sip-4.19.8                 |   py37hf484d3e_0         274 KB\n",
            "    sniffio-1.2.0              |   py37h06a4308_1          15 KB\n",
            "    soupsieve-2.3.2.post1      |   py37h06a4308_0          65 KB\n",
            "    terminado-0.17.1           |   py37h06a4308_0          31 KB\n",
            "    testpath-0.6.0             |   py37h06a4308_0          85 KB\n",
            "    tomli-2.0.1                |   py37h06a4308_0          24 KB\n",
            "    tornado-6.2                |   py37h5eee18b_0         584 KB\n",
            "    traitlets-5.7.1            |   py37h06a4308_0         199 KB\n",
            "    typing-extensions-4.4.0    |   py37h06a4308_0           8 KB\n",
            "    wcwidth-0.2.5              |     pyhd3eb1b0_0          26 KB\n",
            "    webencodings-0.5.1         |           py37_1          19 KB\n",
            "    websocket-client-0.58.0    |   py37h06a4308_4          66 KB\n",
            "    widgetsnbextension-3.5.2   |   py37h06a4308_0         645 KB\n",
            "    zeromq-4.3.4               |       h2531618_0         331 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       127.9 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  anyio              pkgs/main/linux-64::anyio-3.5.0-py37h06a4308_0 \n",
            "  argon2-cffi        pkgs/main/noarch::argon2-cffi-21.3.0-pyhd3eb1b0_0 \n",
            "  argon2-cffi-bindi~ pkgs/main/linux-64::argon2-cffi-bindings-21.2.0-py37h7f8727e_0 \n",
            "  attrs              pkgs/main/linux-64::attrs-22.1.0-py37h06a4308_0 \n",
            "  babel              pkgs/main/linux-64::babel-2.11.0-py37h06a4308_0 \n",
            "  backcall           pkgs/main/noarch::backcall-0.2.0-pyhd3eb1b0_0 \n",
            "  beautifulsoup4     pkgs/main/linux-64::beautifulsoup4-4.11.1-py37h06a4308_0 \n",
            "  bleach             pkgs/main/noarch::bleach-4.1.0-pyhd3eb1b0_0 \n",
            "  dbus               pkgs/main/linux-64::dbus-1.13.18-hb2f20db_0 \n",
            "  debugpy            pkgs/main/linux-64::debugpy-1.5.1-py37h295c915_0 \n",
            "  decorator          pkgs/main/noarch::decorator-5.1.1-pyhd3eb1b0_0 \n",
            "  defusedxml         pkgs/main/noarch::defusedxml-0.7.1-pyhd3eb1b0_0 \n",
            "  entrypoints        pkgs/main/linux-64::entrypoints-0.4-py37h06a4308_0 \n",
            "  expat              pkgs/main/linux-64::expat-2.4.9-h6a678d5_0 \n",
            "  fontconfig         pkgs/main/linux-64::fontconfig-2.14.1-h52c9d5c_1 \n",
            "  freetype           pkgs/main/linux-64::freetype-2.12.1-h4a9f257_0 \n",
            "  glib               pkgs/main/linux-64::glib-2.69.1-he621ea3_2 \n",
            "  gst-plugins-base   pkgs/main/linux-64::gst-plugins-base-1.14.1-h6a678d5_1 \n",
            "  gstreamer          pkgs/main/linux-64::gstreamer-1.14.1-h5eee18b_1 \n",
            "  icu                pkgs/main/linux-64::icu-58.2-he6710b0_3 \n",
            "  importlib_resourc~ pkgs/main/noarch::importlib_resources-5.2.0-pyhd3eb1b0_1 \n",
            "  ipykernel          pkgs/main/linux-64::ipykernel-6.15.2-py37h06a4308_0 \n",
            "  ipython            pkgs/main/linux-64::ipython-7.31.1-py37h06a4308_1 \n",
            "  ipython_genutils   pkgs/main/noarch::ipython_genutils-0.2.0-pyhd3eb1b0_1 \n",
            "  ipywidgets         pkgs/main/noarch::ipywidgets-7.6.5-pyhd3eb1b0_1 \n",
            "  jedi               pkgs/main/linux-64::jedi-0.18.1-py37h06a4308_1 \n",
            "  jinja2             pkgs/main/linux-64::jinja2-3.1.2-py37h06a4308_0 \n",
            "  jpeg               pkgs/main/linux-64::jpeg-9e-h5eee18b_1 \n",
            "  json5              pkgs/main/noarch::json5-0.9.6-pyhd3eb1b0_0 \n",
            "  jsonschema         pkgs/main/linux-64::jsonschema-4.17.3-py37h06a4308_0 \n",
            "  jupyter            pkgs/main/linux-64::jupyter-1.0.0-py37h06a4308_8 \n",
            "  jupyter_client     pkgs/main/linux-64::jupyter_client-7.4.9-py37h06a4308_0 \n",
            "  jupyter_console    pkgs/main/linux-64::jupyter_console-6.4.4-py37h06a4308_0 \n",
            "  jupyter_core       pkgs/main/linux-64::jupyter_core-4.11.2-py37h06a4308_0 \n",
            "  jupyter_server     pkgs/main/linux-64::jupyter_server-1.23.4-py37h06a4308_0 \n",
            "  jupyterlab         pkgs/main/linux-64::jupyterlab-3.5.3-py37h06a4308_0 \n",
            "  jupyterlab_pygmen~ pkgs/main/noarch::jupyterlab_pygments-0.1.2-py_0 \n",
            "  jupyterlab_server  pkgs/main/linux-64::jupyterlab_server-2.19.0-py37h06a4308_0 \n",
            "  jupyterlab_widgets pkgs/main/noarch::jupyterlab_widgets-1.0.0-pyhd3eb1b0_1 \n",
            "  libpng             pkgs/main/linux-64::libpng-1.6.39-h5eee18b_0 \n",
            "  libsodium          pkgs/main/linux-64::libsodium-1.0.18-h7b6447c_0 \n",
            "  libuuid            pkgs/main/linux-64::libuuid-1.41.5-h5eee18b_0 \n",
            "  libxcb             pkgs/main/linux-64::libxcb-1.15-h7f8727e_0 \n",
            "  libxml2            pkgs/main/linux-64::libxml2-2.9.14-h74e7548_0 \n",
            "  markupsafe         pkgs/main/linux-64::markupsafe-2.1.1-py37h7f8727e_0 \n",
            "  matplotlib-inline  pkgs/main/linux-64::matplotlib-inline-0.1.6-py37h06a4308_0 \n",
            "  mistune            pkgs/main/linux-64::mistune-0.8.4-py37h14c3975_1001 \n",
            "  nbclassic          pkgs/main/linux-64::nbclassic-0.5.2-py37h06a4308_0 \n",
            "  nbclient           pkgs/main/linux-64::nbclient-0.5.13-py37h06a4308_0 \n",
            "  nbconvert          pkgs/main/linux-64::nbconvert-6.4.4-py37h06a4308_0 \n",
            "  nbformat           pkgs/main/linux-64::nbformat-5.7.0-py37h06a4308_0 \n",
            "  nest-asyncio       pkgs/main/linux-64::nest-asyncio-1.5.6-py37h06a4308_0 \n",
            "  notebook           pkgs/main/linux-64::notebook-6.5.2-py37h06a4308_0 \n",
            "  notebook-shim      pkgs/main/linux-64::notebook-shim-0.2.2-py37h06a4308_0 \n",
            "  packaging          pkgs/main/linux-64::packaging-22.0-py37h06a4308_0 \n",
            "  pandocfilters      pkgs/main/noarch::pandocfilters-1.5.0-pyhd3eb1b0_0 \n",
            "  parso              pkgs/main/noarch::parso-0.8.3-pyhd3eb1b0_0 \n",
            "  pcre               pkgs/main/linux-64::pcre-8.45-h295c915_0 \n",
            "  pexpect            pkgs/main/noarch::pexpect-4.8.0-pyhd3eb1b0_3 \n",
            "  pickleshare        pkgs/main/noarch::pickleshare-0.7.5-pyhd3eb1b0_1003 \n",
            "  pkgutil-resolve-n~ pkgs/main/linux-64::pkgutil-resolve-name-1.3.10-py37h06a4308_0 \n",
            "  prometheus_client  pkgs/main/linux-64::prometheus_client-0.14.1-py37h06a4308_0 \n",
            "  prompt-toolkit     pkgs/main/linux-64::prompt-toolkit-3.0.36-py37h06a4308_0 \n",
            "  prompt_toolkit     pkgs/main/noarch::prompt_toolkit-3.0.36-hd3eb1b0_0 \n",
            "  psutil             pkgs/main/linux-64::psutil-5.9.0-py37h5eee18b_0 \n",
            "  ptyprocess         pkgs/main/noarch::ptyprocess-0.7.0-pyhd3eb1b0_2 \n",
            "  pygments           pkgs/main/noarch::pygments-2.11.2-pyhd3eb1b0_0 \n",
            "  pyqt               pkgs/main/linux-64::pyqt-5.9.2-py37h05f1152_2 \n",
            "  pyrsistent         pkgs/main/linux-64::pyrsistent-0.18.0-py37heee7806_0 \n",
            "  python-dateutil    pkgs/main/noarch::python-dateutil-2.8.2-pyhd3eb1b0_0 \n",
            "  python-fastjsonsc~ pkgs/main/linux-64::python-fastjsonschema-2.16.2-py37h06a4308_0 \n",
            "  pytz               pkgs/main/linux-64::pytz-2022.7-py37h06a4308_0 \n",
            "  pyzmq              pkgs/main/linux-64::pyzmq-23.2.0-py37h6a678d5_0 \n",
            "  qt                 pkgs/main/linux-64::qt-5.9.7-h5867ecd_1 \n",
            "  qtconsole          pkgs/main/linux-64::qtconsole-5.4.0-py37h06a4308_0 \n",
            "  qtpy               pkgs/main/linux-64::qtpy-2.2.0-py37h06a4308_0 \n",
            "  send2trash         pkgs/main/noarch::send2trash-1.8.0-pyhd3eb1b0_1 \n",
            "  sip                pkgs/main/linux-64::sip-4.19.8-py37hf484d3e_0 \n",
            "  sniffio            pkgs/main/linux-64::sniffio-1.2.0-py37h06a4308_1 \n",
            "  soupsieve          pkgs/main/linux-64::soupsieve-2.3.2.post1-py37h06a4308_0 \n",
            "  terminado          pkgs/main/linux-64::terminado-0.17.1-py37h06a4308_0 \n",
            "  testpath           pkgs/main/linux-64::testpath-0.6.0-py37h06a4308_0 \n",
            "  tomli              pkgs/main/linux-64::tomli-2.0.1-py37h06a4308_0 \n",
            "  tornado            pkgs/main/linux-64::tornado-6.2-py37h5eee18b_0 \n",
            "  traitlets          pkgs/main/linux-64::traitlets-5.7.1-py37h06a4308_0 \n",
            "  typing-extensions  pkgs/main/linux-64::typing-extensions-4.4.0-py37h06a4308_0 \n",
            "  wcwidth            pkgs/main/noarch::wcwidth-0.2.5-pyhd3eb1b0_0 \n",
            "  webencodings       pkgs/main/linux-64::webencodings-0.5.1-py37_1 \n",
            "  websocket-client   pkgs/main/linux-64::websocket-client-0.58.0-py37h06a4308_4 \n",
            "  widgetsnbextension pkgs/main/linux-64::widgetsnbextension-3.5.2-py37h06a4308_0 \n",
            "  zeromq             pkgs/main/linux-64::zeromq-4.3.4-h2531618_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                     2022.10.11-h06a4308_0 --> 2023.01.10-h06a4308_0 \n",
            "  conda                              22.11.1-py37h06a4308_4 --> 23.1.0-py37h06a4308_0 \n",
            "  openssl                                 1.1.1s-h7f8727e_0 --> 1.1.1t-h7f8727e_0 \n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Then, reload the webpage (not restart runtime) to allow Colab to recognize the newly installed python\n",
        "####3. Finally, run the following commands to install tensorflow 1.15:"
      ],
      "metadata": {
        "id": "dsNWer6HQsIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pip install tensorflow==1.15\n",
        "!python3 -m pip install protobuf==3.20.1"
      ],
      "metadata": {
        "id": "WaXLRzqHQtdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e134830a-a973-46a1-be0f-8bcb9b2cd59a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==1.15\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf>=3.6.1\n",
            "  Downloading protobuf-4.22.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=0.7.0\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
            "Collecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.16.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (0.37.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.53.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m76.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.4/503.4 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/site-packages (from tensorflow==1.15) (1.21.6)\n",
            "Collecting google-pasta>=0.1.6\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py\n",
            "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (59.8.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.4.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=6e707a662c6ad6920cd54cb10af084c9853cd5c35754f463c31d13efb53e3e44\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/87/6f/3f34218ef184368cec9ee65bdfd65baf117811f0a0ce1263ff\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, wrapt, werkzeug, termcolor, protobuf, opt-einsum, keras-preprocessing, h5py, grpcio, google-pasta, gast, astor, absl-py, markdown, keras-applications, tensorboard, tensorflow\n",
            "Successfully installed absl-py-1.4.0 astor-0.8.1 gast-0.2.2 google-pasta-0.2.0 grpcio-1.53.0 h5py-3.8.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.4.3 opt-einsum-3.3.0 protobuf-4.22.1 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1 termcolor-2.2.0 werkzeug-2.2.3 wrapt-1.15.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protobuf==3.20.1\n",
            "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.22.1\n",
            "    Uninstalling protobuf-4.22.1:\n",
            "      Successfully uninstalled protobuf-4.22.1\n",
            "Successfully installed protobuf-3.20.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2XB_l-Hgzq_"
      },
      "source": [
        "# Configure settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ozmx1LCLw3SQ"
      },
      "outputs": [],
      "source": [
        "#@markdown ## General Config\n",
        "#@markdown If preferred, a GCP TPU/runtime can be used to run this notebook (instructions below)\n",
        "GCP_RUNTIME = False #@param {type:\"boolean\"}\n",
        "#@markdown How many TPU scores the TPU has: if using colab, NUM_TPU_CORES is 8.\n",
        "NUM_TPU_CORES = 8 #@param {type:\"number\"}\n",
        "#@markdown Which mode to use (a different mode means a different finetuning task): options are:\n",
        "#@markdown * \"MRPC\" - paired sequence method\n",
        "#@markdown * \"MRPC_w_ex_data\" - paired sequence method with external data\n",
        "#@markdown * \"RE\" - single sequence method\n",
        "#@markdown * \"NER\" - single sequence per residue prediction \n",
        "#@markdown \n",
        "#@markdown You can add more modes by creating a new processor and/or a new model_fn inside of the \"mutformer_model_code\" folder downloaded from github, then changing the corresponding code snippets in the code segment named \"Authorize for GCS, Imports, and General Setup\" (also edit the dropdown below).\n",
        "MODE = \"MRPC_w_ex_data\" #@param   [\"MRPC_w_ex_data\", \"MRPC\", \"RE\", \"NER\"]   {type:\"string\"} \n",
        "             ####      ^^^^^ dropdown list for all modes ^^^^^\n",
        "#@markdown Name of the GCS bucket to use (Make sure to set this to the name of your own GCS  bucket):\n",
        "BUCKET_NAME = \"\" #@param {type:\"string\"}\n",
        "BUCKET_PATH = \"gs://\"+BUCKET_NAME\n",
        "#@markdown Where in GCS the data needs to be loaded from (should be the same as the OUTPUT_DATA_DIR variable in the data generation script):\n",
        "PROCESSED_DATA_DIR = \"compiled_finetune_data/MRPC_ex_data_all_finetune_update_loaded\" #@param {type:\"string\"}\n",
        "#@markdown Which folder to store the logs in (the LOGGING_DIR variable can be the same across all finetuning notebooks)\n",
        "LOGGING_DIR = \"MutFormer_finetuning_newbut_try3_logs\" #@param {type:\"string\"}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaNhPg7sG2DS"
      },
      "source": [
        "#If running on a GCP runtime, follow these instructions to set it up:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gicp021G3Xd"
      },
      "source": [
        "###1) Create a VM from the GCP website\n",
        "###2) Open a command prompt on your computer and perform the following steps\"\n",
        "To ssh into the VM, run:\n",
        "\n",
        "```\n",
        "gcloud beta compute ssh --zone <COMPUTE ZONE> <VM NAME> --project <PROJECT NAME> -- -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "Note: Make sure the port above matches the port below (in this case it's 8888)\n",
        "\\\n",
        "\\\n",
        "In the new command prompt that popped out, either run each of the commands below individually, or copy and paste the one liner below:\n",
        "```\n",
        "sudo apt-get update\n",
        "sudo apt-get -y install python3 python3-pip\n",
        "sudo apt-get install pkg-config\n",
        "sudo apt-get install libhdf5-serial-dev\n",
        "sudo apt-get install libffi6 libffi-dev\n",
        "sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm\n",
        "sudo -H pip3 install jupyter_http_over_ws\n",
        "jupyter serverextension enable --py jupyter_http_over_ws\n",
        "jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "One command:\n",
        "```\n",
        "sudo apt-get update ; sudo apt-get -y install python3 python3-pip ; sudo apt-get install pkg-config ; sudo apt-get -y install libhdf5-serial-dev ; sudo apt-get install libffi6 libffi-dev; sudo -H pip3 install jupyter tensorflow==1.14 google-api-python-client tqdm ; sudo -H pip3 install jupyter_http_over_ws ; jupyter serverextension enable --py jupyter_http_over_ws ; jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0   --no-browser\n",
        "```\n",
        "###3) In this notebook, click the \"connect to local runtime\" option under the connect button, and copy and paste the link outputted by command prompt with \"locahost: ...\"\n",
        "###4) Finally, run this code segment, which creates a TPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbFX7QwQG67i"
      },
      "outputs": [],
      "source": [
        "GCE_PROJECT_NAME = \"\" #@param {type:\"string\"}\n",
        "TPU_ZONE = \"us-central1-f\" #@param {type:\"string\"}\n",
        "TPU_NAME = \"mutformer-tpu\" #@param {type:\"string\"}\n",
        "\n",
        "!gcloud alpha compute tpus create $TPU_NAME --accelerator-type=tpu-v2 --version=1.15.5 --zone=$TPU_ZONE ##create new TPU\n",
        "\n",
        "!gsutil iam ch serviceAccount:`gcloud alpha compute tpus describe $TPU_NAME | grep serviceAccount | cut -d' ' -f2`:admin $BUCKET_PATH && echo 'Successfully set permissions!' ##give TPU access to GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXZaQIt1SXQv"
      },
      "source": [
        "#Clone the MutFormer repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L470I2VGCLTe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f356a436-1225-42f1-e9eb-61778b8563ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mutformer'...\n",
            "remote: Enumerating objects: 1574, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/454)\u001b[K\rremote: Counting objects:   1% (5/454)\u001b[K\rremote: Counting objects:   2% (10/454)\u001b[K\rremote: Counting objects:   3% (14/454)\u001b[K\rremote: Counting objects:   4% (19/454)\u001b[K\rremote: Counting objects:   5% (23/454)\u001b[K\rremote: Counting objects:   6% (28/454)\u001b[K\rremote: Counting objects:   7% (32/454)\u001b[K\rremote: Counting objects:   8% (37/454)\u001b[K\rremote: Counting objects:   9% (41/454)\u001b[K\rremote: Counting objects:  10% (46/454)\u001b[K\rremote: Counting objects:  11% (50/454)\u001b[K\rremote: Counting objects:  12% (55/454)\u001b[K\rremote: Counting objects:  13% (60/454)\u001b[K\rremote: Counting objects:  14% (64/454)\u001b[K\rremote: Counting objects:  15% (69/454)\u001b[K\rremote: Counting objects:  16% (73/454)\u001b[K\rremote: Counting objects:  17% (78/454)\u001b[K\rremote: Counting objects:  18% (82/454)\u001b[K\rremote: Counting objects:  19% (87/454)\u001b[K\rremote: Counting objects:  20% (91/454)\u001b[K\rremote: Counting objects:  21% (96/454)\u001b[K\rremote: Counting objects:  22% (100/454)\u001b[K\rremote: Counting objects:  23% (105/454)\u001b[K\rremote: Counting objects:  24% (109/454)\u001b[K\rremote: Counting objects:  25% (114/454)\u001b[K\rremote: Counting objects:  26% (119/454)\u001b[K\rremote: Counting objects:  27% (123/454)\u001b[K\rremote: Counting objects:  28% (128/454)\u001b[K\rremote: Counting objects:  29% (132/454)\u001b[K\rremote: Counting objects:  30% (137/454)\u001b[K\rremote: Counting objects:  31% (141/454)\u001b[K\rremote: Counting objects:  32% (146/454)\u001b[K\rremote: Counting objects:  33% (150/454)\u001b[K\rremote: Counting objects:  34% (155/454)\u001b[K\rremote: Counting objects:  35% (159/454)\u001b[K\rremote: Counting objects:  36% (164/454)\u001b[K\rremote: Counting objects:  37% (168/454)\u001b[K\rremote: Counting objects:  38% (173/454)\u001b[K\rremote: Counting objects:  39% (178/454)\u001b[K\rremote: Counting objects:  40% (182/454)\u001b[K\rremote: Counting objects:  41% (187/454)\u001b[K\rremote: Counting objects:  42% (191/454)\u001b[K\rremote: Counting objects:  43% (196/454)\u001b[K\rremote: Counting objects:  44% (200/454)\u001b[K\rremote: Counting objects:  45% (205/454)\u001b[K\rremote: Counting objects:  46% (209/454)\u001b[K\rremote: Counting objects:  47% (214/454)\u001b[K\rremote: Counting objects:  48% (218/454)\u001b[K\rremote: Counting objects:  49% (223/454)\u001b[K\rremote: Counting objects:  50% (227/454)\u001b[K\rremote: Counting objects:  51% (232/454)\u001b[K\rremote: Counting objects:  52% (237/454)\u001b[K\rremote: Counting objects:  53% (241/454)\u001b[K\rremote: Counting objects:  54% (246/454)\u001b[K\rremote: Counting objects:  55% (250/454)\u001b[K\rremote: Counting objects:  56% (255/454)\u001b[K\rremote: Counting objects:  57% (259/454)\u001b[K\rremote: Counting objects:  58% (264/454)\u001b[K\rremote: Counting objects:  59% (268/454)\u001b[K\rremote: Counting objects:  60% (273/454)\u001b[K\rremote: Counting objects:  61% (277/454)\u001b[K\rremote: Counting objects:  62% (282/454)\u001b[K\rremote: Counting objects:  63% (287/454)\u001b[K\rremote: Counting objects:  64% (291/454)\u001b[K\rremote: Counting objects:  65% (296/454)\u001b[K\rremote: Counting objects:  66% (300/454)\u001b[K\rremote: Counting objects:  67% (305/454)\u001b[K\rremote: Counting objects:  68% (309/454)\u001b[K\rremote: Counting objects:  69% (314/454)\u001b[K\rremote: Counting objects:  70% (318/454)\u001b[K\rremote: Counting objects:  71% (323/454)\u001b[K\rremote: Counting objects:  72% (327/454)\u001b[K\rremote: Counting objects:  73% (332/454)\u001b[K\rremote: Counting objects:  74% (336/454)\u001b[K\rremote: Counting objects:  75% (341/454)\u001b[K\rremote: Counting objects:  76% (346/454)\u001b[K\rremote: Counting objects:  77% (350/454)\u001b[K\rremote: Counting objects:  78% (355/454)\u001b[K\rremote: Counting objects:  79% (359/454)\u001b[K\rremote: Counting objects:  80% (364/454)\u001b[K\rremote: Counting objects:  81% (368/454)\u001b[K\rremote: Counting objects:  82% (373/454)\u001b[K\rremote: Counting objects:  83% (377/454)\u001b[K\rremote: Counting objects:  84% (382/454)\u001b[K\rremote: Counting objects:  85% (386/454)\u001b[K\rremote: Counting objects:  86% (391/454)\u001b[K\rremote: Counting objects:  87% (395/454)\u001b[K\rremote: Counting objects:  88% (400/454)\u001b[K\rremote: Counting objects:  89% (405/454)\u001b[K\rremote: Counting objects:  90% (409/454)\u001b[K\rremote: Counting objects:  91% (414/454)\u001b[K\rremote: Counting objects:  92% (418/454)\u001b[K\rremote: Counting objects:  93% (423/454)\u001b[K\rremote: Counting objects:  94% (427/454)\u001b[K\rremote: Counting objects:  95% (432/454)\u001b[K\rremote: Counting objects:  96% (436/454)\u001b[K\rremote: Counting objects:  97% (441/454)\u001b[K\rremote: Counting objects:  98% (445/454)\u001b[K\rremote: Counting objects:  99% (450/454)\u001b[K\rremote: Counting objects: 100% (454/454)\u001b[K\rremote: Counting objects: 100% (454/454), done.\u001b[K\n",
            "remote: Compressing objects: 100% (192/192), done.\u001b[K\n",
            "remote: Total 1574 (delta 313), reused 364 (delta 256), pack-reused 1120\u001b[K\n",
            "Receiving objects: 100% (1574/1574), 5.93 MiB | 20.57 MiB/s, done.\n",
            "Resolving deltas: 100% (1102/1102), done.\n"
          ]
        }
      ],
      "source": [
        "if GCP_RUNTIME:\n",
        "  !sudo apt-get -y install git\n",
        "#@markdown Where to clone the repo into:\n",
        "REPO_DESTINATION_PATH = \"mutformer\" #@param {type:\"string\"}\n",
        "import os,shutil\n",
        "if not os.path.exists(REPO_DESTINATION_PATH):\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "else:\n",
        "  shutil.rmtree(REPO_DESTINATION_PATH)\n",
        "  os.makedirs(REPO_DESTINATION_PATH)\n",
        "cmd = \"git clone https://github.com/WGLab/mutformer.git \\\"\" + REPO_DESTINATION_PATH + \"\\\"\"\n",
        "!{cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj1mClhQQE_n"
      },
      "source": [
        "#Authorize for GCS, Imports, and General Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9S4CiOh3RzFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68eef50b-684f-42a1-93ae-bb5f0a49f00a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authorize for runtime GCS:\n",
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=VUHlF4Eh2CfCfYhfGfIZUhitfGXO0z&prompt=consent&access_type=offline&code_challenge=F2Ts1Cv8RdZLLx9HGZ2LdcyUOeZiyhSTNCGWyuM5AXs&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AVHEtk5xeBxkk3OhwzB8aup9AK0eA08Xw3qxGzcCXzgDb3um6QzCbR6Yb9MY91NSClVWgQ\n",
            "\n",
            "You are now logged in as [tianqitheodorejiang@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n",
            "Authorize for TPU GCS:\n",
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=8KD7LfSxVpMFXnM840miFEdeF9tcLv&prompt=consent&access_type=offline&code_challenge=A1mMgzhaX4fP1H-MfdzBp-1qo8GIJg1H8RkzjVjByVQ&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AVHEtk6Vt4qn8eQ1xmEIf3sQOLd7GZc41VJg4rC5NL3iWI_h3JictaOXZeu2Zt23V8PyIw\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /content/mutformer/optimization.py:105: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "2023-04-04 21:28:53,622 - tensorflow - INFO - Using TPU runtime\n",
            "INFO:tensorflow:Using TPU runtime\n",
            "2023-04-04 21:28:53,624 - tensorflow - WARNING - From /tmp/ipykernel_9593/4093454482.py:79: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /tmp/ipykernel_9593/4093454482.py:79: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2023-04-04 21:28:53.627485: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2023-04-04 21:28:53.637450: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2023-04-04 21:28:53.637482: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d9ce9da8b4f2): /proc/driver/nvidia/version does not exist\n",
            "2023-04-04 21:28:53,639 - tensorflow - INFO - TPU address is grpc://10.16.92.194:8470\n",
            "INFO:tensorflow:TPU address is grpc://10.16.92.194:8470\n",
            "2023-04-04 21:28:53,640 - tensorflow - WARNING - \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if not GCP_RUNTIME:\n",
        "  def authenticate_user(): ##authentication function that uses link authentication instead of popup\n",
        "    if os.path.exists(\"/content/.config/application_default_credentials.json\"): \n",
        "      return\n",
        "    print(\"Authorize for runtime GCS:\")\n",
        "    !gcloud auth login --no-launch-browser\n",
        "    print(\"Authorize for TPU GCS:\")\n",
        "    !gcloud auth application-default login  --no-launch-browser\n",
        "  authenticate_user()\n",
        "\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import importlib\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "if REPO_DESTINATION_PATH == \"mutformer\":\n",
        "  if os.path.exists(\"mutformer_code\"):\n",
        "    shutil.rmtree(\"mutformer_code\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH,\"mutformer_code\")\n",
        "  REPO_DESTINATION_PATH = \"mutformer_code\"\n",
        "if not os.path.exists(\"mutformer\"):\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "else:\n",
        "  shutil.rmtree(\"mutformer\")\n",
        "  shutil.copytree(REPO_DESTINATION_PATH+\"/mutformer_model_code\",\"mutformer\")\n",
        "if \"mutformer\" in sys.path:\n",
        "  sys.path.remove(\"mutformer\")\n",
        "sys.path.append(\"mutformer\")\n",
        "\n",
        "from mutformer import modeling, optimization, tokenization,run_classifier,run_ner_for_pathogenic  #### <<<<< if you added more modes, change these imports to import the correct processors, \n",
        "from mutformer.modeling import BertModel,BertModelModified                                        #### <<<<< correct training scripts (i.e. run_classifier and run_ner_for_pathogenic), and\n",
        "from mutformer.run_classifier import MrpcProcessor,REProcessor,MrpcWithExDataProcessor            #### <<<<< correct model classes\n",
        "from mutformer.run_ner_for_pathogenic import NERProcessor  \n",
        "\n",
        "##reload modules in case that's needed\n",
        "modules2reload = [modeling, \n",
        "                  optimization, \n",
        "                  tokenization,\n",
        "                  run_classifier,\n",
        "                  run_ner_for_pathogenic]\n",
        "for module in modules2reload:\n",
        "    importlib.reload(module)\n",
        "\n",
        "# configure logging\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "log.handlers = []\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "#@markdown Whether or not to write logs to a file\n",
        "DO_FILE_LOGGING = True #@param {type:\"boolean\"}\n",
        "if DO_FILE_LOGGING:\n",
        "  #@markdown * If using file logging, what path to write logs to\n",
        "  FILE_LOGGING_PATH = 'file_logging/spam.log' #@param {type:\"string\"}\n",
        "  if not os.path.exists(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1])):\n",
        "    os.makedirs(\"/\".join(FILE_LOGGING_PATH.split(\"/\")[:-1]))\n",
        "  fh = logging.FileHandler(FILE_LOGGING_PATH)\n",
        "  fh.setLevel(logging.INFO)\n",
        "  fh.setFormatter(formatter)\n",
        "  log.addHandler(fh)\n",
        "\n",
        "ch = logging.StreamHandler()\n",
        "ch.setLevel(logging.INFO)\n",
        "ch.setFormatter(formatter)\n",
        "log.addHandler(ch)\n",
        "\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "  log.info(\"Using TPU runtime\")\n",
        "  TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "  with tf.Session(TPU_ADDRESS) as session:\n",
        "    log.info('TPU address is ' + TPU_ADDRESS)\n",
        "    ##upload credentials to TPU.\n",
        "    with open(\"/content/.config/application_default_credentials.json\", 'r') as f:\n",
        "      auth_info = json.load(f)\n",
        "    tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "    \n",
        "else:\n",
        "  log.warning('Not connected to TPU runtime')\n",
        "\n",
        "\n",
        "if MODE==\"MRPC\":      ####       vvvvv if you added more modes, change this part to set the processors and training scripts correctly vvvvv\n",
        "  processor = MrpcProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = False\n",
        "elif MODE==\"MRPC_w_ex_data\":\n",
        "  processor = MrpcWithExDataProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = True\n",
        "elif MODE==\"RE\":\n",
        "  processor = REProcessor()\n",
        "  script = run_classifier\n",
        "  USING_EX_DATA = False\n",
        "elif MODE==\"NER\":\n",
        "  processor = NERProcessor()\n",
        "  script = run_ner_for_pathogenic\n",
        "  USING_EX_DATA = False\n",
        "else:\n",
        "  raise Exception(\"The mode specified was not one of the available modes: [\\\"MRPC\\\",\\\"MRPC_w_ex_data\\\" \\\"RE\\\",\\\"NER\\\"].\")\n",
        "label_list = processor.get_labels()\n",
        "                      ####       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkFC96e0cK6n"
      },
      "source": [
        "# Run Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdWEQQ0vWBKU"
      },
      "source": [
        "This following section will perform finetuning tests for testing different models' performance with different parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ_lmHSa7ct_"
      },
      "source": [
        "###General definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1Bpaa3Eo7h8W"
      },
      "outputs": [],
      "source": [
        "def latest_checkpoint(dir):\n",
        "  cmd = \"gsutil ls \"+dir\n",
        "  files = !{cmd}\n",
        "  for file in files:\n",
        "    if \"model.ckpt\" in file:\n",
        "      return file.replace(\".\"+file.split(\".\")[-1],\"\")\n",
        "\n",
        "def training_loop(BATCH_SIZE,\n",
        "                  RESUMING,\n",
        "                  PLANNED_TOTAL_STEPS,\n",
        "                  DECAY_PER_STEP,\n",
        "                  MAX_SEQ_LENGTH,\n",
        "                  MODEL_NAME,\n",
        "                  MODEL,\n",
        "                  INIT_CHECKPOINT_DIR,\n",
        "                  GCS_OUTPUT_MODEL_DIR,\n",
        "                  DATA_GCS_DIR,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  USING_EX_DATA,\n",
        "                  EX_DATA_NUM,\n",
        "                  GCS_LOGGING_DIR,\n",
        "                  CONFIG_FILE,\n",
        "                  FREEZING=None):\n",
        "  \n",
        "  tf.logging.info(\"Using data from: \"+DATA_GCS_DIR)\n",
        "  tf.logging.info(\"Loading model from: \"+INIT_CHECKPOINT_DIR)\n",
        "\n",
        "\n",
        "  RESTORE_CHECKPOINT = None if not RESUMING else tf.train.latest_checkpoint(GCS_OUTPUT_MODEL_DIR)\n",
        "  if not RESUMING:\n",
        "    cmd = \"gsutil -m rm -r \"+GCS_OUTPUT_MODEL_DIR\n",
        "    !{cmd}\n",
        "\n",
        "  try: \n",
        "    INIT_CHECKPOINT = tf.train.latest_checkpoint(INIT_CHECKPOINT_DIR)\n",
        "  except:\n",
        "    INIT_CHECKPOINT = latest_checkpoint(INIT_CHECKPOINT_DIR)\n",
        "  print(\"init checkpoint:\",INIT_CHECKPOINT,\"restore/save checkpont:\",RESTORE_CHECKPOINT)\n",
        "\n",
        "  config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "  if not tf.io.gfile.exists(GCS_OUTPUT_MODEL_DIR+\"/config.json\"):\n",
        "    tf.io.gfile.copy(CONFIG_FILE,GCS_OUTPUT_MODEL_DIR+\"/config.json\")\n",
        "\n",
        "  model_fn = script.model_fn_builder(\n",
        "      bert_config=config,\n",
        "      logging_dir=GCS_LOGGING_DIR,\n",
        "      num_labels=len(label_list),\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      restore_checkpoint=RESTORE_CHECKPOINT,\n",
        "      init_learning_rate=INIT_LEARNING_RATE,\n",
        "      decay_per_step=DECAY_PER_STEP,\n",
        "      num_warmup_steps=NUM_WARMUP_STEPS,\n",
        "      use_tpu=True,\n",
        "      use_one_hot_embeddings=True,\n",
        "      bert=MODEL,\n",
        "      weight_decay=WEIGHT_DECAY,\n",
        "      epsilon=1e-6, ##epsilon is used to prevent dividing by zero\n",
        "      clip_grads=False,\n",
        "      freezing_x_layers=FREEZING,\n",
        "      using_ex_data=USING_EX_DATA)\n",
        "\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      model_dir=GCS_OUTPUT_MODEL_DIR,\n",
        "      save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "      keep_checkpoint_max=KEEP_N_CHECKPOINTS_AT_A_TIME,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=True,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      train_batch_size=BATCH_SIZE)\n",
        "  \n",
        "  train_file_name = \"train.tf_record\"\n",
        "  train_file = os.path.join(DATA_GCS_DIR, train_file_name)\n",
        "\n",
        "  if USING_SHARDS:\n",
        "    shards_folder = DATA_GCS_DIR\n",
        "    input_file = os.path.join(DATA_GCS_DIR, train_file_name)\n",
        "    import re\n",
        "    file_name = input_file.split(\"/\")[-1]\n",
        "    shards = [shards_folder + \"/\" + file for file in tf.io.gfile.listdir(shards_folder) if\n",
        "              re.match(file_name + \"_\\d+\", file)]\n",
        "    shards = sorted(shards,key=lambda shard:int(shard.split(\"_\")[-1]))[START_SHARD:]\n",
        "  else:\n",
        "    shards = [train_file]\n",
        "\n",
        "  if USING_SHARDS:\n",
        "    print(\"\\nUSING SHARDs:\")\n",
        "    for shard in shards:\n",
        "      print(shard)\n",
        "    print(\"\\n\")\n",
        "\n",
        "  tf.logging.info(\"***** Running training *****\")\n",
        "  tf.logging.info(\"  Batch size = %d\", BATCH_SIZE)\n",
        "  for n,shard in enumerate(shards):\n",
        "      train_input_fn = script.file_based_input_fn_builder(\n",
        "          input_file=shard,\n",
        "          seq_length=MAX_SEQ_LENGTH,\n",
        "          is_training=True,\n",
        "          drop_remainder=True,\n",
        "          pred_num=EX_DATA_NUM if USING_EX_DATA else None)\n",
        "      estimator.train(input_fn=train_input_fn, max_steps=PLANNED_TOTAL_STEPS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrgQPrH4kZV7"
      },
      "source": [
        "###Training Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqe2NOckb6OR"
      },
      "source": [
        "Following are two code segments for running the finetuning train loop. They correspond according to:\n",
        "  1. Model/sequence length: different model architectures will be tested using a fixed batch size on data of varying sequence lengths \\\n",
        "  2. Freezing/batch sie: the same model architecture is tested based on varying batch sizes and number of layers being frozen during training\n",
        "\n",
        "Note: One may write more training loops for more tests based on a similar format to these two example training loops below, i.e. batch size/sequence length.\n",
        "\\\n",
        "\\\n",
        "Note: During training, evaluation results on the training dataset will be written into GCS. To view these results, use the colab notebook titled \"mutformer processing and viewing finetuning results.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AjnVKSMlVXz"
      },
      "source": [
        "####Model/sequence length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrCuEbr6dv8U"
      },
      "outputs": [],
      "source": [
        "#@markdown ### IO config\n",
        "#@markdown Folder in GCS where the pretrained models needs to be loaded from:\n",
        "INIT_MODEL_DIR = \"\" #@param {type:\"string\"}\n",
        "#@markdown Folder for where to save the finetuned models:\n",
        "OUTPUT_MODEL_DIR = \"\" #@param {type:\"string\"}\n",
        "#@markdown Which folder inside of LOGGING_DIR to store the logs in\n",
        "RUN_NAME = \"\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ### Training procedure config\n",
        "#@markdown Batch size to use\n",
        "BATCH_SIZE =  16#@param {type:\"integer\"}\n",
        "#@markdown The training loop will loop through a list of pretrained models and a list of sequence lengths, training a model for each combination of pretrained model and sequence length\n",
        "#@markdown * List of pretrained models to load (should indicate the names of the model folders inside the specified INIT_MODEL_DIR\n",
        "MODELS =  [\"MutFormer_em_adap8L\"]#@param\n",
        "#@markdown * List of model architectures for each model in the \"MODELS\" list defined in the entry above: each position in this list must correctly indicate the model architecture of its corresponding model folder in the list \"MODELS\" (BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture without integrated convs, MutFormer_embedded_convs indicates MutFormer with integrated convolutions).\n",
        "MODEL_ARCHITECTURES = [\"MutFormer_embedded_convs\"] #@param\n",
        "#@markdown * List of sequence lengths to test\n",
        "MAX_SEQ_LENGTHS = [1024] #@param\n",
        "#@markdown Whether or not to resume training from a previous checkpoint; if no, always train from scratch\n",
        "RESUMING = False #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not data was generated in shards (for really large databases)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown If training data was generated in shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Training uses a linear learning rate.\n",
        "#@markdown * Start learning rate: training will start with this learning rate on the step that learning rate warmup is complete\n",
        "INIT_LEARNING_RATE =  2e-6 #@param {type:\"number\"}\n",
        "#@markdown * End learning rate: training will alter the learning rate every step linearly so that it finishes with this learning rate on the last step.\n",
        "END_LEARNING_RATE = 3e-8 #@param {type:\"number\"}\n",
        "#@markdown How many steps during training to perform learning rate warmup for (start from learning rate 0 and increase linearly to INIT_LEARNING_RATE): Set to 0 for no warmup.\n",
        "NUM_WARMUP_STEPS =  0#@param {type:\"integer\"}\n",
        "#@markdown What weight decay value to use (MutFormer uses 0.01; a higher weight decay is more resistant to exploding gradients, but also limits the model's ability to learn)\n",
        "WEIGHT_DECAY = 0.01 #@param {type:\"number\"}\n",
        "#@markdown Save a checkpoint every this amount of steps:\n",
        "SAVE_CHECKPOINTS_STEPS =   1000#@param {type:\"integer\"}\n",
        "#@markdown TPUEstimator will keep this number of checkpoints at a time; older checkpoints will all be deleted:\n",
        "KEEP_N_CHECKPOINTS_AT_A_TIME =  100#@param {type:\"integer\"}\n",
        "#@markdown Stopping condition for training can be set by either a certain number of sequences or a certain number of steps. from below, PLANNED_TOTAL_STEPS will override PLANNED_TOTAL_SEQUENCES_SEEN; therefore, if using PLANNED_TOTAL_SEQUENCES_SEEN, set PLANNED_TOTAL_STEPS to -1.\n",
        "#@markdown \n",
        "#@markdown * Option 1: How many sequences the model should train on before stopping:\n",
        "PLANNED_TOTAL_SEQUENCES_SEEN =  2e5 #@param {type:\"number\"}\n",
        "#@markdown * Option 2: How many steps the model should train for before stopping (number of total sequences trained on will depend on the batch size used).\n",
        "PLANNED_TOTAL_STEPS =  14000#@param {type:\"number\"}\n",
        "\n",
        "\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS != -1 else PLANNED_TOTAL_SEQUENCES_SEEN//BATCH_SIZE\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/(PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS!=-1 else PLANNED_TOTAL_SEQUENCES_SEEN/BATCH_SIZE) \n",
        "\n",
        "DATA_INFOS = [[\"N/A\" for MODEL_NAME in MODELS]            ##create an empty 2D list to store all\n",
        "              for MAX_SEQ_LENGTH in MAX_SEQ_LENGTHS]      ##the data info dictionaries\n",
        "                                                                                   \n",
        "for M,MAX_SEQ_LENGTH in enumerate(MAX_SEQ_LENGTHS):\n",
        "  for m,MODEL_NAME in enumerate(MODELS):\n",
        "    print(\"\\n\\n\\nMODEL NAME:\",MODEL_NAME,\n",
        "          \"\\nINPUT MAX SEQ LENGTH:\",MAX_SEQ_LENGTH)\n",
        "\n",
        "\n",
        "    MODEL = getattr(modeling, MODEL_ARCHITECTURES[m])\n",
        "    INIT_CHECKPOINT_DIR = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME\n",
        "    GCS_OUTPUT_MODEL_DIR = BUCKET_PATH+\"/\"+OUTPUT_MODEL_DIR+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "    DATA_GCS_DIR = BUCKET_PATH+\"/\"+PROCESSED_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "    \n",
        "    GCS_LOGGING_DIR = BUCKET_PATH+\"/\"+LOGGING_DIR+\"/\"+RUN_NAME+\"/mn_\"+MODEL_NAME+\"_sl_\"+str(MAX_SEQ_LENGTH)\n",
        "\n",
        "    CONFIG_FILE = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME+\"/config.json\"\n",
        "    \n",
        "    if DATA_INFOS[M][m] == \"N/A\":\n",
        "      DATA_INFOS[M][m] = json.load(tf.gfile.Open(DATA_GCS_DIR+\"/info.json\"))\n",
        "    \n",
        "    EX_DATA_NUM = DATA_INFOS[M][m][\"ex_data_num\"] if USING_EX_DATA else 0\n",
        "    \n",
        "    training_loop(BATCH_SIZE,\n",
        "                  RESUMING,\n",
        "                  PLANNED_TOTAL_STEPS,\n",
        "                  DECAY_PER_STEP,\n",
        "                  MAX_SEQ_LENGTH,\n",
        "                  MODEL_NAME,\n",
        "                  MODEL,\n",
        "                  INIT_CHECKPOINT_DIR,\n",
        "                  GCS_OUTPUT_MODEL_DIR,\n",
        "                  DATA_GCS_DIR,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  USING_EX_DATA,\n",
        "                  EX_DATA_NUM,\n",
        "                  GCS_LOGGING_DIR,\n",
        "                  CONFIG_FILE)\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP6CPmRoIWOm"
      },
      "source": [
        "####Freezing/Batch Size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOuxQ5NeGM4v"
      },
      "outputs": [],
      "source": [
        "#@markdown ### IO config\n",
        "#@markdown Folder in GCS where the pretrained models needs to be loaded from:\n",
        "INIT_MODEL_DIR = \"\" #@param {type:\"string\"}\n",
        "#@markdown Folder for where to save the finetuned models:\n",
        "OUTPUT_MODEL_DIR = \"\" #@param {type:\"string\"}\n",
        "#@markdown Which folder inside of LOGGING_DIR to store the logs in\n",
        "RUN_NAME = \"\" #@param {type:\"string\"}\n",
        "#@markdown \\\n",
        "#@markdown \n",
        "#@markdown \n",
        "#@markdown ### Training procedure config\n",
        "FREEZINGS = [8,6,5] #@param\n",
        "#@markdown Batch size to use\n",
        "BATCH_SIZES =  [16,64] #@param\n",
        "#@markdown The training loop will loop through a list of pretrained models and a list of sequence lengths, training a model for each combination of pretrained model and sequence length\n",
        "#@markdown * Model Name to use (should indicate the name of a model folder inside the specified INIT_MODEL_DIR\n",
        "MODEL_NAME =  \"MutFormer_em_adap8L\" #@param {type:\"string\"}\n",
        "#@markdown * Model architecture to use. Must correctly correspond to the model indicated by the model folder specified by the above \"MODEL_NAME\" parameter (BertModel indicates the original BERT, BertModelModified indicates MutFormer's architecture without integrated convs, MutFormer_embedded_convs indicates MutFormer with integrated convolutions).\n",
        "MODEL_ARCHITECTURE = \"MutFormer_embedded_convs\" #@param\n",
        "#@markdown * List of sequence lengths to test\n",
        "MAX_SEQ_LENGTH = 512 #@param\n",
        "#@markdown Whether or not to resume training from a previous checkpoint; if no, always train from scratch\n",
        "RESUMING = True #@param {type:\"boolean\"}\n",
        "#@markdown Whether or not data was generated in shards (for really large databases)\n",
        "USING_SHARDS = False #@param {type:\"boolean\"}\n",
        "#@markdown If training data was generated in shards, which shard index to start at (defualt 0 for first shard)\n",
        "START_SHARD = 0 #@param {type:\"integer\"}\n",
        "#@markdown Training uses a linear learning rate.\n",
        "#@markdown * Start learning rate: training will start with this learning rate on the step that learning rate warmup is complete\n",
        "INIT_LEARNING_RATE =  1e-5 #@param {type:\"number\"}\n",
        "#@markdown * End learning rate: training will alter the learning rate every step linearly so that it finishes with this learning rate on the last step.\n",
        "END_LEARNING_RATE = 3e-9 #@param {type:\"number\"}\n",
        "#@markdown How many steps during training to perform learning rate warmup for (start from learning rate 0 and increase linearly to INIT_LEARNING_RATE): Set to 0 for no warmup.\n",
        "NUM_WARMUP_STEPS =  0#@param {type:\"integer\"}\n",
        "#@markdown What weight decay value to use (MutFormer uses 0.01; a higher weight decay is more resistant to exploding gradients, but also limits the model's ability to learn)\n",
        "WEIGHT_DECAY = 0.01 #@param {type:\"number\"}\n",
        "#@markdown Save a checkpoint every this amount of steps:\n",
        "SAVE_CHECKPOINTS_STEPS =   1000#@param {type:\"integer\"}\n",
        "#@markdown TPUEstimator will keep this number of checkpoints at a time; older checkpoints outside this range will all be deleted:\n",
        "KEEP_N_CHECKPOINTS_AT_A_TIME =  100#@param {type:\"integer\"}\n",
        "#@markdown Stopping condition for training can be set by either a certain number of sequences or a certain number of steps. from below, PLANNED_TOTAL_STEPS will override PLANNED_TOTAL_SEQUENCES_SEEN; therefore, if using PLANNED_TOTAL_SEQUENCES_SEEN, set PLANNED_TOTAL_STEPS to -1.\n",
        "#@markdown \n",
        "#@markdown * Option 1: How many sequences the model should train on before stopping:\n",
        "PLANNED_TOTAL_SEQUENCES_SEEN =  2e5 #@param {type:\"number\"}\n",
        "#@markdown * Option 2: How many steps the model should train for before stopping (number of total sequences trained on will depend on the batch size used).\n",
        "PLANNED_TOTAL_STEPS =  14000#@param {type:\"number\"}\n",
        "\n",
        "\n",
        "PLANNED_TOTAL_STEPS = PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS != -1 else PLANNED_TOTAL_SEQUENCES_SEEN//BATCH_SIZE\n",
        "DECAY_PER_STEP = (END_LEARNING_RATE-INIT_LEARNING_RATE)/(PLANNED_TOTAL_STEPS if PLANNED_TOTAL_STEPS!=-1 else PLANNED_TOTAL_SEQUENCES_SEEN/BATCH_SIZE) \n",
        "\n",
        "DATA_INFOS = [[\"N/A\" for BATCH_SIZE in BATCH_SIZES]            ##create an empty 2D list to store all\n",
        "              for FREEZING in FREEZINGS]      ##the data info dictionaries\n",
        "                                                                                   \n",
        "for M,FREEZING in enumerate(FREEZINGS):\n",
        "  for m,BATCH_SIZE in enumerate(BATCH_SIZES):\n",
        "    print(\"\\n\\n\\nFreezing layers:\",FREEZING,\n",
        "          \"\\nBATCH SIZE:\",BATCH_SIZE)\n",
        "\n",
        "\n",
        "    MODEL = getattr(modeling, MODEL_ARCHITECTURE)\n",
        "    INIT_CHECKPOINT_DIR = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME\n",
        "    GCS_OUTPUT_MODEL_DIR = BUCKET_PATH+\"/\"+OUTPUT_MODEL_DIR+\"/fl_\"+str(FREEZING)+\"_bs_\"+str(BATCH_SIZE)\n",
        "    DATA_GCS_DIR = BUCKET_PATH+\"/\"+PROCESSED_DATA_DIR+\"/\"+str(MAX_SEQ_LENGTH)\n",
        "    \n",
        "    GCS_LOGGING_DIR = BUCKET_PATH+\"/\"+LOGGING_DIR+\"/\"+RUN_NAME+\"/fl_\"+str(FREEZING)+\"_bs_\"+str(BATCH_SIZE)\n",
        "\n",
        "    CONFIG_FILE = BUCKET_PATH+\"/\"+INIT_MODEL_DIR+\"/\"+MODEL_NAME+\"/config.json\"\n",
        "    \n",
        "    if DATA_INFOS[M][m] == \"N/A\":\n",
        "      DATA_INFOS[M][m] = json.load(tf.gfile.Open(DATA_GCS_DIR+\"/info.json\"))\n",
        "    \n",
        "    EX_DATA_NUM = DATA_INFOS[M][m][\"ex_data_num\"] if USING_EX_DATA else 0\n",
        "    \n",
        "    training_loop(BATCH_SIZE,\n",
        "                  RESUMING,\n",
        "                  PLANNED_TOTAL_STEPS,\n",
        "                  DECAY_PER_STEP,\n",
        "                  MAX_SEQ_LENGTH,\n",
        "                  MODEL_NAME,\n",
        "                  MODEL,\n",
        "                  INIT_CHECKPOINT_DIR,\n",
        "                  GCS_OUTPUT_MODEL_DIR,\n",
        "                  DATA_GCS_DIR,\n",
        "                  USING_SHARDS,\n",
        "                  START_SHARD,\n",
        "                  USING_EX_DATA,\n",
        "                  EX_DATA_NUM,\n",
        "                  GCS_LOGGING_DIR,\n",
        "                  CONFIG_FILE,\n",
        "                  FREEZING=FREEZING,)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "SaNhPg7sG2DS"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "name": "py37",
      "display_name": "Python 3.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}